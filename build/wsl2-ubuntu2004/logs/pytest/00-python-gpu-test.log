============================= test session starts ==============================
platform linux -- Python 3.8.5, pytest-6.1.1, py-1.9.0, pluggy-0.13.1 -- /home/otivedani/xgboost/.venv/bin/python3
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples')
rootdir: /home/otivedani/xgboost/tests, configfile: pytest.ini
plugins: hypothesis-5.37.4
collecting ... collected 69 items

tests/python-gpu/test_device_quantile_dmatrix.py::TestDeviceQuantileDMatrix::test_dmatrix_cupy_init SKIPPED
tests/python-gpu/test_device_quantile_dmatrix.py::TestDeviceQuantileDMatrix::test_dmatrix_feature_weights SKIPPED
tests/python-gpu/test_device_quantile_dmatrix.py::TestDeviceQuantileDMatrix::test_dmatrix_numpy_init PASSED
tests/python-gpu/test_from_cudf.py::TestFromColumnar::test_simple_dmatrix_from_cudf SKIPPED
tests/python-gpu/test_from_cudf.py::TestFromColumnar::test_device_dmatrix_from_cudf SKIPPED
tests/python-gpu/test_from_cudf.py::TestFromColumnar::test_cudf_training_simple_dmatrix SKIPPED
tests/python-gpu/test_from_cudf.py::TestFromColumnar::test_cudf_training_device_dmatrix SKIPPED
tests/python-gpu/test_from_cudf.py::TestFromColumnar::test_cudf_metainfo_simple_dmatrix SKIPPED
tests/python-gpu/test_from_cudf.py::TestFromColumnar::test_cudf_metainfo_device_dmatrix SKIPPED
tests/python-gpu/test_from_cudf.py::test_from_cudf_iter SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_simple_dmat_from_cupy SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_device_dmat_from_cupy SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_cupy_training_device_dmat SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_cupy_training_simple_dmat SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_cupy_metainfo_simple_dmat SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_cupy_metainfo_device_dmat SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_dlpack_simple_dmat SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_dlpack_device_dmat SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_specified_device SKIPPED
tests/python-gpu/test_gpu_basic_models.py::TestGPUBasicModels::test_deterministic_gpu_hist FAILED
tests/python-gpu/test_gpu_basic_models.py::TestGPUBasicModels::test_eta_decay_gpu_hist FAILED
tests/python-gpu/test_gpu_demos.py::test_data_iterator SKIPPED
tests/python-gpu/test_gpu_demos.py::test_dask_training SKIPPED
tests/python-gpu/test_gpu_interaction_constraints.py::TestGPUInteractionConstraints::test_interaction_constraints FAILED
tests/python-gpu/test_gpu_interaction_constraints.py::TestGPUInteractionConstraints::test_training_accuracy FAILED
tests/python-gpu/test_gpu_linear.py::TestGPULinear::test_gpu_coordinate Falsifying example: test_gpu_coordinate(
    self=<test_gpu_linear.TestGPULinear at 0x7fa925f0fbb0>,
    param={'booster': 'gblinear',
     'eta': 0.01,
     'feature_selector': 'cyclic',
     'nthread': 1,
     'tolerance': 1e-05,
     'top_k': 1},
    num_rounds=10,
    dataset=boston,
)
FAILED
tests/python-gpu/test_gpu_linear.py::TestGPULinear::test_gpu_coordinate_regularised Falsifying example: test_gpu_coordinate_regularised(
    self=<test_gpu_linear.TestGPULinear at 0x7fa925c8e040>,
    param={'booster': 'gblinear',
     'eta': 0.01,
     'feature_selector': 'cyclic',
     'nthread': 1,
     'tolerance': 1e-05,
     'top_k': 1},
    num_rounds=10,
    dataset=boston,
    alpha=1e-05,
    lambd=1e-05,
)
FAILED
tests/python-gpu/test_gpu_pickling.py::TestPickling::test_pickled_predictor FAILED
tests/python-gpu/test_gpu_pickling.py::TestPickling::test_pickling FAILED
tests/python-gpu/test_gpu_pickling.py::TestPickling::test_predict_sklearn_pickle FAILED
tests/python-gpu/test_gpu_pickling.py::TestPickling::test_training_on_cpu_only_env ============================= test session starts ==============================
platform linux -- Python 3.8.5, pytest-6.1.1, py-1.9.0, pluggy-0.13.1 -- /home/otivedani/xgboost/.venv/bin/python3
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples')
rootdir: /home/otivedani/xgboost/tests, configfile: pytest.ini
plugins: hypothesis-5.37.4
collecting ... collected 1 item

tests/python-gpu/load_pickle.py::TestLoadPickle::test_training_on_cpu_only_env FAILED

=================================== FAILURES ===================================
_________________ TestLoadPickle.test_training_on_cpu_only_env _________________

self = <unittest.case._Outcome object at 0x7f37def99670>
test_case = <load_pickle.TestLoadPickle testMethod=test_training_on_cpu_only_env>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <load_pickle.TestLoadPickle testMethod=test_training_on_cpu_only_env>
result = <TestCaseFunction test_training_on_cpu_only_env>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <load_pickle.TestLoadPickle testMethod=test_training_on_cpu_only_env>
method = <bound method TestLoadPickle.test_training_on_cpu_only_env of <load_pickle.TestLoadPickle testMethod=test_training_on_cpu_only_env>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <load_pickle.TestLoadPickle testMethod=test_training_on_cpu_only_env>

    def test_training_on_cpu_only_env(self):
        assert os.environ['CUDA_VISIBLE_DEVICES'] == '-1'
        rng = np.random.RandomState(1994)
        X = rng.randn(10, 10)
        y = rng.randn(10)
        with tb.captured_output() as (out, err):
            # Test no thrust exception is thrown
            with pytest.raises(xgb.core.XGBoostError):
                xgb.train({'tree_method': 'gpu_hist'}, xgb.DMatrix(X, y))
    
>           assert out.getvalue().find('No visible GPU is found') != -1
E           AssertionError: assert -1 != -1
E            +  where -1 = <built-in method find of str object at 0x7f3802d8f2f0>('No visible GPU is found')
E            +    where <built-in method find of str object at 0x7f3802d8f2f0> = ''.find
E            +      where '' = <built-in method getvalue of _io.StringIO object at 0x7f37defdff70>()
E            +        where <built-in method getvalue of _io.StringIO object at 0x7f37defdff70> = <_io.StringIO object at 0x7f37defdff70>.getvalue

tests/python-gpu/load_pickle.py:69: AssertionError
=========================== short test summary info ============================
FAILED tests/python-gpu/load_pickle.py::TestLoadPickle::test_training_on_cpu_only_env
============================== 1 failed in 0.11s ===============================
FAILED
tests/python-gpu/test_gpu_pickling.py::TestPickling::test_wrap_gpu_id FAILED
tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_inplace_predict_cudf SKIPPED
tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_inplace_predict_cupy SKIPPED
tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_multi_predict FAILED
tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_predict FAILED
tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_shap Falsifying example: test_shap(
    self=<test_gpu_prediction.TestGPUPredict testMethod=test_shap>,
    num_rounds=1,
    dataset=boston,
    param={'max_depth': 0, 'max_leaves': 1, 'num_parallel_tree': 1},
)
FAILED
tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_shap_interactions Falsifying example: test_shap_interactions(
    self=<test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>,
    num_rounds=1,
    dataset=sparse,
    param={'max_depth': 0, 'max_leaves': 1, 'num_parallel_tree': 1},
)
Traceback (most recent call last):
  File "/home/otivedani/xgboost/tests/python-gpu/test_gpu_prediction.py", line 217, in test_shap_interactions
    pytest.xfail(reason=f'GPU shap with sparse is flaky: {issue}')
  File "/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/outcomes.py", line 172, in xfail
    raise XFailed(reason)
_pytest.outcomes.XFailed: GPU shap with sparse is flaky: https://github.com/dmlc/xgboost/issues/6074

Falsifying example: test_shap_interactions(
    self=<test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>,
    num_rounds=1,
    dataset=boston,
    param={'max_depth': 0, 'max_leaves': 1, 'num_parallel_tree': 1},
)
Traceback (most recent call last):
  File "/home/otivedani/xgboost/tests/python-gpu/test_gpu_prediction.py", line 222, in test_shap_interactions
    bst = xgb.train(param, dmat, num_rounds)
  File "/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/training.py", line 216, in train
    bst = _train_internal(params, dtrain,
  File "/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/training.py", line 111, in _train_internal
    bst.update(dtrain, i, obj)
  File "/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/core.py", line 1178, in update
    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
  File "/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/core.py", line 186, in _check_call
    raise XGBoostError(py_str(_LIB.XGBGetLastError()))
xgboost.core.XGBoostError: [15:55:43] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
Stack trace:
  [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
  [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::GetPredictor(xgboost::HostDeviceVector<float> const*, xgboost::DMatrix*) const+0x225) [0x7fa92b84ec35]
  [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::PredictBatch(xgboost::DMatrix*, xgboost::PredictionCacheEntry*, bool, unsigned int)+0x49) [0x7fa92b84ef29]
  [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x3b2) [0x7fa92b87ed42]
  [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x137c0) [0x7fa94d18f7c0]



FAILED
tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_sklearn FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_map_auc FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_map_map FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_map_ndcg FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_ndcg_auc FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_ndcg_map FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_ndcg_ndcg FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_pairwise_auc_metric FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_pairwise_map_metric FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_pairwise_ndcg_metric FAILED
tests/python-gpu/test_gpu_training_continuation.py::TestGPUTrainingContinuation::test_gpu_training_continuation_binary FAILED
tests/python-gpu/test_gpu_training_continuation.py::TestGPUTrainingContinuation::test_gpu_training_continuation_json FAILED
tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_gpu_hist Falsifying example: test_gpu_hist(
    self=<test_gpu_updaters.TestGPUUpdaters at 0x7fa925e57bb0>,
    param={'colsample_bylevel': 0.5,
     'colsample_bytree': 0.5,
     'grow_policy': 'lossguide',
     'max_bin': 2,
     'max_depth': 0,
     'max_leaves': 1,
     'min_child_weight': 0.5,
     'seed': 0,
     'single_precision_histogram': False},
    num_rounds=1,
    dataset=boston,
)
FAILED
tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_categorical SKIPPED
tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_categorical_32_cat FAILED
tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_gpu_hist_device_dmatrix SKIPPED
tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_external_memory Falsifying example: test_external_memory(
    self=<test_gpu_updaters.TestGPUUpdaters at 0x7fa925f0f250>,
    param={'colsample_bylevel': 0.5,
     'colsample_bytree': 0.5,
     'grow_policy': 'lossguide',
     'max_bin': 2,
     'max_depth': 0,
     'max_leaves': 1,
     'min_child_weight': 0.5,
     'seed': 0,
     'single_precision_histogram': False},
    num_rounds=1,
    dataset=boston,
)
FAILED
tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_empty_dmatrix_prediction FAILED
tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_specified_gpu_id_gpu_update Falsifying example: test_specified_gpu_id_gpu_update(
    self=<test_gpu_updaters.TestGPUUpdaters at 0x7fa9260165e0>,
    dataset=boston,
    gpu_id=0,
)
FAILED
tests/python-gpu/test_gpu_with_dask.py::TestDistributedGPU::test_dask_dataframe SKIPPED
tests/python-gpu/test_gpu_with_dask.py::TestDistributedGPU::test_gpu_hist[local_cuda_cluster0] SKIPPED
tests/python-gpu/test_gpu_with_dask.py::TestDistributedGPU::test_dask_array SKIPPED
tests/python-gpu/test_gpu_with_dask.py::TestDistributedGPU::test_empty_dmatrix SKIPPED
tests/python-gpu/test_gpu_with_dask.py::TestDistributedGPU::test_quantile_basic SKIPPED
tests/python-gpu/test_gpu_with_dask.py::TestDistributedGPU::test_quantile_same_on_all_workers SKIPPED
tests/python-gpu/test_gpu_with_dask.py::test_with_asyncio SKIPPED
tests/python-gpu/test_gpu_with_sklearn.py::test_gpu_binary_classification FAILED
tests/python-gpu/test_gpu_with_sklearn.py::TestGPUBoostFromPrediction::test_boost_from_prediction_gpu_hist FAILED
tests/python-gpu/test_monotonic_constraints.py::TestMonotonicConstraints::test_gpu_hist_basic FAILED
tests/python-gpu/test_monotonic_constraints.py::TestMonotonicConstraints::test_gpu_hist_depthwise FAILED
tests/python-gpu/test_monotonic_constraints.py::TestMonotonicConstraints::test_gpu_hist_lossguide FAILED

=================================== FAILURES ===================================
________________ TestGPUBasicModels.test_deterministic_gpu_hist ________________

self = <unittest.case._Outcome object at 0x7fa926135490>
test_case = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_deterministic_gpu_hist>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_deterministic_gpu_hist>
result = <TestCaseFunction test_deterministic_gpu_hist>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_deterministic_gpu_hist>
method = <bound method TestGPUBasicModels.test_deterministic_gpu_hist of <test_gpu_basic_models.TestGPUBasicModels testMethod=test_deterministic_gpu_hist>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_deterministic_gpu_hist>

    def test_deterministic_gpu_hist(self):
        kRows = 1000
        kCols = 64
        kClasses = 4
        # Create large values to force rounding.
        X = np.random.randn(kRows, kCols) * 1e4
        y = np.random.randint(0, kClasses, size=kRows) * 1e4
    
>       model_0, model_1 = self.run_cls(X, y, True)

tests/python-gpu/test_gpu_basic_models.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_deterministic_gpu_hist>
X = array([[  4368.88329134, -18700.25057369,  -8926.55417994, ...,
         -9350.91472661,  -7110.17177627,   5560.69121...  [  4170.87355911,   6182.83895002,    -43.24459245, ...,
          1745.17785992,   4159.4375747 ,  -2576.33026585]])
y = array([20000., 10000.,     0., 10000.,     0., 10000., 20000.,     0.,
       20000., 10000., 30000.,     0., 30000., ...10000., 30000., 20000., 20000.,     0., 20000.,
       10000., 10000., 10000., 10000., 30000., 30000.,     0., 30000.])
deterministic = True

    def run_cls(self, X, y, deterministic):
        cls = xgb.XGBClassifier(tree_method='gpu_hist',
                                deterministic_histogram=deterministic,
                                single_precision_histogram=True)
>       cls.fit(X, y)

tests/python-gpu/test_gpu_basic_models.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_by...ram=True,
              subsample=None, tree_method='gpu_hist', validate_parameters=None,
              verbosity=None)
X = array([[  4368.88329134, -18700.25057369,  -8926.55417994, ...,
         -9350.91472661,  -7110.17177627,   5560.69121...  [  4170.87355911,   6182.83895002,    -43.24459245, ...,
          1745.17785992,   4159.4375747 ,  -2576.33026585]])
y = array([20000., 10000.,     0., 10000.,     0., 10000., 20000.,     0.,
       20000., 10000., 30000.,     0., 30000., ...10000., 30000., 20000., 20000.,     0., 20000.,
       10000., 10000., 10000., 10000., 30000., 30000.,     0., 30000.])
sample_weight = None, base_margin = None, eval_set = None, eval_metric = None
early_stopping_rounds = None, verbose = True, xgb_model = None
sample_weight_eval_set = None, feature_weights = None, callbacks = None

    def fit(self, X, y, sample_weight=None, base_margin=None,
            eval_set=None, eval_metric=None,
            early_stopping_rounds=None, verbose=True, xgb_model=None,
            sample_weight_eval_set=None, feature_weights=None, callbacks=None):
        # pylint: disable = attribute-defined-outside-init,arguments-differ
    
        evals_result = {}
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
    
        xgb_options = self.get_xgb_params()
    
        if callable(self.objective):
            obj = _objective_decorator(self.objective)
            # Use default value. Is it really not used ?
            xgb_options["objective"] = "binary:logistic"
        else:
            obj = None
    
        if self.n_classes_ > 2:
            # Switch to using a multiclass objective in the underlying
            # XGB instance
            xgb_options['objective'] = 'multi:softprob'
            xgb_options['num_class'] = self.n_classes_
    
        feval = eval_metric if callable(eval_metric) else None
        if eval_metric is not None:
            if callable(eval_metric):
                eval_metric = None
            else:
                xgb_options.update({"eval_metric": eval_metric})
    
        self._le = XGBoostLabelEncoder().fit(y)
        training_labels = self._le.transform(y)
    
        if eval_set is not None:
            if sample_weight_eval_set is None:
                sample_weight_eval_set = [None] * len(eval_set)
            else:
                assert len(sample_weight_eval_set) == len(eval_set)
            evals = list(
                DMatrix(eval_set[i][0],
                        label=self._le.transform(eval_set[i][1]),
                        missing=self.missing, weight=sample_weight_eval_set[i],
                        nthread=self.n_jobs)
                for i in range(len(eval_set))
            )
            nevals = len(evals)
            eval_names = ["validation_{}".format(i) for i in range(nevals)]
            evals = list(zip(evals, eval_names))
        else:
            evals = ()
    
        if len(X.shape) != 2:
            # Simply raise an error here since there might be many
            # different ways of reshaping
            raise ValueError(
                'Please reshape the input data X into 2-dimensional matrix.')
    
        self._features_count = X.shape[1]
        self.n_features_in_ = self._features_count
    
        train_dmatrix = DMatrix(X, label=training_labels, weight=sample_weight,
                                base_margin=base_margin,
                                missing=self.missing, nthread=self.n_jobs)
        train_dmatrix.set_info(feature_weights=feature_weights)
    
>       self._Booster = train(xgb_options, train_dmatrix,
                              self.get_num_boosting_rounds(),
                              evals=evals,
                              early_stopping_rounds=early_stopping_rounds,
                              evals_result=evals_result, obj=obj, feval=feval,
                              verbose_eval=verbose, xgb_model=xgb_model,
                              callbacks=callbacks)

.venv/lib/python3.8/site-packages/xgboost/sklearn.py:836: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa9261355b0>, num_boost_round = 100
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = {}, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa9261355b0>, num_boost_round = 100
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa926135820>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa926135610>
dtrain = <xgboost.core.DMatrix object at 0x7fa9261355b0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:55:41] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
__________________ TestGPUBasicModels.test_eta_decay_gpu_hist __________________

self = <unittest.case._Outcome object at 0x7fa925eafc10>
test_case = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_eta_decay_gpu_hist>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_eta_decay_gpu_hist>
result = <TestCaseFunction test_eta_decay_gpu_hist>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_eta_decay_gpu_hist>
method = <bound method TestGPUBasicModels.test_eta_decay_gpu_hist of <test_gpu_basic_models.TestGPUBasicModels testMethod=test_eta_decay_gpu_hist>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_eta_decay_gpu_hist>

    def test_eta_decay_gpu_hist(self):
>       self.cputest.run_eta_decay('gpu_hist', True)

tests/python-gpu/test_gpu_basic_models.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_callback.TestCallbacks testMethod=runTest>
tree_method = 'gpu_hist', deprecated_callback = True

    def run_eta_decay(self, tree_method, deprecated_callback):
        if deprecated_callback:
            scheduler = xgb.callback.reset_learning_rate
        else:
            scheduler = xgb.callback.LearningRateScheduler
    
        dpath = os.path.join(tm.PROJECT_ROOT, 'demo/data/')
        dtrain = xgb.DMatrix(dpath + 'agaricus.txt.train')
        dtest = xgb.DMatrix(dpath + 'agaricus.txt.test')
        watchlist = [(dtest, 'eval'), (dtrain, 'train')]
        num_round = 4
    
        # learning_rates as a list
        # init eta with 0 to check whether learning_rates work
        param = {'max_depth': 2, 'eta': 0, 'verbosity': 0,
                 'objective': 'binary:logistic', 'eval_metric': 'error',
                 'tree_method': tree_method}
        evals_result = {}
>       bst = xgb.train(param, dtrain, num_round, watchlist,
                        callbacks=[scheduler([
                            0.8, 0.7, 0.6, 0.5
                        ])],
                        evals_result=evals_result)

tests/python/test_callback.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eta': 0, 'eval_metric': 'error', 'max_depth': 2, 'objective': 'binary:logistic', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925eafca0>, num_boost_round = 4
evals = [(<xgboost.core.DMatrix object at 0x7fa925eafd30>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925eafca0>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = None
evals_result = {}, verbose_eval = True, xgb_model = None
callbacks = [<function reset_learning_rate.<locals>.callback at 0x7fa9260378b0>, <function print_evaluation.<locals>.callback at 0x7fa9260fc430>, <function record_evaluation.<locals>.callback at 0x7fa92611b700>]

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eta': 0, 'eval_metric': 'error', 'max_depth': 2, 'objective': 'binary:logistic', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925eafca0>, num_boost_round = 4
evals = [(<xgboost.core.DMatrix object at 0x7fa925eafd30>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925eafca0>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.LegacyCallbacks object at 0x7fa925eaf970>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925eafe80>
dtrain = <xgboost.core.DMatrix object at 0x7fa925eafca0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:55:41] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
__________ TestGPUInteractionConstraints.test_interaction_constraints __________

self = <unittest.case._Outcome object at 0x7fa9260b0b20>
test_case = <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_interaction_constraints>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_interaction_constraints>
result = <TestCaseFunction test_interaction_constraints>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_interaction_constraints>
method = <bound method TestGPUInteractionConstraints.test_interaction_constraints of <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_interaction_constraints>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_interaction_constraints>

    def test_interaction_constraints(self):
>       self.cputest.run_interaction_constraints(tree_method='gpu_hist')

tests/python-gpu/test_gpu_interaction_constraints.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_interaction_constraints.TestInteractionConstraints testMethod=runTest>
tree_method = 'gpu_hist'

    def run_interaction_constraints(self, tree_method):
        x1 = np.random.normal(loc=1.0, scale=1.0, size=1000)
        x2 = np.random.normal(loc=1.0, scale=1.0, size=1000)
        x3 = np.random.choice([1, 2, 3], size=1000, replace=True)
        y = x1 + x2 + x3 + x1 * x2 * x3 \
            + np.random.normal(
                loc=0.001, scale=1.0, size=1000) + 3 * np.sin(x1)
        X = np.column_stack((x1, x2, x3))
        dtrain = xgboost.DMatrix(X, label=y)
    
        params = {
            'max_depth': 3,
            'eta': 0.1,
            'nthread': 2,
            'interaction_constraints': '[[0, 1]]',
            'tree_method': tree_method
        }
        num_boost_round = 12
        # Fit a model that only allows interaction between x1 and x2
>       bst = xgboost.train(
            params, dtrain, num_boost_round, evals=[(dtrain, 'train')])

tests/python/test_interaction_constraints.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eta': 0.1, 'interaction_constraints': '[[0, 1]]', 'max_depth': 3, 'nthread': 2, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa9260b0c70>, num_boost_round = 12
evals = [(<xgboost.core.DMatrix object at 0x7fa9260b0c70>, 'train')], obj = None
feval = None, maximize = None, early_stopping_rounds = None, evals_result = None
verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eta': 0.1, 'interaction_constraints': '[[0, 1]]', 'max_depth': 3, 'nthread': 2, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa9260b0c70>, num_boost_round = 12
evals = [(<xgboost.core.DMatrix object at 0x7fa9260b0c70>, 'train')], obj = None
feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa9260b0d60>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa9260b0d30>
dtrain = <xgboost.core.DMatrix object at 0x7fa9260b0c70>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:55:41] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
_____________ TestGPUInteractionConstraints.test_training_accuracy _____________

self = <unittest.case._Outcome object at 0x7fa9260c0eb0>
test_case = <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_training_accuracy>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_training_accuracy>
result = <TestCaseFunction test_training_accuracy>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_training_accuracy>
method = <bound method TestGPUInteractionConstraints.test_training_accuracy of <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_training_accuracy>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_training_accuracy>

    def test_training_accuracy(self):
>       self.cputest.training_accuracy(tree_method='gpu_hist')

tests/python-gpu/test_gpu_interaction_constraints.py:17: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_interaction_constraints.TestInteractionConstraints testMethod=runTest>
tree_method = 'gpu_hist'

    @pytest.mark.skipif(**tm.no_sklearn())
    def training_accuracy(self, tree_method):
        from sklearn.metrics import accuracy_score
        dtrain = xgboost.DMatrix(dpath + 'agaricus.txt.train?indexing_mode=1')
        dtest = xgboost.DMatrix(dpath + 'agaricus.txt.test?indexing_mode=1')
        params = {
            'eta': 1,
            'max_depth': 6,
            'objective': 'binary:logistic',
            'tree_method': tree_method,
            'interaction_constraints': '[[1,2], [2,3,4]]'
        }
        num_boost_round = 5
    
        params['grow_policy'] = 'lossguide'
>       bst = xgboost.train(params, dtrain, num_boost_round)

tests/python/test_interaction_constraints.py:76: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eta': 1, 'grow_policy': 'lossguide', 'interaction_constraints': '[[1,2], [2,3,4]]', 'max_depth': 6, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa9260c0f10>, num_boost_round = 5
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eta': 1, 'grow_policy': 'lossguide', 'interaction_constraints': '[[1,2], [2,3,4]]', 'max_depth': 6, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa9260c0f10>, num_boost_round = 5
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa9260c06a0>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa9260c0e80>
dtrain = <xgboost.core.DMatrix object at 0x7fa9260c0f10>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:55:41] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
______________________ TestGPULinear.test_gpu_coordinate _______________________

cls = <class '_pytest.runner.CallInfo'>
func = <function call_runtest_hook.<locals>.<lambda> at 0x7fa9260099d0>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: "Optional[Union[Type[BaseException], Tuple[Type[BaseException], ...]]]" = None,
    ) -> "CallInfo[TResult]":
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result = func()  # type: Optional[TResult]

.venv/lib/python3.8/site-packages/_pytest/runner.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: ihook(item=item, **kwds), when=when, reraise=reraise
    )

.venv/lib/python3.8/site-packages/_pytest/runner.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_runtest_call'>, args = ()
kwargs = {'item': <Function test_gpu_coordinate>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fa94e3b0580>
hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
kwargs = {'item': <Function test_gpu_coordinate>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
kwargs = {'item': <Function test_gpu_coordinate>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
caller_kwargs = {'item': <Function test_gpu_coordinate>}, firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fa925ee19d0>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
caller_kwargs = {'item': <Function test_gpu_coordinate>}, firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_gpu_coordinate>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
            item.runtest()
        except Exception as e:
            # Store trace info to allow postmortem debugging
            sys.last_type = type(e)
            sys.last_value = e
            assert e.__traceback__ is not None
            # Skip *this* frame
            sys.last_traceback = e.__traceback__.tb_next
>           raise e

.venv/lib/python3.8/site-packages/_pytest/runner.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_gpu_coordinate>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
>           item.runtest()

.venv/lib/python3.8/site-packages/_pytest/runner.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Function test_gpu_coordinate>

    def runtest(self) -> None:
        """Execute the underlying test function."""
>       self.ihook.pytest_pyfunc_call(pyfuncitem=self)

.venv/lib/python3.8/site-packages/_pytest/python.py:1627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_pyfunc_call'>, args = ()
kwargs = {'pyfuncitem': <Function test_gpu_coordinate>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fa94e3b0580>
hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_gpu_coordinate>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_gpu_coordinate>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_gpu_coordinate>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fa926055430>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_gpu_coordinate>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pyfuncitem = <Function test_gpu_coordinate>

    @hookimpl(trylast=True)
    def pytest_pyfunc_call(pyfuncitem: "Function") -> Optional[object]:
        testfunction = pyfuncitem.obj
        if is_async_function(testfunction):
            async_warn_and_skip(pyfuncitem.nodeid)
        funcargs = pyfuncitem.funcargs
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
>       result = testfunction(**testargs)

.venv/lib/python3.8/site-packages/_pytest/python.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_linear.TestGPULinear object at 0x7fa925f0fbb0>

    @given(parameter_strategy, strategies.integers(10, 50),
>          tm.dataset_strategy)
    @settings(deadline=None)

tests/python-gpu/test_gpu_linear.py:27: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arguments = ()
kwargs = {'self': <test_gpu_linear.TestGPULinear object at 0x7fa925f0fbb0>}
__tracebackhide__ = True
test = <function TestGPULinear.test_gpu_coordinate at 0x7fa927cd4b80>
settings = settings(database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples'), deadline=None, derand...nt_blob=False, report_multiple_bugs=True, stateful_step_count=50, suppress_health_check=(), verbosity=Verbosity.normal)
random = <random.Random object at 0x2948e10>
processed_args = ((), {'self': <test_gpu_linear.TestGPULinear object at 0x7fa925f0fbb0>}, <function default_new_style_executor at 0x7fa...lue=10)})}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_linear.TestGPULinear object at 0x7fa925f0fbb0>))
test_runner = <function default_new_style_executor at 0x7fa94d2a94c0>
search_strategy = WithRunner(TupleStrategy((just(()), fixed_dictionaries({'dataset': _dataset_weight_margin(),
 'num_rounds': integers(m...alue=10)})}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_linear.TestGPULinear object at 0x7fa925f0fbb0>)
runner = <test_gpu_linear.TestGPULinear object at 0x7fa925f0fbb0>
state = <hypothesis.core.StateForActualGivenExecution object at 0x7fa9260550a0>

    @impersonate(test)
    @define_function_signature(test.__name__, test.__doc__, argspec)
    def wrapped_test(*arguments, **kwargs):
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
    
        test = wrapped_test.hypothesis.inner_test
    
        if getattr(test, "is_hypothesis_test", False):
            raise InvalidArgument(
                (
                    "You have applied @given to the test %s more than once, which "
                    "wraps the test several times and is extremely slow. A "
                    "similar effect can be gained by combining the arguments "
                    "of the two calls to given. For example, instead of "
                    "@given(booleans()) @given(integers()), you could write "
                    "@given(booleans(), integers())"
                )
                % (test.__name__,)
            )
    
        settings = wrapped_test._hypothesis_internal_use_settings
    
        random = get_random_for_wrapped_test(test, wrapped_test)
    
        processed_args = process_arguments_to_given(
            wrapped_test, arguments, kwargs, given_kwargs, argspec
        )
        arguments, kwargs, test_runner, search_strategy = processed_args
    
        runner = getattr(search_strategy, "runner", None)
        if isinstance(runner, TestCase) and test.__name__ in dir(TestCase):
            msg = (
                "You have applied @given to the method %s, which is "
                "used by the unittest runner but is not itself a test."
                "  This is not useful in any way." % test.__name__
            )
            fail_health_check(settings, msg, HealthCheck.not_a_test_method)
        if bad_django_TestCase(runner):  # pragma: no cover
            # Covered by the Django tests, but not the pytest coverage task
            raise InvalidArgument(
                "You have applied @given to a method on %s, but this "
                "class does not inherit from the supported versions in "
                "`hypothesis.extra.django`.  Use the Hypothesis variants "
                "to ensure that each example is run in a separate "
                "database transaction." % qualname(type(runner))
            )
    
        state = StateForActualGivenExecution(
            test_runner, search_strategy, test, settings, random, wrapped_test
        )
    
        reproduce_failure = wrapped_test._hypothesis_internal_use_reproduce_failure
    
        # If there was a @reproduce_failure decorator, use it to reproduce
        # the error (or complain that we couldn't). Either way, this will
        # always raise some kind of error.
        if reproduce_failure is not None:
            expected_version, failure = reproduce_failure
            if expected_version != __version__:
                raise InvalidArgument(
                    (
                        "Attempting to reproduce a failure from a different "
                        "version of Hypothesis. This failure is from %s, but "
                        "you are currently running %r. Please change your "
                        "Hypothesis version to a matching one."
                    )
                    % (expected_version, __version__)
                )
            try:
                state.execute_once(
                    ConjectureData.for_buffer(decode_failure(failure)),
                    print_example=True,
                    is_final=True,
                )
                raise DidNotReproduce(
                    "Expected the test to raise an error, but it "
                    "completed successfully."
                )
            except StopTest:
                raise DidNotReproduce(
                    "The shape of the test data has changed in some way "
                    "from where this blob was defined. Are you sure "
                    "you're running the same test?"
                )
            except UnsatisfiedAssumption:
                raise DidNotReproduce(
                    "The test data failed to satisfy an assumption in the "
                    "test. Have you added it since this blob was "
                    "generated?"
                )
    
        # There was no @reproduce_failure, so start by running any explicit
        # examples from @example decorators.
        errors = list(
            execute_explicit_examples(state, wrapped_test, arguments, kwargs)
        )
        with local_settings(state.settings):
            if len(errors) > 1:
                # If we're not going to report multiple bugs, we would have
                # stopped running explicit examples at the first failure.
                assert state.settings.report_multiple_bugs
                for fragments, err in errors:
                    for f in fragments:
                        report(f)
                    tb_lines = traceback.format_exception(
                        type(err), err, err.__traceback__
                    )
                    report("".join(tb_lines))
                msg = "Hypothesis found %d failures in explicit examples."
                raise MultipleFailures(msg % (len(errors)))
            elif errors:
                fragments, the_error_hypothesis_found = errors[0]
                for f in fragments:
                    report(f)
                raise the_error_hypothesis_found
    
        # If there were any explicit examples, they all ran successfully.
        # The next step is to use the Conjecture engine to run the test on
        # many different inputs.
    
        if not (
            Phase.reuse in settings.phases or Phase.generate in settings.phases
        ):
            return
    
        try:
            if isinstance(runner, TestCase) and hasattr(runner, "subTest"):
                subTest = runner.subTest
                try:
                    runner.subTest = types.MethodType(fake_subTest, runner)
                    state.run_engine()
                finally:
                    runner.subTest = subTest
            else:
                state.run_engine()
        except BaseException as e:
            # The exception caught here should either be an actual test
            # failure (or MultipleFailures), or some kind of fatal error
            # that caused the engine to stop.
    
            generated_seed = wrapped_test._hypothesis_internal_use_generated_seed
            with local_settings(settings):
                if not (state.failed_normally or generated_seed is None):
                    if running_under_pytest:
                        report(
                            "You can add @seed(%(seed)d) to this test or "
                            "run pytest with --hypothesis-seed=%(seed)d "
                            "to reproduce this failure." % {"seed": generated_seed}
                        )
                    else:
                        report(
                            "You can add @seed(%d) to this test to "
                            "reproduce this failure." % (generated_seed,)
                        )
                # The dance here is to avoid showing users long tracebacks
                # full of Hypothesis internals they don't care about.
                # We have to do this inline, to avoid adding another
                # internal stack frame just when we've removed the rest.
                #
                # Using a variable for our trimmed error ensures that the line
                # which will actually appear in tracebacks is as clear as
                # possible - "raise the_error_hypothesis_found".
                the_error_hypothesis_found = e.with_traceback(
                    get_trimmed_traceback()
                )
>               raise the_error_hypothesis_found

.venv/lib/python3.8/site-packages/hypothesis/core.py:1162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_linear.TestGPULinear object at 0x7fa925f0fbb0>
param = {'booster': 'gblinear', 'eta': 0.01, 'eval_metric': 'rmse', 'feature_selector': 'cyclic', ...}
num_rounds = 10, dataset = boston

    @given(parameter_strategy, strategies.integers(10, 50),
           tm.dataset_strategy)
    @settings(deadline=None)
    def test_gpu_coordinate(self, param, num_rounds, dataset):
        assume(len(dataset.y) > 0)
        param['updater'] = 'gpu_coord_descent'
        param = dataset.set_params(param)
>       result = train_result(param, dataset.get_dmat(), num_rounds)['train'][dataset.metric]

tests/python-gpu/test_gpu_linear.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

param = {'booster': 'gblinear', 'eta': 0.01, 'eval_metric': 'rmse', 'feature_selector': 'cyclic', ...}
dmat = <xgboost.core.DMatrix object at 0x7fa925d4c8e0>, num_rounds = 10

    def train_result(param, dmat, num_rounds):
        result = {}
>       xgb.train(param, dmat, num_rounds, [(dmat, 'train')], verbose_eval=False,
                  evals_result=result)

tests/python-gpu/test_gpu_linear.py:20: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gblinear', 'eta': 0.01, 'eval_metric': 'rmse', 'feature_selector': 'cyclic', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925d4c8e0>, num_boost_round = 10
evals = [(<xgboost.core.DMatrix object at 0x7fa925d4c8e0>, 'train')], obj = None
feval = None, maximize = None, early_stopping_rounds = None, evals_result = {}
verbose_eval = False, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gblinear', 'eta': 0.01, 'eval_metric': 'rmse', 'feature_selector': 'cyclic', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925d4c8e0>, num_boost_round = 10
evals = [(<xgboost.core.DMatrix object at 0x7fa925d4c8e0>, 'train')], obj = None
feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925d4c490>
evals_result = {}, maximize = None, verbose_eval = False
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925d4c460>
dtrain = <xgboost.core.DMatrix object at 0x7fa925d4c8e0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:55:42] /home/otivedani/xgboost/src/linear/linear_updater.cc:17: Unknown linear updater gpu_coord_descent
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LinearUpdater::Create(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, xgboost::GenericParameter const*)+0x403) [0x7fa92b8951b3]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBLinear::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x252) [0x7fa92b849632]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x137c0) [0x7fa94d18f7c0]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
---------------------------------- Hypothesis ----------------------------------
Falsifying example: test_gpu_coordinate(
    self=<test_gpu_linear.TestGPULinear at 0x7fa925f0fbb0>,
    param={'booster': 'gblinear',
     'eta': 0.01,
     'feature_selector': 'cyclic',
     'nthread': 1,
     'tolerance': 1e-05,
     'top_k': 1},
    num_rounds=10,
    dataset=boston,
)
________________ TestGPULinear.test_gpu_coordinate_regularised _________________

cls = <class '_pytest.runner.CallInfo'>
func = <function call_runtest_hook.<locals>.<lambda> at 0x7fa926009a60>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: "Optional[Union[Type[BaseException], Tuple[Type[BaseException], ...]]]" = None,
    ) -> "CallInfo[TResult]":
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result = func()  # type: Optional[TResult]

.venv/lib/python3.8/site-packages/_pytest/runner.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: ihook(item=item, **kwds), when=when, reraise=reraise
    )

.venv/lib/python3.8/site-packages/_pytest/runner.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_runtest_call'>, args = ()
kwargs = {'item': <Function test_gpu_coordinate_regularised>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fa94e3b0580>
hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
kwargs = {'item': <Function test_gpu_coordinate_regularised>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
kwargs = {'item': <Function test_gpu_coordinate_regularised>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
caller_kwargs = {'item': <Function test_gpu_coordinate_regularised>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fa925c8e0a0>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
caller_kwargs = {'item': <Function test_gpu_coordinate_regularised>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_gpu_coordinate_regularised>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
            item.runtest()
        except Exception as e:
            # Store trace info to allow postmortem debugging
            sys.last_type = type(e)
            sys.last_value = e
            assert e.__traceback__ is not None
            # Skip *this* frame
            sys.last_traceback = e.__traceback__.tb_next
>           raise e

.venv/lib/python3.8/site-packages/_pytest/runner.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_gpu_coordinate_regularised>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
>           item.runtest()

.venv/lib/python3.8/site-packages/_pytest/runner.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Function test_gpu_coordinate_regularised>

    def runtest(self) -> None:
        """Execute the underlying test function."""
>       self.ihook.pytest_pyfunc_call(pyfuncitem=self)

.venv/lib/python3.8/site-packages/_pytest/python.py:1627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_pyfunc_call'>, args = ()
kwargs = {'pyfuncitem': <Function test_gpu_coordinate_regularised>}
notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fa94e3b0580>
hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_gpu_coordinate_regularised>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_gpu_coordinate_regularised>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_gpu_coordinate_regularised>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fa925c8e580>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_gpu_coordinate_regularised>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pyfuncitem = <Function test_gpu_coordinate_regularised>

    @hookimpl(trylast=True)
    def pytest_pyfunc_call(pyfuncitem: "Function") -> Optional[object]:
        testfunction = pyfuncitem.obj
        if is_async_function(testfunction):
            async_warn_and_skip(pyfuncitem.nodeid)
        funcargs = pyfuncitem.funcargs
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
>       result = testfunction(**testargs)

.venv/lib/python3.8/site-packages/_pytest/python.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_linear.TestGPULinear object at 0x7fa925c8e040>

>   ???

tests/python-gpu/test_gpu_linear.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arguments = ()
kwargs = {'self': <test_gpu_linear.TestGPULinear object at 0x7fa925c8e040>}
__tracebackhide__ = True
test = <function TestGPULinear.test_gpu_coordinate_regularised at 0x7fa927cd4820>
settings = settings(database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples'), deadline=None, derand...nt_blob=False, report_multiple_bugs=True, stateful_step_count=50, suppress_health_check=(), verbosity=Verbosity.normal)
random = <random.Random object at 0x2937b00>
processed_args = ((), {'self': <test_gpu_linear.TestGPULinear object at 0x7fa925c8e040>}, <function default_new_style_executor at 0x7fa...lue=10)})}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_linear.TestGPULinear object at 0x7fa925c8e040>))
test_runner = <function default_new_style_executor at 0x7fa94d2a94c0>
search_strategy = WithRunner(TupleStrategy((just(()), fixed_dictionaries({'alpha': floats(min_value=1e-05, max_value=2.0),
 'dataset': _...alue=10)})}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_linear.TestGPULinear object at 0x7fa925c8e040>)
runner = <test_gpu_linear.TestGPULinear object at 0x7fa925c8e040>
state = <hypothesis.core.StateForActualGivenExecution object at 0x7fa925d6ab50>

    @impersonate(test)
    @define_function_signature(test.__name__, test.__doc__, argspec)
    def wrapped_test(*arguments, **kwargs):
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
    
        test = wrapped_test.hypothesis.inner_test
    
        if getattr(test, "is_hypothesis_test", False):
            raise InvalidArgument(
                (
                    "You have applied @given to the test %s more than once, which "
                    "wraps the test several times and is extremely slow. A "
                    "similar effect can be gained by combining the arguments "
                    "of the two calls to given. For example, instead of "
                    "@given(booleans()) @given(integers()), you could write "
                    "@given(booleans(), integers())"
                )
                % (test.__name__,)
            )
    
        settings = wrapped_test._hypothesis_internal_use_settings
    
        random = get_random_for_wrapped_test(test, wrapped_test)
    
        processed_args = process_arguments_to_given(
            wrapped_test, arguments, kwargs, given_kwargs, argspec
        )
        arguments, kwargs, test_runner, search_strategy = processed_args
    
        runner = getattr(search_strategy, "runner", None)
        if isinstance(runner, TestCase) and test.__name__ in dir(TestCase):
            msg = (
                "You have applied @given to the method %s, which is "
                "used by the unittest runner but is not itself a test."
                "  This is not useful in any way." % test.__name__
            )
            fail_health_check(settings, msg, HealthCheck.not_a_test_method)
        if bad_django_TestCase(runner):  # pragma: no cover
            # Covered by the Django tests, but not the pytest coverage task
            raise InvalidArgument(
                "You have applied @given to a method on %s, but this "
                "class does not inherit from the supported versions in "
                "`hypothesis.extra.django`.  Use the Hypothesis variants "
                "to ensure that each example is run in a separate "
                "database transaction." % qualname(type(runner))
            )
    
        state = StateForActualGivenExecution(
            test_runner, search_strategy, test, settings, random, wrapped_test
        )
    
        reproduce_failure = wrapped_test._hypothesis_internal_use_reproduce_failure
    
        # If there was a @reproduce_failure decorator, use it to reproduce
        # the error (or complain that we couldn't). Either way, this will
        # always raise some kind of error.
        if reproduce_failure is not None:
            expected_version, failure = reproduce_failure
            if expected_version != __version__:
                raise InvalidArgument(
                    (
                        "Attempting to reproduce a failure from a different "
                        "version of Hypothesis. This failure is from %s, but "
                        "you are currently running %r. Please change your "
                        "Hypothesis version to a matching one."
                    )
                    % (expected_version, __version__)
                )
            try:
                state.execute_once(
                    ConjectureData.for_buffer(decode_failure(failure)),
                    print_example=True,
                    is_final=True,
                )
                raise DidNotReproduce(
                    "Expected the test to raise an error, but it "
                    "completed successfully."
                )
            except StopTest:
                raise DidNotReproduce(
                    "The shape of the test data has changed in some way "
                    "from where this blob was defined. Are you sure "
                    "you're running the same test?"
                )
            except UnsatisfiedAssumption:
                raise DidNotReproduce(
                    "The test data failed to satisfy an assumption in the "
                    "test. Have you added it since this blob was "
                    "generated?"
                )
    
        # There was no @reproduce_failure, so start by running any explicit
        # examples from @example decorators.
        errors = list(
            execute_explicit_examples(state, wrapped_test, arguments, kwargs)
        )
        with local_settings(state.settings):
            if len(errors) > 1:
                # If we're not going to report multiple bugs, we would have
                # stopped running explicit examples at the first failure.
                assert state.settings.report_multiple_bugs
                for fragments, err in errors:
                    for f in fragments:
                        report(f)
                    tb_lines = traceback.format_exception(
                        type(err), err, err.__traceback__
                    )
                    report("".join(tb_lines))
                msg = "Hypothesis found %d failures in explicit examples."
                raise MultipleFailures(msg % (len(errors)))
            elif errors:
                fragments, the_error_hypothesis_found = errors[0]
                for f in fragments:
                    report(f)
                raise the_error_hypothesis_found
    
        # If there were any explicit examples, they all ran successfully.
        # The next step is to use the Conjecture engine to run the test on
        # many different inputs.
    
        if not (
            Phase.reuse in settings.phases or Phase.generate in settings.phases
        ):
            return
    
        try:
            if isinstance(runner, TestCase) and hasattr(runner, "subTest"):
                subTest = runner.subTest
                try:
                    runner.subTest = types.MethodType(fake_subTest, runner)
                    state.run_engine()
                finally:
                    runner.subTest = subTest
            else:
                state.run_engine()
        except BaseException as e:
            # The exception caught here should either be an actual test
            # failure (or MultipleFailures), or some kind of fatal error
            # that caused the engine to stop.
    
            generated_seed = wrapped_test._hypothesis_internal_use_generated_seed
            with local_settings(settings):
                if not (state.failed_normally or generated_seed is None):
                    if running_under_pytest:
                        report(
                            "You can add @seed(%(seed)d) to this test or "
                            "run pytest with --hypothesis-seed=%(seed)d "
                            "to reproduce this failure." % {"seed": generated_seed}
                        )
                    else:
                        report(
                            "You can add @seed(%d) to this test to "
                            "reproduce this failure." % (generated_seed,)
                        )
                # The dance here is to avoid showing users long tracebacks
                # full of Hypothesis internals they don't care about.
                # We have to do this inline, to avoid adding another
                # internal stack frame just when we've removed the rest.
                #
                # Using a variable for our trimmed error ensures that the line
                # which will actually appear in tracebacks is as clear as
                # possible - "raise the_error_hypothesis_found".
                the_error_hypothesis_found = e.with_traceback(
                    get_trimmed_traceback()
                )
>               raise the_error_hypothesis_found

.venv/lib/python3.8/site-packages/hypothesis/core.py:1162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_linear.TestGPULinear object at 0x7fa925c8e040>
param = {'alpha': 1e-05, 'booster': 'gblinear', 'eta': 0.01, 'eval_metric': 'rmse', ...}
num_rounds = 10, dataset = boston, alpha = 1e-05, lambd = 1e-05

    @given(parameter_strategy, strategies.integers(10, 50),
           tm.dataset_strategy, strategies.floats(1e-5, 2.0),
           strategies.floats(1e-5, 2.0))
    @settings(deadline=None)
    def test_gpu_coordinate_regularised(self, param, num_rounds, dataset, alpha, lambd):
        assume(len(dataset.y) > 0)
        param['updater'] = 'gpu_coord_descent'
        param['alpha'] = alpha
        param['lambda'] = lambd
        param = dataset.set_params(param)
>       result = train_result(param, dataset.get_dmat(), num_rounds)['train'][dataset.metric]

tests/python-gpu/test_gpu_linear.py:49: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

param = {'alpha': 1e-05, 'booster': 'gblinear', 'eta': 0.01, 'eval_metric': 'rmse', ...}
dmat = <xgboost.core.DMatrix object at 0x7fa925d36280>, num_rounds = 10

    def train_result(param, dmat, num_rounds):
        result = {}
>       xgb.train(param, dmat, num_rounds, [(dmat, 'train')], verbose_eval=False,
                  evals_result=result)

tests/python-gpu/test_gpu_linear.py:20: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'alpha': 1e-05, 'booster': 'gblinear', 'eta': 0.01, 'eval_metric': 'rmse', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925d36280>, num_boost_round = 10
evals = [(<xgboost.core.DMatrix object at 0x7fa925d36280>, 'train')], obj = None
feval = None, maximize = None, early_stopping_rounds = None, evals_result = {}
verbose_eval = False, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'alpha': 1e-05, 'booster': 'gblinear', 'eta': 0.01, 'eval_metric': 'rmse', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925d36280>, num_boost_round = 10
evals = [(<xgboost.core.DMatrix object at 0x7fa925d36280>, 'train')], obj = None
feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925d36d90>
evals_result = {}, maximize = None, verbose_eval = False
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925d361c0>
dtrain = <xgboost.core.DMatrix object at 0x7fa925d36280>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:55:42] /home/otivedani/xgboost/src/linear/linear_updater.cc:17: Unknown linear updater gpu_coord_descent
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LinearUpdater::Create(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, xgboost::GenericParameter const*)+0x403) [0x7fa92b8951b3]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBLinear::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x252) [0x7fa92b849632]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x137c0) [0x7fa94d18f7c0]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
---------------------------------- Hypothesis ----------------------------------
Falsifying example: test_gpu_coordinate_regularised(
    self=<test_gpu_linear.TestGPULinear at 0x7fa925c8e040>,
    param={'booster': 'gblinear',
     'eta': 0.01,
     'feature_selector': 'cyclic',
     'nthread': 1,
     'tolerance': 1e-05,
     'top_k': 1},
    num_rounds=10,
    dataset=boston,
    alpha=1e-05,
    lambd=1e-05,
)
_____________________ TestPickling.test_pickled_predictor ______________________

self = <unittest.case._Outcome object at 0x7fa925d4cb50>
test_case = <test_gpu_pickling.TestPickling testMethod=test_pickled_predictor>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_pickled_predictor>
result = <TestCaseFunction test_pickled_predictor>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_pickled_predictor>
method = <bound method TestPickling.test_pickled_predictor of <test_gpu_pickling.TestPickling testMethod=test_pickled_predictor>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_pickled_predictor>

    def test_pickled_predictor(self):
        x, y = build_dataset()
        train_x = xgb.DMatrix(x, label=y)
    
        param = {'tree_method': 'gpu_hist',
                 'verbosity': 1, 'predictor': 'gpu_predictor'}
>       bst = xgb.train(param, train_x)

tests/python-gpu/test_gpu_pickling.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'predictor': 'gpu_predictor', 'tree_method': 'gpu_hist', 'verbosity': 1}
dtrain = <xgboost.core.DMatrix object at 0x7fa925d4c2b0>, num_boost_round = 10
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'predictor': 'gpu_predictor', 'tree_method': 'gpu_hist', 'validate_parameters': True, 'verbosity': 1}
dtrain = <xgboost.core.DMatrix object at 0x7fa925d4c2b0>, num_boost_round = 10
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925d4c640>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925d4c280>
dtrain = <xgboost.core.DMatrix object at 0x7fa925d4c2b0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:55:42] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
__________________________ TestPickling.test_pickling __________________________

self = <unittest.case._Outcome object at 0x7fa925a30460>
test_case = <test_gpu_pickling.TestPickling testMethod=test_pickling>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_pickling>
result = <TestCaseFunction test_pickling>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_pickling>
method = <bound method TestPickling.test_pickling of <test_gpu_pickling.TestPickling testMethod=test_pickling>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_pickling>

    def test_pickling(self):
        x, y = build_dataset()
        train_x = xgb.DMatrix(x, label=y)
        param = {'tree_method': 'gpu_hist',
                 'verbosity': 1}
>       bst = xgb.train(param, train_x)

tests/python-gpu/test_gpu_pickling.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'tree_method': 'gpu_hist', 'verbosity': 1}
dtrain = <xgboost.core.DMatrix object at 0x7fa925a301c0>, num_boost_round = 10
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'tree_method': 'gpu_hist', 'validate_parameters': True, 'verbosity': 1}
dtrain = <xgboost.core.DMatrix object at 0x7fa925a301c0>, num_boost_round = 10
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925a301f0>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925a304f0>
dtrain = <xgboost.core.DMatrix object at 0x7fa925a301c0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:55:42] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
___________________ TestPickling.test_predict_sklearn_pickle ___________________

self = <unittest.case._Outcome object at 0x7fa925f06820>
test_case = <test_gpu_pickling.TestPickling testMethod=test_predict_sklearn_pickle>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_predict_sklearn_pickle>
result = <TestCaseFunction test_predict_sklearn_pickle>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_predict_sklearn_pickle>
method = <bound method TestPickling.test_predict_sklearn_pickle of <test_gpu_pickling.TestPickling testMethod=test_predict_sklearn_pickle>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_predict_sklearn_pickle>

    def test_predict_sklearn_pickle(self):
        x, y = build_dataset()
    
        kwargs = {'tree_method': 'gpu_hist',
                  'predictor': 'gpu_predictor',
                  'objective': 'binary:logistic',
                  'n_estimators': 10}
    
        model = XGBClassifier(**kwargs)
>       model.fit(x, y)

tests/python-gpu/test_gpu_pickling.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_by... scale_pos_weight=None, subsample=None,
              tree_method='gpu_hist', validate_parameters=None, verbosity=None)
X = array([[  0.        ,   1.01010101,   2.02020202,   3.03030303,
          4.04040404,   5.05050505,   6.06060606,   7....93,  93.93939394,
         94.94949495,  95.95959596,  96.96969697,  97.97979798,
         98.98989899, 100.        ]])
y = array([ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,
        5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ])
sample_weight = None, base_margin = None, eval_set = None, eval_metric = None
early_stopping_rounds = None, verbose = True, xgb_model = None
sample_weight_eval_set = None, feature_weights = None, callbacks = None

    def fit(self, X, y, sample_weight=None, base_margin=None,
            eval_set=None, eval_metric=None,
            early_stopping_rounds=None, verbose=True, xgb_model=None,
            sample_weight_eval_set=None, feature_weights=None, callbacks=None):
        # pylint: disable = attribute-defined-outside-init,arguments-differ
    
        evals_result = {}
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
    
        xgb_options = self.get_xgb_params()
    
        if callable(self.objective):
            obj = _objective_decorator(self.objective)
            # Use default value. Is it really not used ?
            xgb_options["objective"] = "binary:logistic"
        else:
            obj = None
    
        if self.n_classes_ > 2:
            # Switch to using a multiclass objective in the underlying
            # XGB instance
            xgb_options['objective'] = 'multi:softprob'
            xgb_options['num_class'] = self.n_classes_
    
        feval = eval_metric if callable(eval_metric) else None
        if eval_metric is not None:
            if callable(eval_metric):
                eval_metric = None
            else:
                xgb_options.update({"eval_metric": eval_metric})
    
        self._le = XGBoostLabelEncoder().fit(y)
        training_labels = self._le.transform(y)
    
        if eval_set is not None:
            if sample_weight_eval_set is None:
                sample_weight_eval_set = [None] * len(eval_set)
            else:
                assert len(sample_weight_eval_set) == len(eval_set)
            evals = list(
                DMatrix(eval_set[i][0],
                        label=self._le.transform(eval_set[i][1]),
                        missing=self.missing, weight=sample_weight_eval_set[i],
                        nthread=self.n_jobs)
                for i in range(len(eval_set))
            )
            nevals = len(evals)
            eval_names = ["validation_{}".format(i) for i in range(nevals)]
            evals = list(zip(evals, eval_names))
        else:
            evals = ()
    
        if len(X.shape) != 2:
            # Simply raise an error here since there might be many
            # different ways of reshaping
            raise ValueError(
                'Please reshape the input data X into 2-dimensional matrix.')
    
        self._features_count = X.shape[1]
        self.n_features_in_ = self._features_count
    
        train_dmatrix = DMatrix(X, label=training_labels, weight=sample_weight,
                                base_margin=base_margin,
                                missing=self.missing, nthread=self.n_jobs)
        train_dmatrix.set_info(feature_weights=feature_weights)
    
>       self._Booster = train(xgb_options, train_dmatrix,
                              self.get_num_boosting_rounds(),
                              evals=evals,
                              early_stopping_rounds=early_stopping_rounds,
                              evals_result=evals_result, obj=obj, feval=feval,
                              verbose_eval=verbose, xgb_model=xgb_model,
                              callbacks=callbacks)

.venv/lib/python3.8/site-packages/xgboost/sklearn.py:836: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925f06970>, num_boost_round = 10
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = {}, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925f06970>, num_boost_round = 10
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925f06bb0>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925f06af0>
dtrain = <xgboost.core.DMatrix object at 0x7fa925f06970>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:55:42] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
__________________ TestPickling.test_training_on_cpu_only_env __________________

self = <unittest.case._Outcome object at 0x7fa925d4c6a0>
test_case = <test_gpu_pickling.TestPickling testMethod=test_training_on_cpu_only_env>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_training_on_cpu_only_env>
result = <TestCaseFunction test_training_on_cpu_only_env>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_training_on_cpu_only_env>
method = <bound method TestPickling.test_training_on_cpu_only_env of <test_gpu_pickling.TestPickling testMethod=test_training_on_cpu_only_env>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_training_on_cpu_only_env>

    def test_training_on_cpu_only_env(self):
        cuda_environment = {'CUDA_VISIBLE_DEVICES': '-1'}
        env = os.environ.copy()
        env.update(cuda_environment)
        args = self.args_template.copy()
        args.append(
            "./tests/python-gpu/"
            "load_pickle.py::TestLoadPickle::test_training_on_cpu_only_env")
        status = subprocess.call(args, env=env)
>       assert status == 0
E       AssertionError: assert 1 == 0
E         +1
E         -0

tests/python-gpu/test_gpu_pickling.py:173: AssertionError
________________________ TestPickling.test_wrap_gpu_id _________________________

self = <unittest.case._Outcome object at 0x7fa926006ac0>
test_case = <test_gpu_pickling.TestPickling testMethod=test_wrap_gpu_id>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_wrap_gpu_id>
result = <TestCaseFunction test_wrap_gpu_id>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_wrap_gpu_id>
method = <bound method TestPickling.test_wrap_gpu_id of <test_gpu_pickling.TestPickling testMethod=test_wrap_gpu_id>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_wrap_gpu_id>

    @pytest.mark.mgpu
    def test_wrap_gpu_id(self):
        X, y = build_dataset()
        dtrain = xgb.DMatrix(X, y)
    
>       bst = xgb.train({'tree_method': 'gpu_hist',
                         'gpu_id': 1},
                        dtrain, num_boost_round=6)

tests/python-gpu/test_gpu_pickling.py:81: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'gpu_id': 1, 'tree_method': 'gpu_hist'}
dtrain = <xgboost.core.DMatrix object at 0x7fa926006b20>, num_boost_round = 6
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'gpu_id': 1, 'tree_method': 'gpu_hist', 'validate_parameters': True}
dtrain = <xgboost.core.DMatrix object at 0x7fa926006b20>, num_boost_round = 6
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa926006be0>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa926006b50>
dtrain = <xgboost.core.DMatrix object at 0x7fa926006b20>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:55:43] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
______________________ TestGPUPredict.test_multi_predict _______________________

self = <unittest.case._Outcome object at 0x7fa925feea30>
test_case = <test_gpu_prediction.TestGPUPredict testMethod=test_multi_predict>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_multi_predict>
result = <TestCaseFunction test_multi_predict>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_multi_predict>
method = <bound method TestGPUPredict.test_multi_predict of <test_gpu_prediction.TestGPUPredict testMethod=test_multi_predict>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_multi_predict>

    @pytest.mark.skipif(**tm.no_sklearn())
    def test_multi_predict(self):
        from sklearn.datasets import make_regression
        from sklearn.model_selection import train_test_split
    
        n = 1000
        X, y = make_regression(n, random_state=rng)
        X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                            random_state=123)
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dtest = xgb.DMatrix(X_test)
    
        params = {}
        params["tree_method"] = "gpu_hist"
    
        params['predictor'] = "gpu_predictor"
>       bst_gpu_predict = xgb.train(params, dtrain)

tests/python-gpu/test_gpu_prediction.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'predictor': 'gpu_predictor', 'tree_method': 'gpu_hist'}
dtrain = <xgboost.core.DMatrix object at 0x7fa925feebe0>, num_boost_round = 10
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'predictor': 'gpu_predictor', 'tree_method': 'gpu_hist', 'validate_parameters': True}
dtrain = <xgboost.core.DMatrix object at 0x7fa925feebe0>, num_boost_round = 10
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925feeaf0>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925feeb80>
dtrain = <xgboost.core.DMatrix object at 0x7fa925feebe0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:55:43] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
_________________________ TestGPUPredict.test_predict __________________________

self = <unittest.case._Outcome object at 0x7fa925eff9a0>
test_case = <test_gpu_prediction.TestGPUPredict testMethod=test_predict>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_predict>
result = <TestCaseFunction test_predict>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_predict>
method = <bound method TestGPUPredict.test_predict of <test_gpu_prediction.TestGPUPredict testMethod=test_predict>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_predict>

    def test_predict(self):
        iterations = 10
        np.random.seed(1)
        test_num_rows = [10, 1000, 5000]
        test_num_cols = [10, 50, 500]
        # This test passes for tree_method=gpu_hist and tree_method=exact. but
        # for `hist` and `approx` the floating point error accumulates faster
        # and fails even tol is set to 1e-4.  For `hist`, the mismatching rate
        # with 5000 rows is 0.04.
        for num_rows in test_num_rows:
            for num_cols in test_num_cols:
                dtrain = xgb.DMatrix(np.random.randn(num_rows, num_cols),
                                     label=[0, 1] * int(num_rows / 2))
                dval = xgb.DMatrix(np.random.randn(num_rows, num_cols),
                                   label=[0, 1] * int(num_rows / 2))
                dtest = xgb.DMatrix(np.random.randn(num_rows, num_cols),
                                    label=[0, 1] * int(num_rows / 2))
                watchlist = [(dtrain, 'train'), (dval, 'validation')]
                res = {}
                param = {
                    "objective": "binary:logistic",
                    "predictor": "gpu_predictor",
                    'eval_metric': 'logloss',
                    'tree_method': 'gpu_hist',
                    'max_depth': 1
                }
>               bst = xgb.train(param, dtrain, iterations, evals=watchlist,
                                evals_result=res)

tests/python-gpu/test_gpu_prediction.py:49: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eval_metric': 'logloss', 'max_depth': 1, 'objective': 'binary:logistic', 'predictor': 'gpu_predictor', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925effaf0>, num_boost_round = 10
evals = [(<xgboost.core.DMatrix object at 0x7fa925effaf0>, 'train'), (<xgboost.core.DMatrix object at 0x7fa925effac0>, 'validation')]
obj = None, feval = None, maximize = None, early_stopping_rounds = None
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eval_metric': 'logloss', 'max_depth': 1, 'objective': 'binary:logistic', 'predictor': 'gpu_predictor', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925effaf0>, num_boost_round = 10
evals = [(<xgboost.core.DMatrix object at 0x7fa925effaf0>, 'train'), (<xgboost.core.DMatrix object at 0x7fa925effac0>, 'validation')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925effbe0>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925effa60>
dtrain = <xgboost.core.DMatrix object at 0x7fa925effaf0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:55:43] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
___________________________ TestGPUPredict.test_shap ___________________________

self = <unittest.case._Outcome object at 0x7fa93bcdc910>
test_case = <test_gpu_prediction.TestGPUPredict testMethod=test_shap>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_shap>
result = <TestCaseFunction test_shap>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_shap>
method = <bound method accept.<locals>.test_shap of <test_gpu_prediction.TestGPUPredict testMethod=test_shap>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_shap>

>   ???

tests/python-gpu/test_gpu_prediction.py:198: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arguments = ()
kwargs = {'self': <test_gpu_prediction.TestGPUPredict testMethod=test_shap>}
__tracebackhide__ = True
test = <function TestGPUPredict.test_shap at 0x7fa927cfadc0>
settings = settings(database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples'), deadline=None, derand...nt_blob=False, report_multiple_bugs=True, stateful_step_count=50, suppress_health_check=(), verbosity=Verbosity.normal)
random = <random.Random object at 0x2996c40>
processed_args = ((), {'self': <test_gpu_prediction.TestGPUPredict testMethod=test_shap>}, <function default_new_style_executor at 0x7f...nknown>)}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_prediction.TestGPUPredict testMethod=test_shap>))
test_runner = <function default_new_style_executor at 0x7fa94d2a94c0>
search_strategy = WithRunner(TupleStrategy((just(()), fixed_dictionaries({'dataset': _dataset_weight_margin(),
 'num_rounds': integers(m...unknown>)}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_prediction.TestGPUPredict testMethod=test_shap>)
runner = <test_gpu_prediction.TestGPUPredict testMethod=test_shap>
state = <hypothesis.core.StateForActualGivenExecution object at 0x7fa925e4eee0>

    @impersonate(test)
    @define_function_signature(test.__name__, test.__doc__, argspec)
    def wrapped_test(*arguments, **kwargs):
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
    
        test = wrapped_test.hypothesis.inner_test
    
        if getattr(test, "is_hypothesis_test", False):
            raise InvalidArgument(
                (
                    "You have applied @given to the test %s more than once, which "
                    "wraps the test several times and is extremely slow. A "
                    "similar effect can be gained by combining the arguments "
                    "of the two calls to given. For example, instead of "
                    "@given(booleans()) @given(integers()), you could write "
                    "@given(booleans(), integers())"
                )
                % (test.__name__,)
            )
    
        settings = wrapped_test._hypothesis_internal_use_settings
    
        random = get_random_for_wrapped_test(test, wrapped_test)
    
        processed_args = process_arguments_to_given(
            wrapped_test, arguments, kwargs, given_kwargs, argspec
        )
        arguments, kwargs, test_runner, search_strategy = processed_args
    
        runner = getattr(search_strategy, "runner", None)
        if isinstance(runner, TestCase) and test.__name__ in dir(TestCase):
            msg = (
                "You have applied @given to the method %s, which is "
                "used by the unittest runner but is not itself a test."
                "  This is not useful in any way." % test.__name__
            )
            fail_health_check(settings, msg, HealthCheck.not_a_test_method)
        if bad_django_TestCase(runner):  # pragma: no cover
            # Covered by the Django tests, but not the pytest coverage task
            raise InvalidArgument(
                "You have applied @given to a method on %s, but this "
                "class does not inherit from the supported versions in "
                "`hypothesis.extra.django`.  Use the Hypothesis variants "
                "to ensure that each example is run in a separate "
                "database transaction." % qualname(type(runner))
            )
    
        state = StateForActualGivenExecution(
            test_runner, search_strategy, test, settings, random, wrapped_test
        )
    
        reproduce_failure = wrapped_test._hypothesis_internal_use_reproduce_failure
    
        # If there was a @reproduce_failure decorator, use it to reproduce
        # the error (or complain that we couldn't). Either way, this will
        # always raise some kind of error.
        if reproduce_failure is not None:
            expected_version, failure = reproduce_failure
            if expected_version != __version__:
                raise InvalidArgument(
                    (
                        "Attempting to reproduce a failure from a different "
                        "version of Hypothesis. This failure is from %s, but "
                        "you are currently running %r. Please change your "
                        "Hypothesis version to a matching one."
                    )
                    % (expected_version, __version__)
                )
            try:
                state.execute_once(
                    ConjectureData.for_buffer(decode_failure(failure)),
                    print_example=True,
                    is_final=True,
                )
                raise DidNotReproduce(
                    "Expected the test to raise an error, but it "
                    "completed successfully."
                )
            except StopTest:
                raise DidNotReproduce(
                    "The shape of the test data has changed in some way "
                    "from where this blob was defined. Are you sure "
                    "you're running the same test?"
                )
            except UnsatisfiedAssumption:
                raise DidNotReproduce(
                    "The test data failed to satisfy an assumption in the "
                    "test. Have you added it since this blob was "
                    "generated?"
                )
    
        # There was no @reproduce_failure, so start by running any explicit
        # examples from @example decorators.
        errors = list(
            execute_explicit_examples(state, wrapped_test, arguments, kwargs)
        )
        with local_settings(state.settings):
            if len(errors) > 1:
                # If we're not going to report multiple bugs, we would have
                # stopped running explicit examples at the first failure.
                assert state.settings.report_multiple_bugs
                for fragments, err in errors:
                    for f in fragments:
                        report(f)
                    tb_lines = traceback.format_exception(
                        type(err), err, err.__traceback__
                    )
                    report("".join(tb_lines))
                msg = "Hypothesis found %d failures in explicit examples."
                raise MultipleFailures(msg % (len(errors)))
            elif errors:
                fragments, the_error_hypothesis_found = errors[0]
                for f in fragments:
                    report(f)
                raise the_error_hypothesis_found
    
        # If there were any explicit examples, they all ran successfully.
        # The next step is to use the Conjecture engine to run the test on
        # many different inputs.
    
        if not (
            Phase.reuse in settings.phases or Phase.generate in settings.phases
        ):
            return
    
        try:
            if isinstance(runner, TestCase) and hasattr(runner, "subTest"):
                subTest = runner.subTest
                try:
                    runner.subTest = types.MethodType(fake_subTest, runner)
                    state.run_engine()
                finally:
                    runner.subTest = subTest
            else:
                state.run_engine()
        except BaseException as e:
            # The exception caught here should either be an actual test
            # failure (or MultipleFailures), or some kind of fatal error
            # that caused the engine to stop.
    
            generated_seed = wrapped_test._hypothesis_internal_use_generated_seed
            with local_settings(settings):
                if not (state.failed_normally or generated_seed is None):
                    if running_under_pytest:
                        report(
                            "You can add @seed(%(seed)d) to this test or "
                            "run pytest with --hypothesis-seed=%(seed)d "
                            "to reproduce this failure." % {"seed": generated_seed}
                        )
                    else:
                        report(
                            "You can add @seed(%d) to this test to "
                            "reproduce this failure." % (generated_seed,)
                        )
                # The dance here is to avoid showing users long tracebacks
                # full of Hypothesis internals they don't care about.
                # We have to do this inline, to avoid adding another
                # internal stack frame just when we've removed the rest.
                #
                # Using a variable for our trimmed error ensures that the line
                # which will actually appear in tracebacks is as clear as
                # possible - "raise the_error_hypothesis_found".
                the_error_hypothesis_found = e.with_traceback(
                    get_trimmed_traceback()
                )
>               raise the_error_hypothesis_found

.venv/lib/python3.8/site-packages/hypothesis/core.py:1162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_shap>, num_rounds = 1
dataset = boston
param = {'eval_metric': 'rmse', 'gpu_id': 0, 'max_depth': 0, 'max_leaves': 1, ...}

    @given(strategies.integers(1, 10),
           tm.dataset_strategy, shap_parameter_strategy)
    @settings(deadline=None)
    def test_shap(self, num_rounds, dataset, param):
        param.update({"predictor": "gpu_predictor", "gpu_id": 0})
        param = dataset.set_params(param)
        dmat = dataset.get_dmat()
>       bst = xgb.train(param, dmat, num_rounds)

tests/python-gpu/test_gpu_prediction.py:204: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eval_metric': 'rmse', 'gpu_id': 0, 'max_depth': 0, 'max_leaves': 1, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925f3b250>, num_boost_round = 1
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eval_metric': 'rmse', 'gpu_id': 0, 'max_depth': 0, 'max_leaves': 1, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925f3b250>, num_boost_round = 1
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925f3b280>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925f3bd30>
dtrain = <xgboost.core.DMatrix object at 0x7fa925f3b250>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:55:43] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::GetPredictor(xgboost::HostDeviceVector<float> const*, xgboost::DMatrix*) const+0x225) [0x7fa92b84ec35]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::PredictBatch(xgboost::DMatrix*, xgboost::PredictionCacheEntry*, bool, unsigned int)+0x49) [0x7fa92b84ef29]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x3b2) [0x7fa92b87ed42]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x137c0) [0x7fa94d18f7c0]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
---------------------------------- Hypothesis ----------------------------------
Falsifying example: test_shap(
    self=<test_gpu_prediction.TestGPUPredict testMethod=test_shap>,
    num_rounds=1,
    dataset=boston,
    param={'max_depth': 0, 'max_leaves': 1, 'num_parallel_tree': 1},
)
____________________ TestGPUPredict.test_shap_interactions _____________________

self = <unittest.case._Outcome object at 0x7fa925da03d0>
test_case = <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>
result = <TestCaseFunction test_shap_interactions>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>
method = <bound method accept.<locals>.test_shap_interactions of <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>

>   ???

tests/python-gpu/test_gpu_prediction.py:212: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arguments = ()
kwargs = {'self': <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>}
__tracebackhide__ = True
test = <function TestGPUPredict.test_shap_interactions at 0x7fa927cfaee0>
settings = settings(database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples'), deadline=None, derand...nt_blob=False, report_multiple_bugs=True, stateful_step_count=50, suppress_health_check=(), verbosity=Verbosity.normal)
random = <random.Random object at 0x281bb30>
processed_args = ((), {'self': <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>}, <function default_new_style_exe...p(lambda args: dict(args, **kwargs)))), runner=<test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>))
test_runner = <function default_new_style_executor at 0x7fa94d2a94c0>
search_strategy = WithRunner(TupleStrategy((just(()), fixed_dictionaries({'dataset': _dataset_weight_margin(),
 'num_rounds': integers(m...ap(lambda args: dict(args, **kwargs)))), runner=<test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>)
runner = <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>
state = <hypothesis.core.StateForActualGivenExecution object at 0x7fa926024d30>

    @impersonate(test)
    @define_function_signature(test.__name__, test.__doc__, argspec)
    def wrapped_test(*arguments, **kwargs):
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
    
        test = wrapped_test.hypothesis.inner_test
    
        if getattr(test, "is_hypothesis_test", False):
            raise InvalidArgument(
                (
                    "You have applied @given to the test %s more than once, which "
                    "wraps the test several times and is extremely slow. A "
                    "similar effect can be gained by combining the arguments "
                    "of the two calls to given. For example, instead of "
                    "@given(booleans()) @given(integers()), you could write "
                    "@given(booleans(), integers())"
                )
                % (test.__name__,)
            )
    
        settings = wrapped_test._hypothesis_internal_use_settings
    
        random = get_random_for_wrapped_test(test, wrapped_test)
    
        processed_args = process_arguments_to_given(
            wrapped_test, arguments, kwargs, given_kwargs, argspec
        )
        arguments, kwargs, test_runner, search_strategy = processed_args
    
        runner = getattr(search_strategy, "runner", None)
        if isinstance(runner, TestCase) and test.__name__ in dir(TestCase):
            msg = (
                "You have applied @given to the method %s, which is "
                "used by the unittest runner but is not itself a test."
                "  This is not useful in any way." % test.__name__
            )
            fail_health_check(settings, msg, HealthCheck.not_a_test_method)
        if bad_django_TestCase(runner):  # pragma: no cover
            # Covered by the Django tests, but not the pytest coverage task
            raise InvalidArgument(
                "You have applied @given to a method on %s, but this "
                "class does not inherit from the supported versions in "
                "`hypothesis.extra.django`.  Use the Hypothesis variants "
                "to ensure that each example is run in a separate "
                "database transaction." % qualname(type(runner))
            )
    
        state = StateForActualGivenExecution(
            test_runner, search_strategy, test, settings, random, wrapped_test
        )
    
        reproduce_failure = wrapped_test._hypothesis_internal_use_reproduce_failure
    
        # If there was a @reproduce_failure decorator, use it to reproduce
        # the error (or complain that we couldn't). Either way, this will
        # always raise some kind of error.
        if reproduce_failure is not None:
            expected_version, failure = reproduce_failure
            if expected_version != __version__:
                raise InvalidArgument(
                    (
                        "Attempting to reproduce a failure from a different "
                        "version of Hypothesis. This failure is from %s, but "
                        "you are currently running %r. Please change your "
                        "Hypothesis version to a matching one."
                    )
                    % (expected_version, __version__)
                )
            try:
                state.execute_once(
                    ConjectureData.for_buffer(decode_failure(failure)),
                    print_example=True,
                    is_final=True,
                )
                raise DidNotReproduce(
                    "Expected the test to raise an error, but it "
                    "completed successfully."
                )
            except StopTest:
                raise DidNotReproduce(
                    "The shape of the test data has changed in some way "
                    "from where this blob was defined. Are you sure "
                    "you're running the same test?"
                )
            except UnsatisfiedAssumption:
                raise DidNotReproduce(
                    "The test data failed to satisfy an assumption in the "
                    "test. Have you added it since this blob was "
                    "generated?"
                )
    
        # There was no @reproduce_failure, so start by running any explicit
        # examples from @example decorators.
        errors = list(
            execute_explicit_examples(state, wrapped_test, arguments, kwargs)
        )
        with local_settings(state.settings):
            if len(errors) > 1:
                # If we're not going to report multiple bugs, we would have
                # stopped running explicit examples at the first failure.
                assert state.settings.report_multiple_bugs
                for fragments, err in errors:
                    for f in fragments:
                        report(f)
                    tb_lines = traceback.format_exception(
                        type(err), err, err.__traceback__
                    )
                    report("".join(tb_lines))
                msg = "Hypothesis found %d failures in explicit examples."
                raise MultipleFailures(msg % (len(errors)))
            elif errors:
                fragments, the_error_hypothesis_found = errors[0]
                for f in fragments:
                    report(f)
                raise the_error_hypothesis_found
    
        # If there were any explicit examples, they all ran successfully.
        # The next step is to use the Conjecture engine to run the test on
        # many different inputs.
    
        if not (
            Phase.reuse in settings.phases or Phase.generate in settings.phases
        ):
            return
    
        try:
            if isinstance(runner, TestCase) and hasattr(runner, "subTest"):
                subTest = runner.subTest
                try:
                    runner.subTest = types.MethodType(fake_subTest, runner)
                    state.run_engine()
                finally:
                    runner.subTest = subTest
            else:
                state.run_engine()
        except BaseException as e:
            # The exception caught here should either be an actual test
            # failure (or MultipleFailures), or some kind of fatal error
            # that caused the engine to stop.
    
            generated_seed = wrapped_test._hypothesis_internal_use_generated_seed
            with local_settings(settings):
                if not (state.failed_normally or generated_seed is None):
                    if running_under_pytest:
                        report(
                            "You can add @seed(%(seed)d) to this test or "
                            "run pytest with --hypothesis-seed=%(seed)d "
                            "to reproduce this failure." % {"seed": generated_seed}
                        )
                    else:
                        report(
                            "You can add @seed(%d) to this test to "
                            "reproduce this failure." % (generated_seed,)
                        )
                # The dance here is to avoid showing users long tracebacks
                # full of Hypothesis internals they don't care about.
                # We have to do this inline, to avoid adding another
                # internal stack frame just when we've removed the rest.
                #
                # Using a variable for our trimmed error ensures that the line
                # which will actually appear in tracebacks is as clear as
                # possible - "raise the_error_hypothesis_found".
                the_error_hypothesis_found = e.with_traceback(
                    get_trimmed_traceback()
                )
>               raise the_error_hypothesis_found

.venv/lib/python3.8/site-packages/hypothesis/core.py:1162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arguments = ()
kwargs = {'self': <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>}
__tracebackhide__ = True
test = <function TestGPUPredict.test_shap_interactions at 0x7fa927cfaee0>
settings = settings(database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples'), deadline=None, derand...nt_blob=False, report_multiple_bugs=True, stateful_step_count=50, suppress_health_check=(), verbosity=Verbosity.normal)
random = <random.Random object at 0x281bb30>
processed_args = ((), {'self': <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>}, <function default_new_style_exe...p(lambda args: dict(args, **kwargs)))), runner=<test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>))
test_runner = <function default_new_style_executor at 0x7fa94d2a94c0>
search_strategy = WithRunner(TupleStrategy((just(()), fixed_dictionaries({'dataset': _dataset_weight_margin(),
 'num_rounds': integers(m...ap(lambda args: dict(args, **kwargs)))), runner=<test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>)
runner = <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>
state = <hypothesis.core.StateForActualGivenExecution object at 0x7fa926024d30>

    @impersonate(test)
    @define_function_signature(test.__name__, test.__doc__, argspec)
    def wrapped_test(*arguments, **kwargs):
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
    
        test = wrapped_test.hypothesis.inner_test
    
        if getattr(test, "is_hypothesis_test", False):
            raise InvalidArgument(
                (
                    "You have applied @given to the test %s more than once, which "
                    "wraps the test several times and is extremely slow. A "
                    "similar effect can be gained by combining the arguments "
                    "of the two calls to given. For example, instead of "
                    "@given(booleans()) @given(integers()), you could write "
                    "@given(booleans(), integers())"
                )
                % (test.__name__,)
            )
    
        settings = wrapped_test._hypothesis_internal_use_settings
    
        random = get_random_for_wrapped_test(test, wrapped_test)
    
        processed_args = process_arguments_to_given(
            wrapped_test, arguments, kwargs, given_kwargs, argspec
        )
        arguments, kwargs, test_runner, search_strategy = processed_args
    
        runner = getattr(search_strategy, "runner", None)
        if isinstance(runner, TestCase) and test.__name__ in dir(TestCase):
            msg = (
                "You have applied @given to the method %s, which is "
                "used by the unittest runner but is not itself a test."
                "  This is not useful in any way." % test.__name__
            )
            fail_health_check(settings, msg, HealthCheck.not_a_test_method)
        if bad_django_TestCase(runner):  # pragma: no cover
            # Covered by the Django tests, but not the pytest coverage task
            raise InvalidArgument(
                "You have applied @given to a method on %s, but this "
                "class does not inherit from the supported versions in "
                "`hypothesis.extra.django`.  Use the Hypothesis variants "
                "to ensure that each example is run in a separate "
                "database transaction." % qualname(type(runner))
            )
    
        state = StateForActualGivenExecution(
            test_runner, search_strategy, test, settings, random, wrapped_test
        )
    
        reproduce_failure = wrapped_test._hypothesis_internal_use_reproduce_failure
    
        # If there was a @reproduce_failure decorator, use it to reproduce
        # the error (or complain that we couldn't). Either way, this will
        # always raise some kind of error.
        if reproduce_failure is not None:
            expected_version, failure = reproduce_failure
            if expected_version != __version__:
                raise InvalidArgument(
                    (
                        "Attempting to reproduce a failure from a different "
                        "version of Hypothesis. This failure is from %s, but "
                        "you are currently running %r. Please change your "
                        "Hypothesis version to a matching one."
                    )
                    % (expected_version, __version__)
                )
            try:
                state.execute_once(
                    ConjectureData.for_buffer(decode_failure(failure)),
                    print_example=True,
                    is_final=True,
                )
                raise DidNotReproduce(
                    "Expected the test to raise an error, but it "
                    "completed successfully."
                )
            except StopTest:
                raise DidNotReproduce(
                    "The shape of the test data has changed in some way "
                    "from where this blob was defined. Are you sure "
                    "you're running the same test?"
                )
            except UnsatisfiedAssumption:
                raise DidNotReproduce(
                    "The test data failed to satisfy an assumption in the "
                    "test. Have you added it since this blob was "
                    "generated?"
                )
    
        # There was no @reproduce_failure, so start by running any explicit
        # examples from @example decorators.
        errors = list(
            execute_explicit_examples(state, wrapped_test, arguments, kwargs)
        )
        with local_settings(state.settings):
            if len(errors) > 1:
                # If we're not going to report multiple bugs, we would have
                # stopped running explicit examples at the first failure.
                assert state.settings.report_multiple_bugs
                for fragments, err in errors:
                    for f in fragments:
                        report(f)
                    tb_lines = traceback.format_exception(
                        type(err), err, err.__traceback__
                    )
                    report("".join(tb_lines))
                msg = "Hypothesis found %d failures in explicit examples."
                raise MultipleFailures(msg % (len(errors)))
            elif errors:
                fragments, the_error_hypothesis_found = errors[0]
                for f in fragments:
                    report(f)
                raise the_error_hypothesis_found
    
        # If there were any explicit examples, they all ran successfully.
        # The next step is to use the Conjecture engine to run the test on
        # many different inputs.
    
        if not (
            Phase.reuse in settings.phases or Phase.generate in settings.phases
        ):
            return
    
        try:
            if isinstance(runner, TestCase) and hasattr(runner, "subTest"):
                subTest = runner.subTest
                try:
                    runner.subTest = types.MethodType(fake_subTest, runner)
>                   state.run_engine()

.venv/lib/python3.8/site-packages/hypothesis/core.py:1127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <hypothesis.core.StateForActualGivenExecution object at 0x7fa926024d30>

    def run_engine(self):
        """Run the test function many times, on database input and generated
        input, using the Conjecture engine.
        """
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
        try:
            database_key = self.wrapped_test._hypothesis_internal_database_key
        except AttributeError:
            if global_force_seed is None:
                database_key = function_digest(self.test)
            else:
                database_key = None
    
        runner = ConjectureRunner(
            self._execute_once_for_engine,
            settings=self.settings,
            random=self.random,
            database_key=database_key,
        )
        # Use the Conjecture engine to run the test function many times
        # on different inputs.
        runner.run()
        note_statistics(runner.statistics)
    
        if runner.call_count == 0:
            return
        if runner.interesting_examples:
            self.falsifying_examples = sorted(
                runner.interesting_examples.values(),
                key=lambda d: sort_key(d.buffer),
                reverse=True,
            )
        else:
            if runner.valid_examples == 0:
                raise Unsatisfiable(
                    "Unable to satisfy assumptions of hypothesis %s."
                    % (get_pretty_function_description(self.test),)
                )
    
        if not self.falsifying_examples:
            return
        elif not self.settings.report_multiple_bugs:
            # Pretend that we only found one failure, by discarding the others.
            del self.falsifying_examples[:-1]
    
        # The engine found one or more failures, so we need to reproduce and
        # report them.
    
        self.failed_normally = True
    
        flaky = 0
    
        if runner.best_observed_targets:
            for line in describe_targets(runner.best_observed_targets):
                report(line)
            report("")
    
        for falsifying_example in self.falsifying_examples:
            info = falsifying_example.extra_information
    
            ran_example = ConjectureData.for_buffer(falsifying_example.buffer)
            self.__was_flaky = False
            assert info.__expected_exception is not None
            try:
                self.execute_once(
                    ran_example,
                    print_example=not self.is_find,
                    is_final=True,
                    expected_failure=(
                        info.__expected_exception,
                        info.__expected_traceback,
                    ),
                )
            except (UnsatisfiedAssumption, StopTest):
                report(traceback.format_exc())
                self.__flaky(
                    "Unreliable assumption: An example which satisfied "
                    "assumptions on the first run now fails it."
                )
            except BaseException as e:
                if len(self.falsifying_examples) <= 1:
                    # There is only one failure, so we can report it by raising
                    # it directly.
                    raise
    
                # We are reporting multiple failures, so we need to manually
                # print each exception's stack trace and information.
                tb = get_trimmed_traceback()
                report("".join(traceback.format_exception(type(e), e, tb)))
    
            finally:  # pragma: no cover
                # Mostly useful for ``find`` and ensuring that objects that
                # hold on to a reference to ``data`` know that it's now been
                # finished and they shouldn't attempt to draw more data from
                # it.
                ran_example.freeze()
    
                # This section is in fact entirely covered by the tests in
                # test_reproduce_failure, but it seems to trigger a lovely set
                # of coverage bugs: The branches show up as uncovered (despite
                # definitely being covered - you can add an assert False else
                # branch to verify this and see it fail - and additionally the
                # second branch still complains about lack of coverage even if
                # you add a pragma: no cover to it!
                # See https://bitbucket.org/ned/coveragepy/issues/623/
                if self.settings.print_blob:
                    report(
                        (
                            "\nYou can reproduce this example by temporarily "
                            "adding @reproduce_failure(%r, %r) as a decorator "
                            "on your test case"
                        )
                        % (__version__, encode_failure(falsifying_example.buffer))
                    )
            if self.__was_flaky:
                flaky += 1
    
        # If we only have one example then we should have raised an error or
        # flaky prior to this point.
        assert len(self.falsifying_examples) > 1
    
        if flaky > 0:
            raise Flaky(
                (
                    "Hypothesis found %d distinct failures, but %d of them "
                    "exhibited some sort of flaky behaviour."
                )
                % (len(self.falsifying_examples), flaky)
            )
        else:
>           raise MultipleFailures(
                ("Hypothesis found %d distinct failures.")
                % (len(self.falsifying_examples))
            )
E           hypothesis.errors.MultipleFailures: Hypothesis found 2 distinct failures.

.venv/lib/python3.8/site-packages/hypothesis/core.py:859: MultipleFailures
---------------------------------- Hypothesis ----------------------------------
Falsifying example: test_shap_interactions(
    self=<test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>,
    num_rounds=1,
    dataset=sparse,
    param={'max_depth': 0, 'max_leaves': 1, 'num_parallel_tree': 1},
)
Traceback (most recent call last):
  File "/home/otivedani/xgboost/tests/python-gpu/test_gpu_prediction.py", line 217, in test_shap_interactions
    pytest.xfail(reason=f'GPU shap with sparse is flaky: {issue}')
  File "/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/outcomes.py", line 172, in xfail
    raise XFailed(reason)
_pytest.outcomes.XFailed: GPU shap with sparse is flaky: https://github.com/dmlc/xgboost/issues/6074

Falsifying example: test_shap_interactions(
    self=<test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>,
    num_rounds=1,
    dataset=boston,
    param={'max_depth': 0, 'max_leaves': 1, 'num_parallel_tree': 1},
)
Traceback (most recent call last):
  File "/home/otivedani/xgboost/tests/python-gpu/test_gpu_prediction.py", line 222, in test_shap_interactions
    bst = xgb.train(param, dmat, num_rounds)
  File "/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/training.py", line 216, in train
    bst = _train_internal(params, dtrain,
  File "/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/training.py", line 111, in _train_internal
    bst.update(dtrain, i, obj)
  File "/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/core.py", line 1178, in update
    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
  File "/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/core.py", line 186, in _check_call
    raise XGBoostError(py_str(_LIB.XGBGetLastError()))
xgboost.core.XGBoostError: [15:55:43] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
Stack trace:
  [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
  [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::GetPredictor(xgboost::HostDeviceVector<float> const*, xgboost::DMatrix*) const+0x225) [0x7fa92b84ec35]
  [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::PredictBatch(xgboost::DMatrix*, xgboost::PredictionCacheEntry*, bool, unsigned int)+0x49) [0x7fa92b84ef29]
  [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x3b2) [0x7fa92b87ed42]
  [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x137c0) [0x7fa94d18f7c0]


_________________________ TestGPUPredict.test_sklearn __________________________

self = <unittest.case._Outcome object at 0x7fa925e4e190>
test_case = <test_gpu_prediction.TestGPUPredict testMethod=test_sklearn>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_sklearn>
result = <TestCaseFunction test_sklearn>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_sklearn>
method = <bound method TestGPUPredict.test_sklearn of <test_gpu_prediction.TestGPUPredict testMethod=test_sklearn>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_sklearn>

    @pytest.mark.skipif(**tm.no_sklearn())
    def test_sklearn(self):
        m, n = 15000, 14
        tr_size = 2500
        X = np.random.rand(m, n)
        y = 200 * np.matmul(X, np.arange(-3, -3 + n))
        X_train, y_train = X[:tr_size, :], y[:tr_size]
        X_test, y_test = X[tr_size:, :], y[tr_size:]
    
        # First with cpu_predictor
        params = {'tree_method': 'gpu_hist',
                  'predictor': 'cpu_predictor',
                  'n_jobs': -1,
                  'seed': 123}
>       m = xgb.XGBRegressor(**params).fit(X_train, y_train)

tests/python-gpu/test_gpu_prediction.py:116: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,
             colsample_bynode=None, colsample_bytr...s_weight=None, seed=123, subsample=None,
             tree_method='gpu_hist', validate_parameters=None, verbosity=None)
X = array([[0.44190428, 0.58157741, 0.98975171, ..., 0.79860424, 0.29733138,
        0.02760601],
       [0.59343245, 0.84...44,
        0.07752733],
       [0.70544139, 0.52570295, 0.57479217, ..., 0.85518733, 0.85803751,
        0.04484167]])
y = array([3106.44411075, 4751.03056709, 4862.18427997, ..., 4386.84112123,
       5714.23745935, 4689.73084773])
sample_weight = None, base_margin = None, eval_set = None, eval_metric = None
early_stopping_rounds = None, verbose = True, xgb_model = None
sample_weight_eval_set = None, feature_weights = None, callbacks = None

    def fit(self, X, y, sample_weight=None, base_margin=None,
            eval_set=None, eval_metric=None, early_stopping_rounds=None,
            verbose=True, xgb_model=None, sample_weight_eval_set=None,
            feature_weights=None,
            callbacks=None):
        # pylint: disable=invalid-name,attribute-defined-outside-init
        """Fit gradient boosting model
    
        Parameters
        ----------
        X : array_like
            Feature matrix
        y : array_like
            Labels
        sample_weight : array_like
            instance weights
        base_margin : array_like
            global bias for each instance.
        eval_set : list, optional
            A list of (X, y) tuple pairs to use as validation sets, for which
            metrics will be computed.
            Validation metrics will help us track the performance of the model.
        eval_metric : str, list of str, or callable, optional
            If a str, should be a built-in evaluation metric to use. See
            doc/parameter.rst.
            If a list of str, should be the list of multiple built-in evaluation metrics
            to use.
            If callable, a custom evaluation metric. The call
            signature is ``func(y_predicted, y_true)`` where ``y_true`` will be a
            DMatrix object such that you may need to call the ``get_label``
            method. It must return a str, value pair where the str is a name
            for the evaluation and value is the value of the evaluation
            function. The callable custom objective is always minimized.
        early_stopping_rounds : int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **eval_set**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **eval_set**, the last entry will be used
            for early stopping.
            If there's more than one metric in **eval_metric**, the last metric will be
            used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``clf.best_score``, ``clf.best_iteration`` and ``clf.best_ntree_limit``.
        verbose : bool
            If `verbose` and an evaluation set is used, writes the evaluation
            metric measured on the validation set to stderr.
        xgb_model : str
            file name of stored XGBoost model or 'Booster' instance XGBoost model to be
            loaded before training (allows training continuation).
        sample_weight_eval_set : list, optional
            A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of
            instance weights on the i-th validation set.
        feature_weights: array_like
            Weight for each feature, defines the probability of each feature
            being selected when colsample is being used.  All values must be
            greater than 0, otherwise a `ValueError` is thrown.
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using :ref:`callback_api`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        """
        self.n_features_in_ = X.shape[1]
    
        train_dmatrix = DMatrix(data=X, label=y, weight=sample_weight,
                                base_margin=base_margin,
                                missing=self.missing,
                                nthread=self.n_jobs)
        train_dmatrix.set_info(feature_weights=feature_weights)
    
        evals_result = {}
    
        if eval_set is not None:
            if not isinstance(eval_set[0], (list, tuple)):
                raise TypeError('Unexpected input type for `eval_set`')
            if sample_weight_eval_set is None:
                sample_weight_eval_set = [None] * len(eval_set)
            else:
                assert len(eval_set) == len(sample_weight_eval_set)
            evals = list(
                DMatrix(eval_set[i][0], label=eval_set[i][1], missing=self.missing,
                        weight=sample_weight_eval_set[i], nthread=self.n_jobs)
                for i in range(len(eval_set)))
            evals = list(zip(evals, ["validation_{}".format(i) for i in
                                     range(len(evals))]))
        else:
            evals = ()
    
        params = self.get_xgb_params()
    
        if callable(self.objective):
            obj = _objective_decorator(self.objective)
            params["objective"] = "reg:squarederror"
        else:
            obj = None
    
        feval = eval_metric if callable(eval_metric) else None
        if eval_metric is not None:
            if callable(eval_metric):
                eval_metric = None
            else:
                params.update({'eval_metric': eval_metric})
    
>       self._Booster = train(params, train_dmatrix,
                              self.get_num_boosting_rounds(), evals=evals,
                              early_stopping_rounds=early_stopping_rounds,
                              evals_result=evals_result,
                              obj=obj, feval=feval,
                              verbose_eval=verbose, xgb_model=xgb_model,
                              callbacks=callbacks)

.venv/lib/python3.8/site-packages/xgboost/sklearn.py:549: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925e4e760>, num_boost_round = 100
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = {}, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925e4e760>, num_boost_round = 100
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925cd41c0>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925e4efa0>
dtrain = <xgboost.core.DMatrix object at 0x7fa925e4e760>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:55:43] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
____________________ TestRanking.test_training_rank_map_auc ____________________

self = <unittest.case._Outcome object at 0x7fa925c39460>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_auc>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_auc>
result = <TestCaseFunction test_training_rank_map_auc>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_auc>
method = <bound method TestRanking.test_training_rank_map_auc of <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_auc>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_auc>

    def test_training_rank_map_auc(self):
        """
        Train an XGBoost ranking model with map objective function and compare auc metric
        """
>       self.__test_training_with_rank_objective('rank:map', 'auc')

tests/python-gpu/test_gpu_ranking.py:183: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:map'
metric_name = 'auc', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'auc', 'gpu_id': 0, 'objective': 'rank:map', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'auc', 'gpu_id': 0, 'objective': 'rank:map', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925c398e0>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925c394f0>
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:04] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
____________________ TestRanking.test_training_rank_map_map ____________________

self = <unittest.case._Outcome object at 0x7fa93bcbc4f0>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_map>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_map>
result = <TestCaseFunction test_training_rank_map_map>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_map>
method = <bound method TestRanking.test_training_rank_map_map of <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_map>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_map>

    def test_training_rank_map_map(self):
        """
        Train an XGBoost ranking model with map objective function and compare map metric
        """
>       self.__test_training_with_rank_objective('rank:map', 'map')

tests/python-gpu/test_gpu_ranking.py:177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:map'
metric_name = 'map', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'map', 'gpu_id': 0, 'objective': 'rank:map', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'map', 'gpu_id': 0, 'objective': 'rank:map', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa93bcbc550>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa93bcbc490>
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:04] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
___________________ TestRanking.test_training_rank_map_ndcg ____________________

self = <unittest.case._Outcome object at 0x7fa925c394c0>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_ndcg>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_ndcg>
result = <TestCaseFunction test_training_rank_map_ndcg>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_ndcg>
method = <bound method TestRanking.test_training_rank_map_ndcg of <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_ndcg>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_ndcg>

    def test_training_rank_map_ndcg(self):
        """
        Train an XGBoost ranking model with map objective function and compare ndcg metric
        """
>       self.__test_training_with_rank_objective('rank:map', 'ndcg')

tests/python-gpu/test_gpu_ranking.py:189: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:map'
metric_name = 'ndcg', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'ndcg', 'gpu_id': 0, 'objective': 'rank:map', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'ndcg', 'gpu_id': 0, 'objective': 'rank:map', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925c39b80>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925c39430>
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:04] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
___________________ TestRanking.test_training_rank_ndcg_auc ____________________

self = <unittest.case._Outcome object at 0x7fa92609cbe0>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_auc>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_auc>
result = <TestCaseFunction test_training_rank_ndcg_auc>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_auc>
method = <bound method TestRanking.test_training_rank_ndcg_auc of <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_auc>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_auc>

    def test_training_rank_ndcg_auc(self):
        """
        Train an XGBoost ranking model with ndcg objective function and compare auc metric
        """
>       self.__test_training_with_rank_objective('rank:ndcg', 'auc')

tests/python-gpu/test_gpu_ranking.py:165: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:ndcg'
metric_name = 'auc', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'auc', 'gpu_id': 0, 'objective': 'rank:ndcg', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'auc', 'gpu_id': 0, 'objective': 'rank:ndcg', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa92609c250>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa92609c190>
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:04] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
___________________ TestRanking.test_training_rank_ndcg_map ____________________

self = <unittest.case._Outcome object at 0x7fa925ee16a0>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_map>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_map>
result = <TestCaseFunction test_training_rank_ndcg_map>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_map>
method = <bound method TestRanking.test_training_rank_ndcg_map of <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_map>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_map>

    def test_training_rank_ndcg_map(self):
        """
        Train an XGBoost ranking model with ndcg objective function and compare map metric
        """
>       self.__test_training_with_rank_objective('rank:ndcg', 'map')

tests/python-gpu/test_gpu_ranking.py:159: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:ndcg'
metric_name = 'map', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'map', 'gpu_id': 0, 'objective': 'rank:ndcg', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'map', 'gpu_id': 0, 'objective': 'rank:ndcg', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925ee1c10>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925ee1790>
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:04] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
___________________ TestRanking.test_training_rank_ndcg_ndcg ___________________

self = <unittest.case._Outcome object at 0x7fa925ab8040>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_ndcg>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_ndcg>
result = <TestCaseFunction test_training_rank_ndcg_ndcg>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_ndcg>
method = <bound method TestRanking.test_training_rank_ndcg_ndcg of <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_ndcg>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_ndcg>

    def test_training_rank_ndcg_ndcg(self):
        """
        Train an XGBoost ranking model with ndcg objective function and compare ndcg metric
        """
>       self.__test_training_with_rank_objective('rank:ndcg', 'ndcg')

tests/python-gpu/test_gpu_ranking.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:ndcg'
metric_name = 'ndcg', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'ndcg', 'gpu_id': 0, 'objective': 'rank:ndcg', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'ndcg', 'gpu_id': 0, 'objective': 'rank:ndcg', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925ab80d0>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925ab8370>
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:04] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
______________ TestRanking.test_training_rank_pairwise_auc_metric ______________

self = <unittest.case._Outcome object at 0x7fa925b2f8b0>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_auc_metric>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_auc_metric>
result = <TestCaseFunction test_training_rank_pairwise_auc_metric>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_auc_metric>
method = <bound method TestRanking.test_training_rank_pairwise_auc_metric of <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_auc_metric>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_auc_metric>

    def test_training_rank_pairwise_auc_metric(self):
        """
        Train an XGBoost ranking model with pairwise objective function and compare auc metric
        """
>       self.__test_training_with_rank_objective('rank:pairwise', 'auc')

tests/python-gpu/test_gpu_ranking.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:pairwise'
metric_name = 'auc', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'auc', 'gpu_id': 0, 'objective': 'rank:pairwise', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'auc', 'gpu_id': 0, 'objective': 'rank:pairwise', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925b2f730>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925b2f070>
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:04] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
______________ TestRanking.test_training_rank_pairwise_map_metric ______________

self = <unittest.case._Outcome object at 0x7fa925cb33d0>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_map_metric>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_map_metric>
result = <TestCaseFunction test_training_rank_pairwise_map_metric>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_map_metric>
method = <bound method TestRanking.test_training_rank_pairwise_map_metric of <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_map_metric>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_map_metric>

    def test_training_rank_pairwise_map_metric(self):
        """
        Train an XGBoost ranking model with pairwise objective function and compare map metric
        """
>       self.__test_training_with_rank_objective('rank:pairwise', 'map')

tests/python-gpu/test_gpu_ranking.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:pairwise'
metric_name = 'map', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'map', 'gpu_id': 0, 'objective': 'rank:pairwise', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'map', 'gpu_id': 0, 'objective': 'rank:pairwise', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925cb3670>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925cb3070>
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:04] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
_____________ TestRanking.test_training_rank_pairwise_ndcg_metric ______________

self = <unittest.case._Outcome object at 0x7fa925c39430>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_ndcg_metric>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_ndcg_metric>
result = <TestCaseFunction test_training_rank_pairwise_ndcg_metric>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_ndcg_metric>
method = <bound method TestRanking.test_training_rank_pairwise_ndcg_metric of <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_ndcg_metric>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_ndcg_metric>

    def test_training_rank_pairwise_ndcg_metric(self):
        """
        Train an XGBoost ranking model with pairwise objective function and compare ndcg metric
        """
>       self.__test_training_with_rank_objective('rank:pairwise', 'ndcg')

tests/python-gpu/test_gpu_ranking.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:pairwise'
metric_name = 'ndcg', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'ndcg', 'gpu_id': 0, 'objective': 'rank:pairwise', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'ndcg', 'gpu_id': 0, 'objective': 'rank:pairwise', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fa925d8a970>, 'eval'), (<xgboost.core.DMatrix object at 0x7fa925c39640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925d4cee0>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925c39310>
dtrain = <xgboost.core.DMatrix object at 0x7fa925c39640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:04] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
______ TestGPUTrainingContinuation.test_gpu_training_continuation_binary _______

self = <unittest.case._Outcome object at 0x7fa925cdf130>
test_case = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_binary>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_binary>
result = <TestCaseFunction test_gpu_training_continuation_binary>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_binary>
method = <bound method TestGPUTrainingContinuation.test_gpu_training_continuation_binary of <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_binary>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_binary>

    def test_gpu_training_continuation_binary(self):
>       self.run_training_continuation(False)

tests/python-gpu/test_gpu_training_continuation.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_binary>
use_json = False

    def run_training_continuation(self, use_json):
        kRows = 64
        kCols = 32
        X = np.random.randn(kRows, kCols)
        y = np.random.randn(kRows)
        dtrain = xgb.DMatrix(X, y)
        params = {'tree_method': 'gpu_hist', 'max_depth': '2',
                  'gamma': '0.1', 'alpha': '0.01',
                  'enable_experimental_json_serialization': use_json}
>       bst_0 = xgb.train(params, dtrain, num_boost_round=64)

tests/python-gpu/test_gpu_training_continuation.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'alpha': '0.01', 'enable_experimental_json_serialization': False, 'gamma': '0.1', 'max_depth': '2', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925cdf070>, num_boost_round = 64
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'alpha': '0.01', 'enable_experimental_json_serialization': False, 'gamma': '0.1', 'max_depth': '2', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925cdf070>, num_boost_round = 64
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925cdf490>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925cdf160>
dtrain = <xgboost.core.DMatrix object at 0x7fa925cdf070>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:04] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
_______ TestGPUTrainingContinuation.test_gpu_training_continuation_json ________

self = <unittest.case._Outcome object at 0x7fa925eff700>
test_case = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_json>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_json>
result = <TestCaseFunction test_gpu_training_continuation_json>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_json>
method = <bound method TestGPUTrainingContinuation.test_gpu_training_continuation_json of <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_json>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_json>

    def test_gpu_training_continuation_json(self):
>       self.run_training_continuation(True)

tests/python-gpu/test_gpu_training_continuation.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_json>
use_json = True

    def run_training_continuation(self, use_json):
        kRows = 64
        kCols = 32
        X = np.random.randn(kRows, kCols)
        y = np.random.randn(kRows)
        dtrain = xgb.DMatrix(X, y)
        params = {'tree_method': 'gpu_hist', 'max_depth': '2',
                  'gamma': '0.1', 'alpha': '0.01',
                  'enable_experimental_json_serialization': use_json}
>       bst_0 = xgb.train(params, dtrain, num_boost_round=64)

tests/python-gpu/test_gpu_training_continuation.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'alpha': '0.01', 'enable_experimental_json_serialization': True, 'gamma': '0.1', 'max_depth': '2', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925eff640>, num_boost_round = 64
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'alpha': '0.01', 'enable_experimental_json_serialization': True, 'gamma': '0.1', 'max_depth': '2', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925eff640>, num_boost_round = 64
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925effb50>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925efff70>
dtrain = <xgboost.core.DMatrix object at 0x7fa925eff640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:05] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
________________________ TestGPUUpdaters.test_gpu_hist _________________________

cls = <class '_pytest.runner.CallInfo'>
func = <function call_runtest_hook.<locals>.<lambda> at 0x7fa925e261f0>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: "Optional[Union[Type[BaseException], Tuple[Type[BaseException], ...]]]" = None,
    ) -> "CallInfo[TResult]":
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result = func()  # type: Optional[TResult]

.venv/lib/python3.8/site-packages/_pytest/runner.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: ihook(item=item, **kwds), when=when, reraise=reraise
    )

.venv/lib/python3.8/site-packages/_pytest/runner.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_runtest_call'>, args = ()
kwargs = {'item': <Function test_gpu_hist>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fa94e3b0580>
hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
kwargs = {'item': <Function test_gpu_hist>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
kwargs = {'item': <Function test_gpu_hist>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
caller_kwargs = {'item': <Function test_gpu_hist>}, firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fa925e57c40>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
caller_kwargs = {'item': <Function test_gpu_hist>}, firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_gpu_hist>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
            item.runtest()
        except Exception as e:
            # Store trace info to allow postmortem debugging
            sys.last_type = type(e)
            sys.last_value = e
            assert e.__traceback__ is not None
            # Skip *this* frame
            sys.last_traceback = e.__traceback__.tb_next
>           raise e

.venv/lib/python3.8/site-packages/_pytest/runner.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_gpu_hist>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
>           item.runtest()

.venv/lib/python3.8/site-packages/_pytest/runner.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Function test_gpu_hist>

    def runtest(self) -> None:
        """Execute the underlying test function."""
>       self.ihook.pytest_pyfunc_call(pyfuncitem=self)

.venv/lib/python3.8/site-packages/_pytest/python.py:1627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_pyfunc_call'>, args = ()
kwargs = {'pyfuncitem': <Function test_gpu_hist>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fa94e3b0580>
hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_gpu_hist>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_gpu_hist>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_gpu_hist>}, firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fa925e576a0>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_gpu_hist>}, firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pyfuncitem = <Function test_gpu_hist>

    @hookimpl(trylast=True)
    def pytest_pyfunc_call(pyfuncitem: "Function") -> Optional[object]:
        testfunction = pyfuncitem.obj
        if is_async_function(testfunction):
            async_warn_and_skip(pyfuncitem.nodeid)
        funcargs = pyfuncitem.funcargs
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
>       result = testfunction(**testargs)

.venv/lib/python3.8/site-packages/_pytest/python.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fa925e57bb0>

    @given(parameter_strategy, strategies.integers(1, 20),
>          tm.dataset_strategy)
    @settings(deadline=None)

tests/python-gpu/test_gpu_updaters.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arguments = ()
kwargs = {'self': <test_gpu_updaters.TestGPUUpdaters object at 0x7fa925e57bb0>}
__tracebackhide__ = True
test = <function TestGPUUpdaters.test_gpu_hist at 0x7fa927a55160>
settings = settings(database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples'), deadline=None, derand...nt_blob=False, report_multiple_bugs=True, stateful_step_count=50, suppress_health_check=(), verbosity=Verbosity.normal)
random = <random.Random object at 0x287f830>
processed_args = ((), {'self': <test_gpu_updaters.TestGPUUpdaters object at 0x7fa925e57bb0>}, <function default_new_style_executor at 0...own>)}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_updaters.TestGPUUpdaters object at 0x7fa925e57bb0>))
test_runner = <function default_new_style_executor at 0x7fa94d2a94c0>
search_strategy = WithRunner(TupleStrategy((just(()), fixed_dictionaries({'dataset': _dataset_weight_margin(),
 'num_rounds': integers(m...nown>)}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_updaters.TestGPUUpdaters object at 0x7fa925e57bb0>)
runner = <test_gpu_updaters.TestGPUUpdaters object at 0x7fa925e57bb0>
state = <hypothesis.core.StateForActualGivenExecution object at 0x7fa925eff7f0>

    @impersonate(test)
    @define_function_signature(test.__name__, test.__doc__, argspec)
    def wrapped_test(*arguments, **kwargs):
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
    
        test = wrapped_test.hypothesis.inner_test
    
        if getattr(test, "is_hypothesis_test", False):
            raise InvalidArgument(
                (
                    "You have applied @given to the test %s more than once, which "
                    "wraps the test several times and is extremely slow. A "
                    "similar effect can be gained by combining the arguments "
                    "of the two calls to given. For example, instead of "
                    "@given(booleans()) @given(integers()), you could write "
                    "@given(booleans(), integers())"
                )
                % (test.__name__,)
            )
    
        settings = wrapped_test._hypothesis_internal_use_settings
    
        random = get_random_for_wrapped_test(test, wrapped_test)
    
        processed_args = process_arguments_to_given(
            wrapped_test, arguments, kwargs, given_kwargs, argspec
        )
        arguments, kwargs, test_runner, search_strategy = processed_args
    
        runner = getattr(search_strategy, "runner", None)
        if isinstance(runner, TestCase) and test.__name__ in dir(TestCase):
            msg = (
                "You have applied @given to the method %s, which is "
                "used by the unittest runner but is not itself a test."
                "  This is not useful in any way." % test.__name__
            )
            fail_health_check(settings, msg, HealthCheck.not_a_test_method)
        if bad_django_TestCase(runner):  # pragma: no cover
            # Covered by the Django tests, but not the pytest coverage task
            raise InvalidArgument(
                "You have applied @given to a method on %s, but this "
                "class does not inherit from the supported versions in "
                "`hypothesis.extra.django`.  Use the Hypothesis variants "
                "to ensure that each example is run in a separate "
                "database transaction." % qualname(type(runner))
            )
    
        state = StateForActualGivenExecution(
            test_runner, search_strategy, test, settings, random, wrapped_test
        )
    
        reproduce_failure = wrapped_test._hypothesis_internal_use_reproduce_failure
    
        # If there was a @reproduce_failure decorator, use it to reproduce
        # the error (or complain that we couldn't). Either way, this will
        # always raise some kind of error.
        if reproduce_failure is not None:
            expected_version, failure = reproduce_failure
            if expected_version != __version__:
                raise InvalidArgument(
                    (
                        "Attempting to reproduce a failure from a different "
                        "version of Hypothesis. This failure is from %s, but "
                        "you are currently running %r. Please change your "
                        "Hypothesis version to a matching one."
                    )
                    % (expected_version, __version__)
                )
            try:
                state.execute_once(
                    ConjectureData.for_buffer(decode_failure(failure)),
                    print_example=True,
                    is_final=True,
                )
                raise DidNotReproduce(
                    "Expected the test to raise an error, but it "
                    "completed successfully."
                )
            except StopTest:
                raise DidNotReproduce(
                    "The shape of the test data has changed in some way "
                    "from where this blob was defined. Are you sure "
                    "you're running the same test?"
                )
            except UnsatisfiedAssumption:
                raise DidNotReproduce(
                    "The test data failed to satisfy an assumption in the "
                    "test. Have you added it since this blob was "
                    "generated?"
                )
    
        # There was no @reproduce_failure, so start by running any explicit
        # examples from @example decorators.
        errors = list(
            execute_explicit_examples(state, wrapped_test, arguments, kwargs)
        )
        with local_settings(state.settings):
            if len(errors) > 1:
                # If we're not going to report multiple bugs, we would have
                # stopped running explicit examples at the first failure.
                assert state.settings.report_multiple_bugs
                for fragments, err in errors:
                    for f in fragments:
                        report(f)
                    tb_lines = traceback.format_exception(
                        type(err), err, err.__traceback__
                    )
                    report("".join(tb_lines))
                msg = "Hypothesis found %d failures in explicit examples."
                raise MultipleFailures(msg % (len(errors)))
            elif errors:
                fragments, the_error_hypothesis_found = errors[0]
                for f in fragments:
                    report(f)
                raise the_error_hypothesis_found
    
        # If there were any explicit examples, they all ran successfully.
        # The next step is to use the Conjecture engine to run the test on
        # many different inputs.
    
        if not (
            Phase.reuse in settings.phases or Phase.generate in settings.phases
        ):
            return
    
        try:
            if isinstance(runner, TestCase) and hasattr(runner, "subTest"):
                subTest = runner.subTest
                try:
                    runner.subTest = types.MethodType(fake_subTest, runner)
                    state.run_engine()
                finally:
                    runner.subTest = subTest
            else:
                state.run_engine()
        except BaseException as e:
            # The exception caught here should either be an actual test
            # failure (or MultipleFailures), or some kind of fatal error
            # that caused the engine to stop.
    
            generated_seed = wrapped_test._hypothesis_internal_use_generated_seed
            with local_settings(settings):
                if not (state.failed_normally or generated_seed is None):
                    if running_under_pytest:
                        report(
                            "You can add @seed(%(seed)d) to this test or "
                            "run pytest with --hypothesis-seed=%(seed)d "
                            "to reproduce this failure." % {"seed": generated_seed}
                        )
                    else:
                        report(
                            "You can add @seed(%d) to this test to "
                            "reproduce this failure." % (generated_seed,)
                        )
                # The dance here is to avoid showing users long tracebacks
                # full of Hypothesis internals they don't care about.
                # We have to do this inline, to avoid adding another
                # internal stack frame just when we've removed the rest.
                #
                # Using a variable for our trimmed error ensures that the line
                # which will actually appear in tracebacks is as clear as
                # possible - "raise the_error_hypothesis_found".
                the_error_hypothesis_found = e.with_traceback(
                    get_trimmed_traceback()
                )
>               raise the_error_hypothesis_found

.venv/lib/python3.8/site-packages/hypothesis/core.py:1162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fa925e57bb0>
param = {'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'grow_policy': 'lossguide', ...}
num_rounds = 1, dataset = boston

    @given(parameter_strategy, strategies.integers(1, 20),
           tm.dataset_strategy)
    @settings(deadline=None)
    def test_gpu_hist(self, param, num_rounds, dataset):
        param['tree_method'] = 'gpu_hist'
        param = dataset.set_params(param)
>       result = train_result(param, dataset.get_dmat(), num_rounds)

tests/python-gpu/test_gpu_updaters.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

param = {'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'grow_policy': 'lossguide', ...}
dmat = <xgboost.core.DMatrix object at 0x7fa925b2f820>, num_rounds = 1

    def train_result(param, dmat, num_rounds):
        result = {}
>       xgb.train(param, dmat, num_rounds, [(dmat, 'train')], verbose_eval=False,
                  evals_result=result)

tests/python-gpu/test_gpu_updaters.py:28: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'grow_policy': 'lossguide', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925b2f820>, num_boost_round = 1
evals = [(<xgboost.core.DMatrix object at 0x7fa925b2f820>, 'train')], obj = None
feval = None, maximize = None, early_stopping_rounds = None, evals_result = {}
verbose_eval = False, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'grow_policy': 'lossguide', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925b2f820>, num_boost_round = 1
evals = [(<xgboost.core.DMatrix object at 0x7fa925b2f820>, 'train')], obj = None
feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925b2f490>
evals_result = {}, maximize = None, verbose_eval = False
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925b2f250>
dtrain = <xgboost.core.DMatrix object at 0x7fa925b2f820>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:05] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
---------------------------------- Hypothesis ----------------------------------
Falsifying example: test_gpu_hist(
    self=<test_gpu_updaters.TestGPUUpdaters at 0x7fa925e57bb0>,
    param={'colsample_bylevel': 0.5,
     'colsample_bytree': 0.5,
     'grow_policy': 'lossguide',
     'max_bin': 2,
     'max_depth': 0,
     'max_leaves': 1,
     'min_child_weight': 0.5,
     'seed': 0,
     'single_precision_histogram': False},
    num_rounds=1,
    dataset=boston,
)
___________________ TestGPUUpdaters.test_categorical_32_cat ____________________

cls = <class '_pytest.runner.CallInfo'>
func = <function call_runtest_hook.<locals>.<lambda> at 0x7fa926004dc0>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: "Optional[Union[Type[BaseException], Tuple[Type[BaseException], ...]]]" = None,
    ) -> "CallInfo[TResult]":
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result = func()  # type: Optional[TResult]

.venv/lib/python3.8/site-packages/_pytest/runner.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: ihook(item=item, **kwds), when=when, reraise=reraise
    )

.venv/lib/python3.8/site-packages/_pytest/runner.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_runtest_call'>, args = ()
kwargs = {'item': <Function test_categorical_32_cat>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fa94e3b0580>
hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
kwargs = {'item': <Function test_categorical_32_cat>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
kwargs = {'item': <Function test_categorical_32_cat>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
caller_kwargs = {'item': <Function test_categorical_32_cat>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fa925daae80>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
caller_kwargs = {'item': <Function test_categorical_32_cat>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_categorical_32_cat>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
            item.runtest()
        except Exception as e:
            # Store trace info to allow postmortem debugging
            sys.last_type = type(e)
            sys.last_value = e
            assert e.__traceback__ is not None
            # Skip *this* frame
            sys.last_traceback = e.__traceback__.tb_next
>           raise e

.venv/lib/python3.8/site-packages/_pytest/runner.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_categorical_32_cat>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
>           item.runtest()

.venv/lib/python3.8/site-packages/_pytest/runner.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Function test_categorical_32_cat>

    def runtest(self) -> None:
        """Execute the underlying test function."""
>       self.ihook.pytest_pyfunc_call(pyfuncitem=self)

.venv/lib/python3.8/site-packages/_pytest/python.py:1627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_pyfunc_call'>, args = ()
kwargs = {'pyfuncitem': <Function test_categorical_32_cat>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fa94e3b0580>
hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_categorical_32_cat>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_categorical_32_cat>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_categorical_32_cat>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fa925daaeb0>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_categorical_32_cat>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pyfuncitem = <Function test_categorical_32_cat>

    @hookimpl(trylast=True)
    def pytest_pyfunc_call(pyfuncitem: "Function") -> Optional[object]:
        testfunction = pyfuncitem.obj
        if is_async_function(testfunction):
            async_warn_and_skip(pyfuncitem.nodeid)
        funcargs = pyfuncitem.funcargs
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
>       result = testfunction(**testargs)

.venv/lib/python3.8/site-packages/_pytest/python.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fa925daae50>

    def test_categorical_32_cat(self):
        '''32 hits the bound of integer bitset, so special test'''
        rows = 1000
        cols = 10
        cats = 32
        rounds = 4
>       self.run_categorical_basic(rows, cols, rounds, cats)

tests/python-gpu/test_gpu_updaters.py:97: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fa925daae50>, rows = 1000
cols = 10, rounds = 4, cats = 32

    def run_categorical_basic(self, rows, cols, rounds, cats):
>       import pandas as pd
E       ModuleNotFoundError: No module named 'pandas'

tests/python-gpu/test_gpu_updaters.py:45: ModuleNotFoundError
_____________________ TestGPUUpdaters.test_external_memory _____________________

cls = <class '_pytest.runner.CallInfo'>
func = <function call_runtest_hook.<locals>.<lambda> at 0x7fa9260a6dc0>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: "Optional[Union[Type[BaseException], Tuple[Type[BaseException], ...]]]" = None,
    ) -> "CallInfo[TResult]":
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result = func()  # type: Optional[TResult]

.venv/lib/python3.8/site-packages/_pytest/runner.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: ihook(item=item, **kwds), when=when, reraise=reraise
    )

.venv/lib/python3.8/site-packages/_pytest/runner.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_runtest_call'>, args = ()
kwargs = {'item': <Function test_external_memory>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fa94e3b0580>
hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
kwargs = {'item': <Function test_external_memory>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
kwargs = {'item': <Function test_external_memory>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
caller_kwargs = {'item': <Function test_external_memory>}, firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fa925f0ffd0>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
caller_kwargs = {'item': <Function test_external_memory>}, firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_external_memory>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
            item.runtest()
        except Exception as e:
            # Store trace info to allow postmortem debugging
            sys.last_type = type(e)
            sys.last_value = e
            assert e.__traceback__ is not None
            # Skip *this* frame
            sys.last_traceback = e.__traceback__.tb_next
>           raise e

.venv/lib/python3.8/site-packages/_pytest/runner.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_external_memory>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
>           item.runtest()

.venv/lib/python3.8/site-packages/_pytest/runner.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Function test_external_memory>

    def runtest(self) -> None:
        """Execute the underlying test function."""
>       self.ihook.pytest_pyfunc_call(pyfuncitem=self)

.venv/lib/python3.8/site-packages/_pytest/python.py:1627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_pyfunc_call'>, args = ()
kwargs = {'pyfuncitem': <Function test_external_memory>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fa94e3b0580>
hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_external_memory>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_external_memory>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_external_memory>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fa925f0f340>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_external_memory>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pyfuncitem = <Function test_external_memory>

    @hookimpl(trylast=True)
    def pytest_pyfunc_call(pyfuncitem: "Function") -> Optional[object]:
        testfunction = pyfuncitem.obj
        if is_async_function(testfunction):
            async_warn_and_skip(pyfuncitem.nodeid)
        funcargs = pyfuncitem.funcargs
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
>       result = testfunction(**testargs)

.venv/lib/python3.8/site-packages/_pytest/python.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fa925f0f250>

>   ???

tests/python-gpu/test_gpu_updaters.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arguments = ()
kwargs = {'self': <test_gpu_updaters.TestGPUUpdaters object at 0x7fa925f0f250>}
__tracebackhide__ = True
test = <function TestGPUUpdaters.test_external_memory at 0x7fa927a55b80>
settings = settings(database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples'), deadline=None, derand...nt_blob=False, report_multiple_bugs=True, stateful_step_count=50, suppress_health_check=(), verbosity=Verbosity.normal)
random = <random.Random object at 0x27f0a20>
processed_args = ((), {'self': <test_gpu_updaters.TestGPUUpdaters object at 0x7fa925f0f250>}, <function default_new_style_executor at 0...own>)}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_updaters.TestGPUUpdaters object at 0x7fa925f0f250>))
test_runner = <function default_new_style_executor at 0x7fa94d2a94c0>
search_strategy = WithRunner(TupleStrategy((just(()), fixed_dictionaries({'dataset': _dataset_weight_margin(),
 'num_rounds': integers(m...nown>)}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_updaters.TestGPUUpdaters object at 0x7fa925f0f250>)
runner = <test_gpu_updaters.TestGPUUpdaters object at 0x7fa925f0f250>
state = <hypothesis.core.StateForActualGivenExecution object at 0x7fa926016370>

    @impersonate(test)
    @define_function_signature(test.__name__, test.__doc__, argspec)
    def wrapped_test(*arguments, **kwargs):
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
    
        test = wrapped_test.hypothesis.inner_test
    
        if getattr(test, "is_hypothesis_test", False):
            raise InvalidArgument(
                (
                    "You have applied @given to the test %s more than once, which "
                    "wraps the test several times and is extremely slow. A "
                    "similar effect can be gained by combining the arguments "
                    "of the two calls to given. For example, instead of "
                    "@given(booleans()) @given(integers()), you could write "
                    "@given(booleans(), integers())"
                )
                % (test.__name__,)
            )
    
        settings = wrapped_test._hypothesis_internal_use_settings
    
        random = get_random_for_wrapped_test(test, wrapped_test)
    
        processed_args = process_arguments_to_given(
            wrapped_test, arguments, kwargs, given_kwargs, argspec
        )
        arguments, kwargs, test_runner, search_strategy = processed_args
    
        runner = getattr(search_strategy, "runner", None)
        if isinstance(runner, TestCase) and test.__name__ in dir(TestCase):
            msg = (
                "You have applied @given to the method %s, which is "
                "used by the unittest runner but is not itself a test."
                "  This is not useful in any way." % test.__name__
            )
            fail_health_check(settings, msg, HealthCheck.not_a_test_method)
        if bad_django_TestCase(runner):  # pragma: no cover
            # Covered by the Django tests, but not the pytest coverage task
            raise InvalidArgument(
                "You have applied @given to a method on %s, but this "
                "class does not inherit from the supported versions in "
                "`hypothesis.extra.django`.  Use the Hypothesis variants "
                "to ensure that each example is run in a separate "
                "database transaction." % qualname(type(runner))
            )
    
        state = StateForActualGivenExecution(
            test_runner, search_strategy, test, settings, random, wrapped_test
        )
    
        reproduce_failure = wrapped_test._hypothesis_internal_use_reproduce_failure
    
        # If there was a @reproduce_failure decorator, use it to reproduce
        # the error (or complain that we couldn't). Either way, this will
        # always raise some kind of error.
        if reproduce_failure is not None:
            expected_version, failure = reproduce_failure
            if expected_version != __version__:
                raise InvalidArgument(
                    (
                        "Attempting to reproduce a failure from a different "
                        "version of Hypothesis. This failure is from %s, but "
                        "you are currently running %r. Please change your "
                        "Hypothesis version to a matching one."
                    )
                    % (expected_version, __version__)
                )
            try:
                state.execute_once(
                    ConjectureData.for_buffer(decode_failure(failure)),
                    print_example=True,
                    is_final=True,
                )
                raise DidNotReproduce(
                    "Expected the test to raise an error, but it "
                    "completed successfully."
                )
            except StopTest:
                raise DidNotReproduce(
                    "The shape of the test data has changed in some way "
                    "from where this blob was defined. Are you sure "
                    "you're running the same test?"
                )
            except UnsatisfiedAssumption:
                raise DidNotReproduce(
                    "The test data failed to satisfy an assumption in the "
                    "test. Have you added it since this blob was "
                    "generated?"
                )
    
        # There was no @reproduce_failure, so start by running any explicit
        # examples from @example decorators.
        errors = list(
            execute_explicit_examples(state, wrapped_test, arguments, kwargs)
        )
        with local_settings(state.settings):
            if len(errors) > 1:
                # If we're not going to report multiple bugs, we would have
                # stopped running explicit examples at the first failure.
                assert state.settings.report_multiple_bugs
                for fragments, err in errors:
                    for f in fragments:
                        report(f)
                    tb_lines = traceback.format_exception(
                        type(err), err, err.__traceback__
                    )
                    report("".join(tb_lines))
                msg = "Hypothesis found %d failures in explicit examples."
                raise MultipleFailures(msg % (len(errors)))
            elif errors:
                fragments, the_error_hypothesis_found = errors[0]
                for f in fragments:
                    report(f)
                raise the_error_hypothesis_found
    
        # If there were any explicit examples, they all ran successfully.
        # The next step is to use the Conjecture engine to run the test on
        # many different inputs.
    
        if not (
            Phase.reuse in settings.phases or Phase.generate in settings.phases
        ):
            return
    
        try:
            if isinstance(runner, TestCase) and hasattr(runner, "subTest"):
                subTest = runner.subTest
                try:
                    runner.subTest = types.MethodType(fake_subTest, runner)
                    state.run_engine()
                finally:
                    runner.subTest = subTest
            else:
                state.run_engine()
        except BaseException as e:
            # The exception caught here should either be an actual test
            # failure (or MultipleFailures), or some kind of fatal error
            # that caused the engine to stop.
    
            generated_seed = wrapped_test._hypothesis_internal_use_generated_seed
            with local_settings(settings):
                if not (state.failed_normally or generated_seed is None):
                    if running_under_pytest:
                        report(
                            "You can add @seed(%(seed)d) to this test or "
                            "run pytest with --hypothesis-seed=%(seed)d "
                            "to reproduce this failure." % {"seed": generated_seed}
                        )
                    else:
                        report(
                            "You can add @seed(%d) to this test to "
                            "reproduce this failure." % (generated_seed,)
                        )
                # The dance here is to avoid showing users long tracebacks
                # full of Hypothesis internals they don't care about.
                # We have to do this inline, to avoid adding another
                # internal stack frame just when we've removed the rest.
                #
                # Using a variable for our trimmed error ensures that the line
                # which will actually appear in tracebacks is as clear as
                # possible - "raise the_error_hypothesis_found".
                the_error_hypothesis_found = e.with_traceback(
                    get_trimmed_traceback()
                )
>               raise the_error_hypothesis_found

.venv/lib/python3.8/site-packages/hypothesis/core.py:1162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fa925f0f250>
param = {'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'grow_policy': 'lossguide', ...}
num_rounds = 1, dataset = boston

    @given(parameter_strategy, strategies.integers(1, 20),
           tm.dataset_strategy)
    @settings(deadline=None)
    def test_external_memory(self, param, num_rounds, dataset):
        # We cannot handle empty dataset yet
        assume(len(dataset.y) > 0)
        param['tree_method'] = 'gpu_hist'
        param = dataset.set_params(param)
>       external_result = train_result(param, dataset.get_external_dmat(), num_rounds)

tests/python-gpu/test_gpu_updaters.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = boston

    def get_external_dmat(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            path = os.path.join(tmpdir, 'tmptmp_1234.csv')
            np.savetxt(path,
                       np.hstack((self.y.reshape(len(self.y), 1), self.X)),
                       delimiter=',')
            assert os.path.exists(path)
            uri = path + '?format=csv&label_column=0#tmptmp_'
            # The uri looks like:
            # 'tmptmp_1234.csv?format=csv&label_column=0#tmptmp_'
>           return xgb.DMatrix(uri, weight=self.w, base_margin=self.margin)

tests/python/testing.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.DMatrix object at 0x7fa925b8e7c0>
data = '/tmp/tmpp_ox_rdr/tmptmp_1234.csv?format=csv&label_column=0#tmptmp_'
label = None
weight = array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
       0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,..., 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
       0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])
base_margin = array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,
       0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,..., 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,
       0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])
missing = None, silent = False, feature_names = None, feature_types = None
nthread = None, enable_categorical = False

    def __init__(self, data, label=None, weight=None, base_margin=None,
                 missing=None,
                 silent=False,
                 feature_names=None,
                 feature_types=None,
                 nthread=None,
                 enable_categorical=False):
        """Parameters
        ----------
        data : os.PathLike/string/numpy.array/scipy.sparse/pd.DataFrame/
               dt.Frame/cudf.DataFrame/cupy.array/dlpack
            Data source of DMatrix.
            When data is string or os.PathLike type, it represents the path
            libsvm format txt file, csv file (by specifying uri parameter
            'path_to_csv?format=csv'), or binary file that xgboost can read
            from.
        label : list, numpy 1-D array or cudf.DataFrame, optional
            Label of the training data.
        missing : float, optional
            Value in the input data which needs to be present as a missing
            value. If None, defaults to np.nan.
        weight : list, numpy 1-D array or cudf.DataFrame , optional
            Weight for each instance.
    
            .. note:: For ranking task, weights are per-group.
    
                In ranking task, one weight is assigned to each group (not each
                data point). This is because we only care about the relative
                ordering of data points within each group, so it doesn't make
                sense to assign weights to individual data points.
    
        silent : boolean, optional
            Whether print messages during construction
        feature_names : list, optional
            Set names for features.
        feature_types : list, optional
            Set types for features.
        nthread : integer, optional
            Number of threads to use for loading data when parallelization is
            applicable. If -1, uses maximum threads available on the system.
    
        enable_categorical: boolean, optional
    
            .. versionadded:: 1.3.0
    
            Experimental support of specializing for categorical features.  Do
            not set to True unless you are interested in development.
            Currently it's only available for `gpu_hist` tree method with 1 vs
            rest (one hot) categorical split.  Also, JSON serialization format,
            `gpu_predictor` and pandas input are required.
    
        """
        if isinstance(data, list):
            raise TypeError('Input data can not be a list.')
    
        self.missing = missing if missing is not None else np.nan
        self.nthread = nthread if nthread is not None else -1
        self.silent = silent
    
        # force into void_p, mac need to pass things in as void_p
        if data is None:
            self.handle = None
            return
    
        from .data import dispatch_data_backend
>       handle, feature_names, feature_types = dispatch_data_backend(
            data, missing=self.missing,
            threads=self.nthread,
            feature_names=feature_names,
            feature_types=feature_types,
            enable_categorical=enable_categorical)

.venv/lib/python3.8/site-packages/xgboost/core.py:445: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = '/tmp/tmpp_ox_rdr/tmptmp_1234.csv?format=csv&label_column=0#tmptmp_'
missing = nan, threads = -1, feature_names = None, feature_types = None
enable_categorical = False

    def dispatch_data_backend(data, missing, threads,
                              feature_names, feature_types,
                              enable_categorical=False):
        '''Dispatch data for DMatrix.'''
        if _is_scipy_csr(data):
            return _from_scipy_csr(data, missing, feature_names, feature_types)
        if _is_scipy_csc(data):
            return _from_scipy_csc(data, missing, feature_names, feature_types)
        if _is_scipy_coo(data):
            return _from_scipy_csr(data.tocsr(), missing, feature_names, feature_types)
        if _is_numpy_array(data):
            return _from_numpy_array(data, missing, threads, feature_names,
                                     feature_types)
        if _is_uri(data):
>           return _from_uri(data, missing, feature_names, feature_types)

.venv/lib/python3.8/site-packages/xgboost/data.py:532: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = '/tmp/tmpp_ox_rdr/tmptmp_1234.csv?format=csv&label_column=0#tmptmp_'
missing = nan, feature_names = None, feature_types = None

    def _from_uri(data, missing, feature_names, feature_types):
        _warn_unused_missing(data, missing)
        handle = ctypes.c_void_p()
>       _check_call(_LIB.XGDMatrixCreateFromFile(c_str(os.fspath(data)),
                                                 ctypes.c_int(1),
                                                 ctypes.byref(handle)))

.venv/lib/python3.8/site-packages/xgboost/data.py:488: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:06] /home/otivedani/xgboost/src/data/data.cc:706: Encountered parser error:
E           [15:59:06] /home/otivedani/xgboost/src/data/./sparse_page_source.h:124: Cache file tmptmp_ exists already; Is there another DMatrix with the same cache prefix? Otherwise please remove it manually.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::data::CheckCacheFileExists(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)+0x19c) [0x7fa92b7ef1ec]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::data::SparsePageSource::SparsePageSource<xgboost::data::FileAdapter>(xgboost::data::FileAdapter*, float, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long)+0x1cf) [0x7fa92b80eeef]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::DMatrix* xgboost::DMatrix::Create<xgboost::data::FileAdapter>(xgboost::data::FileAdapter*, float, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long)+0x10d) [0x7fa92b81355d]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::DMatrix::Load(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long)+0x309) [0x7fa92b7dfc79]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGDMatrixCreateFromFile+0x129) [0x7fa92b74fe39]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]
E           
E           
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0xd3776) [0x7fa92b73e776]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGDMatrixCreateFromFile+0x129) [0x7fa92b74fe39]
E             [bt] (3) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (4) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (5) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]
E             [bt] (6) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x137c0) [0x7fa94d18f7c0]
E             [bt] (7) /home/otivedani/xgboost/.venv/bin/python3(_PyObject_MakeTpCall+0x296) [0x5f46d6]
E             [bt] (8) /home/otivedani/xgboost/.venv/bin/python3(_PyEval_EvalFrameDefault+0x5de6) [0x570936]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
---------------------------------- Hypothesis ----------------------------------
Falsifying example: test_external_memory(
    self=<test_gpu_updaters.TestGPUUpdaters at 0x7fa925f0f250>,
    param={'colsample_bylevel': 0.5,
     'colsample_bytree': 0.5,
     'grow_policy': 'lossguide',
     'max_bin': 2,
     'max_depth': 0,
     'max_leaves': 1,
     'min_child_weight': 0.5,
     'seed': 0,
     'single_precision_histogram': False},
    num_rounds=1,
    dataset=boston,
)
________________ TestGPUUpdaters.test_empty_dmatrix_prediction _________________

cls = <class '_pytest.runner.CallInfo'>
func = <function call_runtest_hook.<locals>.<lambda> at 0x7fa925b6d8b0>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: "Optional[Union[Type[BaseException], Tuple[Type[BaseException], ...]]]" = None,
    ) -> "CallInfo[TResult]":
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result = func()  # type: Optional[TResult]

.venv/lib/python3.8/site-packages/_pytest/runner.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: ihook(item=item, **kwds), when=when, reraise=reraise
    )

.venv/lib/python3.8/site-packages/_pytest/runner.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_runtest_call'>, args = ()
kwargs = {'item': <Function test_empty_dmatrix_prediction>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fa94e3b0580>
hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
kwargs = {'item': <Function test_empty_dmatrix_prediction>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
kwargs = {'item': <Function test_empty_dmatrix_prediction>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
caller_kwargs = {'item': <Function test_empty_dmatrix_prediction>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fa93b29b280>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
caller_kwargs = {'item': <Function test_empty_dmatrix_prediction>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_empty_dmatrix_prediction>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
            item.runtest()
        except Exception as e:
            # Store trace info to allow postmortem debugging
            sys.last_type = type(e)
            sys.last_value = e
            assert e.__traceback__ is not None
            # Skip *this* frame
            sys.last_traceback = e.__traceback__.tb_next
>           raise e

.venv/lib/python3.8/site-packages/_pytest/runner.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_empty_dmatrix_prediction>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
>           item.runtest()

.venv/lib/python3.8/site-packages/_pytest/runner.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Function test_empty_dmatrix_prediction>

    def runtest(self) -> None:
        """Execute the underlying test function."""
>       self.ihook.pytest_pyfunc_call(pyfuncitem=self)

.venv/lib/python3.8/site-packages/_pytest/python.py:1627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_pyfunc_call'>, args = ()
kwargs = {'pyfuncitem': <Function test_empty_dmatrix_prediction>}
notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fa94e3b0580>
hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_empty_dmatrix_prediction>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_empty_dmatrix_prediction>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_empty_dmatrix_prediction>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fa93b29b370>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_empty_dmatrix_prediction>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pyfuncitem = <Function test_empty_dmatrix_prediction>

    @hookimpl(trylast=True)
    def pytest_pyfunc_call(pyfuncitem: "Function") -> Optional[object]:
        testfunction = pyfuncitem.obj
        if is_async_function(testfunction):
            async_warn_and_skip(pyfuncitem.nodeid)
        funcargs = pyfuncitem.funcargs
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
>       result = testfunction(**testargs)

.venv/lib/python3.8/site-packages/_pytest/python.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fa93b29b2b0>

    def test_empty_dmatrix_prediction(self):
        # FIXME(trivialfis): This should be done with all updaters
        kRows = 0
        kCols = 100
    
        X = np.empty((kRows, kCols))
        y = np.empty((kRows))
    
        dtrain = xgb.DMatrix(X, y)
    
>       bst = xgb.train({'verbosity': 2,
                         'tree_method': 'gpu_hist',
                         'gpu_id': 0},
                        dtrain,
                        verbose_eval=True,
                        num_boost_round=6,
                        evals=[(dtrain, 'Train')])

tests/python-gpu/test_gpu_updaters.py:133: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'gpu_id': 0, 'tree_method': 'gpu_hist', 'verbosity': 2}
dtrain = <xgboost.core.DMatrix object at 0x7fa93b29b460>, num_boost_round = 6
evals = [(<xgboost.core.DMatrix object at 0x7fa93b29b460>, 'Train')], obj = None
feval = None, maximize = None, early_stopping_rounds = None, evals_result = None
verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'gpu_id': 0, 'tree_method': 'gpu_hist', 'validate_parameters': True, 'verbosity': 2}
dtrain = <xgboost.core.DMatrix object at 0x7fa93b29b460>, num_boost_round = 6
evals = [(<xgboost.core.DMatrix object at 0x7fa93b29b460>, 'Train')], obj = None
feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa93b29b520>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa93b29b4c0>
dtrain = <xgboost.core.DMatrix object at 0x7fa93b29b460>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:06] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
_______________ TestGPUUpdaters.test_specified_gpu_id_gpu_update _______________

cls = <class '_pytest.runner.CallInfo'>
func = <function call_runtest_hook.<locals>.<lambda> at 0x7fa9260a9940>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: "Optional[Union[Type[BaseException], Tuple[Type[BaseException], ...]]]" = None,
    ) -> "CallInfo[TResult]":
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result = func()  # type: Optional[TResult]

.venv/lib/python3.8/site-packages/_pytest/runner.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: ihook(item=item, **kwds), when=when, reraise=reraise
    )

.venv/lib/python3.8/site-packages/_pytest/runner.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_runtest_call'>, args = ()
kwargs = {'item': <Function test_specified_gpu_id_gpu_update>}
notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fa94e3b0580>
hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
kwargs = {'item': <Function test_specified_gpu_id_gpu_update>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
kwargs = {'item': <Function test_specified_gpu_id_gpu_update>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
caller_kwargs = {'item': <Function test_specified_gpu_id_gpu_update>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fa9260167f0>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
caller_kwargs = {'item': <Function test_specified_gpu_id_gpu_update>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_specified_gpu_id_gpu_update>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
            item.runtest()
        except Exception as e:
            # Store trace info to allow postmortem debugging
            sys.last_type = type(e)
            sys.last_value = e
            assert e.__traceback__ is not None
            # Skip *this* frame
            sys.last_traceback = e.__traceback__.tb_next
>           raise e

.venv/lib/python3.8/site-packages/_pytest/runner.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_specified_gpu_id_gpu_update>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
>           item.runtest()

.venv/lib/python3.8/site-packages/_pytest/runner.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Function test_specified_gpu_id_gpu_update>

    def runtest(self) -> None:
        """Execute the underlying test function."""
>       self.ihook.pytest_pyfunc_call(pyfuncitem=self)

.venv/lib/python3.8/site-packages/_pytest/python.py:1627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_pyfunc_call'>, args = ()
kwargs = {'pyfuncitem': <Function test_specified_gpu_id_gpu_update>}
notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fa94e3b0580>
hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_specified_gpu_id_gpu_update>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_specified_gpu_id_gpu_update>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_specified_gpu_id_gpu_update>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fa926016b80>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_specified_gpu_id_gpu_update>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pyfuncitem = <Function test_specified_gpu_id_gpu_update>

    @hookimpl(trylast=True)
    def pytest_pyfunc_call(pyfuncitem: "Function") -> Optional[object]:
        testfunction = pyfuncitem.obj
        if is_async_function(testfunction):
            async_warn_and_skip(pyfuncitem.nodeid)
        funcargs = pyfuncitem.funcargs
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
>       result = testfunction(**testargs)

.venv/lib/python3.8/site-packages/_pytest/python.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fa9260165e0>

>   ???

tests/python-gpu/test_gpu_updaters.py:149: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arguments = ()
kwargs = {'self': <test_gpu_updaters.TestGPUUpdaters object at 0x7fa9260165e0>}
__tracebackhide__ = True
test = <function TestGPUUpdaters.test_specified_gpu_id_gpu_update at 0x7fa927a5c0d0>
settings = settings(database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples'), deadline=None, derand...nt_blob=False, report_multiple_bugs=True, stateful_step_count=50, suppress_health_check=(), verbosity=Verbosity.normal)
random = <random.Random object at 0x292f0e0>
processed_args = ((), {'self': <test_gpu_updaters.TestGPUUpdaters object at 0x7fa9260165e0>}, <function default_new_style_executor at 0...e=10)}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_updaters.TestGPUUpdaters object at 0x7fa9260165e0>))
test_runner = <function default_new_style_executor at 0x7fa94d2a94c0>
search_strategy = WithRunner(TupleStrategy((just(()), fixed_dictionaries({'dataset': _dataset_weight_margin(),
 'gpu_id': integers(min_v...ue=10)}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_updaters.TestGPUUpdaters object at 0x7fa9260165e0>)
runner = <test_gpu_updaters.TestGPUUpdaters object at 0x7fa9260165e0>
state = <hypothesis.core.StateForActualGivenExecution object at 0x7fa925de1d90>

    @impersonate(test)
    @define_function_signature(test.__name__, test.__doc__, argspec)
    def wrapped_test(*arguments, **kwargs):
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
    
        test = wrapped_test.hypothesis.inner_test
    
        if getattr(test, "is_hypothesis_test", False):
            raise InvalidArgument(
                (
                    "You have applied @given to the test %s more than once, which "
                    "wraps the test several times and is extremely slow. A "
                    "similar effect can be gained by combining the arguments "
                    "of the two calls to given. For example, instead of "
                    "@given(booleans()) @given(integers()), you could write "
                    "@given(booleans(), integers())"
                )
                % (test.__name__,)
            )
    
        settings = wrapped_test._hypothesis_internal_use_settings
    
        random = get_random_for_wrapped_test(test, wrapped_test)
    
        processed_args = process_arguments_to_given(
            wrapped_test, arguments, kwargs, given_kwargs, argspec
        )
        arguments, kwargs, test_runner, search_strategy = processed_args
    
        runner = getattr(search_strategy, "runner", None)
        if isinstance(runner, TestCase) and test.__name__ in dir(TestCase):
            msg = (
                "You have applied @given to the method %s, which is "
                "used by the unittest runner but is not itself a test."
                "  This is not useful in any way." % test.__name__
            )
            fail_health_check(settings, msg, HealthCheck.not_a_test_method)
        if bad_django_TestCase(runner):  # pragma: no cover
            # Covered by the Django tests, but not the pytest coverage task
            raise InvalidArgument(
                "You have applied @given to a method on %s, but this "
                "class does not inherit from the supported versions in "
                "`hypothesis.extra.django`.  Use the Hypothesis variants "
                "to ensure that each example is run in a separate "
                "database transaction." % qualname(type(runner))
            )
    
        state = StateForActualGivenExecution(
            test_runner, search_strategy, test, settings, random, wrapped_test
        )
    
        reproduce_failure = wrapped_test._hypothesis_internal_use_reproduce_failure
    
        # If there was a @reproduce_failure decorator, use it to reproduce
        # the error (or complain that we couldn't). Either way, this will
        # always raise some kind of error.
        if reproduce_failure is not None:
            expected_version, failure = reproduce_failure
            if expected_version != __version__:
                raise InvalidArgument(
                    (
                        "Attempting to reproduce a failure from a different "
                        "version of Hypothesis. This failure is from %s, but "
                        "you are currently running %r. Please change your "
                        "Hypothesis version to a matching one."
                    )
                    % (expected_version, __version__)
                )
            try:
                state.execute_once(
                    ConjectureData.for_buffer(decode_failure(failure)),
                    print_example=True,
                    is_final=True,
                )
                raise DidNotReproduce(
                    "Expected the test to raise an error, but it "
                    "completed successfully."
                )
            except StopTest:
                raise DidNotReproduce(
                    "The shape of the test data has changed in some way "
                    "from where this blob was defined. Are you sure "
                    "you're running the same test?"
                )
            except UnsatisfiedAssumption:
                raise DidNotReproduce(
                    "The test data failed to satisfy an assumption in the "
                    "test. Have you added it since this blob was "
                    "generated?"
                )
    
        # There was no @reproduce_failure, so start by running any explicit
        # examples from @example decorators.
        errors = list(
            execute_explicit_examples(state, wrapped_test, arguments, kwargs)
        )
        with local_settings(state.settings):
            if len(errors) > 1:
                # If we're not going to report multiple bugs, we would have
                # stopped running explicit examples at the first failure.
                assert state.settings.report_multiple_bugs
                for fragments, err in errors:
                    for f in fragments:
                        report(f)
                    tb_lines = traceback.format_exception(
                        type(err), err, err.__traceback__
                    )
                    report("".join(tb_lines))
                msg = "Hypothesis found %d failures in explicit examples."
                raise MultipleFailures(msg % (len(errors)))
            elif errors:
                fragments, the_error_hypothesis_found = errors[0]
                for f in fragments:
                    report(f)
                raise the_error_hypothesis_found
    
        # If there were any explicit examples, they all ran successfully.
        # The next step is to use the Conjecture engine to run the test on
        # many different inputs.
    
        if not (
            Phase.reuse in settings.phases or Phase.generate in settings.phases
        ):
            return
    
        try:
            if isinstance(runner, TestCase) and hasattr(runner, "subTest"):
                subTest = runner.subTest
                try:
                    runner.subTest = types.MethodType(fake_subTest, runner)
                    state.run_engine()
                finally:
                    runner.subTest = subTest
            else:
                state.run_engine()
        except BaseException as e:
            # The exception caught here should either be an actual test
            # failure (or MultipleFailures), or some kind of fatal error
            # that caused the engine to stop.
    
            generated_seed = wrapped_test._hypothesis_internal_use_generated_seed
            with local_settings(settings):
                if not (state.failed_normally or generated_seed is None):
                    if running_under_pytest:
                        report(
                            "You can add @seed(%(seed)d) to this test or "
                            "run pytest with --hypothesis-seed=%(seed)d "
                            "to reproduce this failure." % {"seed": generated_seed}
                        )
                    else:
                        report(
                            "You can add @seed(%d) to this test to "
                            "reproduce this failure." % (generated_seed,)
                        )
                # The dance here is to avoid showing users long tracebacks
                # full of Hypothesis internals they don't care about.
                # We have to do this inline, to avoid adding another
                # internal stack frame just when we've removed the rest.
                #
                # Using a variable for our trimmed error ensures that the line
                # which will actually appear in tracebacks is as clear as
                # possible - "raise the_error_hypothesis_found".
                the_error_hypothesis_found = e.with_traceback(
                    get_trimmed_traceback()
                )
>               raise the_error_hypothesis_found

.venv/lib/python3.8/site-packages/hypothesis/core.py:1162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fa9260165e0>
dataset = boston, gpu_id = 0

    @pytest.mark.mgpu
    @given(tm.dataset_strategy, strategies.integers(0, 10))
    @settings(deadline=None, max_examples=10)
    def test_specified_gpu_id_gpu_update(self, dataset, gpu_id):
        param = {'tree_method': 'gpu_hist', 'gpu_id': gpu_id}
        param = dataset.set_params(param)
>       result = train_result(param, dataset.get_dmat(), 10)

tests/python-gpu/test_gpu_updaters.py:154: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

param = {'eval_metric': 'rmse', 'gpu_id': 0, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist'}
dmat = <xgboost.core.DMatrix object at 0x7fa925b398e0>, num_rounds = 10

    def train_result(param, dmat, num_rounds):
        result = {}
>       xgb.train(param, dmat, num_rounds, [(dmat, 'train')], verbose_eval=False,
                  evals_result=result)

tests/python-gpu/test_gpu_updaters.py:28: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eval_metric': 'rmse', 'gpu_id': 0, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist'}
dtrain = <xgboost.core.DMatrix object at 0x7fa925b398e0>, num_boost_round = 10
evals = [(<xgboost.core.DMatrix object at 0x7fa925b398e0>, 'train')], obj = None
feval = None, maximize = None, early_stopping_rounds = None, evals_result = {}
verbose_eval = False, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eval_metric': 'rmse', 'gpu_id': 0, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925b398e0>, num_boost_round = 10
evals = [(<xgboost.core.DMatrix object at 0x7fa925b398e0>, 'train')], obj = None
feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925b397f0>
evals_result = {}, maximize = None, verbose_eval = False
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925b39940>
dtrain = <xgboost.core.DMatrix object at 0x7fa925b398e0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:06] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
---------------------------------- Hypothesis ----------------------------------
Falsifying example: test_specified_gpu_id_gpu_update(
    self=<test_gpu_updaters.TestGPUUpdaters at 0x7fa9260165e0>,
    dataset=boston,
    gpu_id=0,
)
________________________ test_gpu_binary_classification ________________________

cls = <class '_pytest.runner.CallInfo'>
func = <function call_runtest_hook.<locals>.<lambda> at 0x7fa925f86af0>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: "Optional[Union[Type[BaseException], Tuple[Type[BaseException], ...]]]" = None,
    ) -> "CallInfo[TResult]":
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result = func()  # type: Optional[TResult]

.venv/lib/python3.8/site-packages/_pytest/runner.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: ihook(item=item, **kwds), when=when, reraise=reraise
    )

.venv/lib/python3.8/site-packages/_pytest/runner.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_runtest_call'>, args = ()
kwargs = {'item': <Function test_gpu_binary_classification>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fa94e3b0580>
hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
kwargs = {'item': <Function test_gpu_binary_classification>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
kwargs = {'item': <Function test_gpu_binary_classification>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
caller_kwargs = {'item': <Function test_gpu_binary_classification>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fa925dce070>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fa92a6cb5b0>>]
caller_kwargs = {'item': <Function test_gpu_binary_classification>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_gpu_binary_classification>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
            item.runtest()
        except Exception as e:
            # Store trace info to allow postmortem debugging
            sys.last_type = type(e)
            sys.last_value = e
            assert e.__traceback__ is not None
            # Skip *this* frame
            sys.last_traceback = e.__traceback__.tb_next
>           raise e

.venv/lib/python3.8/site-packages/_pytest/runner.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_gpu_binary_classification>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
>           item.runtest()

.venv/lib/python3.8/site-packages/_pytest/runner.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Function test_gpu_binary_classification>

    def runtest(self) -> None:
        """Execute the underlying test function."""
>       self.ihook.pytest_pyfunc_call(pyfuncitem=self)

.venv/lib/python3.8/site-packages/_pytest/python.py:1627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_pyfunc_call'>, args = ()
kwargs = {'pyfuncitem': <Function test_gpu_binary_classification>}
notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fa94e3b0580>
hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_gpu_binary_classification>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_gpu_binary_classification>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_gpu_binary_classification>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fa925dce250>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_gpu_binary_classification>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pyfuncitem = <Function test_gpu_binary_classification>

    @hookimpl(trylast=True)
    def pytest_pyfunc_call(pyfuncitem: "Function") -> Optional[object]:
        testfunction = pyfuncitem.obj
        if is_async_function(testfunction):
            async_warn_and_skip(pyfuncitem.nodeid)
        funcargs = pyfuncitem.funcargs
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
>       result = testfunction(**testargs)

.venv/lib/python3.8/site-packages/_pytest/python.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def test_gpu_binary_classification():
        from sklearn.datasets import load_digits
        from sklearn.model_selection import KFold
    
        digits = load_digits(2)
        y = digits['target']
        X = digits['data']
        kf = KFold(n_splits=2, shuffle=True, random_state=rng)
        for cls in (xgb.XGBClassifier, xgb.XGBRFClassifier):
            for train_index, test_index in kf.split(X, y):
>               xgb_model = cls(
                    random_state=42, tree_method='gpu_hist',
                    n_estimators=4, gpu_id='0').fit(X[train_index], y[train_index])

tests/python-gpu/test_gpu_with_sklearn.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_by... scale_pos_weight=None, subsample=None, tree_method='gpu_hist',
              validate_parameters=None, verbosity=None)
X = array([[ 0.,  0.,  0., ..., 10.,  0.,  0.],
       [ 0.,  0.,  1., ...,  3.,  0.,  0.],
       [ 0.,  0.,  0., ..., 16... 0.,  1., ..., 10.,  0.,  0.],
       [ 0.,  0.,  8., ...,  7.,  0.,  0.],
       [ 0.,  0.,  6., ...,  6.,  0.,  0.]])
y = array([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
       0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,...0, 0, 1, 1, 0, 0, 1, 1, 1,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
       1, 1, 1, 0])
sample_weight = None, base_margin = None, eval_set = None, eval_metric = None
early_stopping_rounds = None, verbose = True, xgb_model = None
sample_weight_eval_set = None, feature_weights = None, callbacks = None

    def fit(self, X, y, sample_weight=None, base_margin=None,
            eval_set=None, eval_metric=None,
            early_stopping_rounds=None, verbose=True, xgb_model=None,
            sample_weight_eval_set=None, feature_weights=None, callbacks=None):
        # pylint: disable = attribute-defined-outside-init,arguments-differ
    
        evals_result = {}
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
    
        xgb_options = self.get_xgb_params()
    
        if callable(self.objective):
            obj = _objective_decorator(self.objective)
            # Use default value. Is it really not used ?
            xgb_options["objective"] = "binary:logistic"
        else:
            obj = None
    
        if self.n_classes_ > 2:
            # Switch to using a multiclass objective in the underlying
            # XGB instance
            xgb_options['objective'] = 'multi:softprob'
            xgb_options['num_class'] = self.n_classes_
    
        feval = eval_metric if callable(eval_metric) else None
        if eval_metric is not None:
            if callable(eval_metric):
                eval_metric = None
            else:
                xgb_options.update({"eval_metric": eval_metric})
    
        self._le = XGBoostLabelEncoder().fit(y)
        training_labels = self._le.transform(y)
    
        if eval_set is not None:
            if sample_weight_eval_set is None:
                sample_weight_eval_set = [None] * len(eval_set)
            else:
                assert len(sample_weight_eval_set) == len(eval_set)
            evals = list(
                DMatrix(eval_set[i][0],
                        label=self._le.transform(eval_set[i][1]),
                        missing=self.missing, weight=sample_weight_eval_set[i],
                        nthread=self.n_jobs)
                for i in range(len(eval_set))
            )
            nevals = len(evals)
            eval_names = ["validation_{}".format(i) for i in range(nevals)]
            evals = list(zip(evals, eval_names))
        else:
            evals = ()
    
        if len(X.shape) != 2:
            # Simply raise an error here since there might be many
            # different ways of reshaping
            raise ValueError(
                'Please reshape the input data X into 2-dimensional matrix.')
    
        self._features_count = X.shape[1]
        self.n_features_in_ = self._features_count
    
        train_dmatrix = DMatrix(X, label=training_labels, weight=sample_weight,
                                base_margin=base_margin,
                                missing=self.missing, nthread=self.n_jobs)
        train_dmatrix.set_info(feature_weights=feature_weights)
    
>       self._Booster = train(xgb_options, train_dmatrix,
                              self.get_num_boosting_rounds(),
                              evals=evals,
                              early_stopping_rounds=early_stopping_rounds,
                              evals_result=evals_result, obj=obj, feval=feval,
                              verbose_eval=verbose, xgb_model=xgb_model,
                              callbacks=callbacks)

.venv/lib/python3.8/site-packages/xgboost/sklearn.py:836: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925dce100>, num_boost_round = 4
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = {}, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa925dce100>, num_boost_round = 4
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925dcecd0>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925dce190>
dtrain = <xgboost.core.DMatrix object at 0x7fa925dce100>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:06] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
________ TestGPUBoostFromPrediction.test_boost_from_prediction_gpu_hist ________

self = <unittest.case._Outcome object at 0x7fa93b29b100>
test_case = <test_gpu_with_sklearn.TestGPUBoostFromPrediction testMethod=test_boost_from_prediction_gpu_hist>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_with_sklearn.TestGPUBoostFromPrediction testMethod=test_boost_from_prediction_gpu_hist>
result = <TestCaseFunction test_boost_from_prediction_gpu_hist>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_with_sklearn.TestGPUBoostFromPrediction testMethod=test_boost_from_prediction_gpu_hist>
method = <bound method TestGPUBoostFromPrediction.test_boost_from_prediction_gpu_hist of <test_gpu_with_sklearn.TestGPUBoostFromPrediction testMethod=test_boost_from_prediction_gpu_hist>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_with_sklearn.TestGPUBoostFromPrediction testMethod=test_boost_from_prediction_gpu_hist>

    def test_boost_from_prediction_gpu_hist(self):
>       self.cpu_test.run_boost_from_prediction('gpu_hist')

tests/python-gpu/test_gpu_with_sklearn.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_with_sklearn.TestBoostFromPrediction testMethod=runTest>
tree_method = 'gpu_hist'

    def run_boost_from_prediction(self, tree_method):
        from sklearn.datasets import load_breast_cancer
        X, y = load_breast_cancer(return_X_y=True)
        model_0 = xgb.XGBClassifier(
            learning_rate=0.3, random_state=0, n_estimators=4,
            tree_method=tree_method)
>       model_0.fit(X=X, y=y)

tests/python/test_with_sklearn.py:991: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_by... scale_pos_weight=None, subsample=None, tree_method='gpu_hist',
              validate_parameters=None, verbosity=None)
X = array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,
        1.189e-01],
       [2.057e+01, 1.777e+01, ...4.087e-01,
        1.240e-01],
       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,
        7.039e-02]])
y = array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])
sample_weight = None, base_margin = None, eval_set = None, eval_metric = None
early_stopping_rounds = None, verbose = True, xgb_model = None
sample_weight_eval_set = None, feature_weights = None, callbacks = None

    def fit(self, X, y, sample_weight=None, base_margin=None,
            eval_set=None, eval_metric=None,
            early_stopping_rounds=None, verbose=True, xgb_model=None,
            sample_weight_eval_set=None, feature_weights=None, callbacks=None):
        # pylint: disable = attribute-defined-outside-init,arguments-differ
    
        evals_result = {}
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
    
        xgb_options = self.get_xgb_params()
    
        if callable(self.objective):
            obj = _objective_decorator(self.objective)
            # Use default value. Is it really not used ?
            xgb_options["objective"] = "binary:logistic"
        else:
            obj = None
    
        if self.n_classes_ > 2:
            # Switch to using a multiclass objective in the underlying
            # XGB instance
            xgb_options['objective'] = 'multi:softprob'
            xgb_options['num_class'] = self.n_classes_
    
        feval = eval_metric if callable(eval_metric) else None
        if eval_metric is not None:
            if callable(eval_metric):
                eval_metric = None
            else:
                xgb_options.update({"eval_metric": eval_metric})
    
        self._le = XGBoostLabelEncoder().fit(y)
        training_labels = self._le.transform(y)
    
        if eval_set is not None:
            if sample_weight_eval_set is None:
                sample_weight_eval_set = [None] * len(eval_set)
            else:
                assert len(sample_weight_eval_set) == len(eval_set)
            evals = list(
                DMatrix(eval_set[i][0],
                        label=self._le.transform(eval_set[i][1]),
                        missing=self.missing, weight=sample_weight_eval_set[i],
                        nthread=self.n_jobs)
                for i in range(len(eval_set))
            )
            nevals = len(evals)
            eval_names = ["validation_{}".format(i) for i in range(nevals)]
            evals = list(zip(evals, eval_names))
        else:
            evals = ()
    
        if len(X.shape) != 2:
            # Simply raise an error here since there might be many
            # different ways of reshaping
            raise ValueError(
                'Please reshape the input data X into 2-dimensional matrix.')
    
        self._features_count = X.shape[1]
        self.n_features_in_ = self._features_count
    
        train_dmatrix = DMatrix(X, label=training_labels, weight=sample_weight,
                                base_margin=base_margin,
                                missing=self.missing, nthread=self.n_jobs)
        train_dmatrix.set_info(feature_weights=feature_weights)
    
>       self._Booster = train(xgb_options, train_dmatrix,
                              self.get_num_boosting_rounds(),
                              evals=evals,
                              early_stopping_rounds=early_stopping_rounds,
                              evals_result=evals_result, obj=obj, feval=feval,
                              verbose_eval=verbose, xgb_model=xgb_model,
                              callbacks=callbacks)

.venv/lib/python3.8/site-packages/xgboost/sklearn.py:836: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa93b29b850>, num_boost_round = 4
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = {}, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fa93b29b850>, num_boost_round = 4
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa93b29b940>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa93b29b0d0>
dtrain = <xgboost.core.DMatrix object at 0x7fa93b29b850>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:06] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
_________________ TestMonotonicConstraints.test_gpu_hist_basic _________________

self = <unittest.case._Outcome object at 0x7fa93bc53070>
test_case = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_basic>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_basic>
result = <TestCaseFunction test_gpu_hist_basic>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_basic>
method = <bound method TestMonotonicConstraints.test_gpu_hist_basic of <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_basic>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_basic>

    @pytest.mark.skipif(**tm.no_sklearn())
    def test_gpu_hist_basic(self):
>       assert_constraint(1, 'gpu_hist')

tests/python-gpu/test_monotonic_constraints.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

constraint = 1, tree_method = 'gpu_hist'

    def assert_constraint(constraint, tree_method):
        from sklearn.datasets import make_regression
        n = 1000
        X, y = make_regression(n, random_state=rng, n_features=1, n_informative=1)
        dtrain = xgb.DMatrix(X, y)
        param = {}
        param['tree_method'] = tree_method
        param['monotone_constraints'] = "(" + str(constraint) + ")"
>       bst = xgb.train(param, dtrain)

tests/python-gpu/test_monotonic_constraints.py:31: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'monotone_constraints': '(1)', 'tree_method': 'gpu_hist'}
dtrain = <xgboost.core.DMatrix object at 0x7fa93bc53190>, num_boost_round = 10
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'monotone_constraints': '(1)', 'tree_method': 'gpu_hist', 'validate_parameters': True}
dtrain = <xgboost.core.DMatrix object at 0x7fa93bc53190>, num_boost_round = 10
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa93bc530d0>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa93bc531c0>
dtrain = <xgboost.core.DMatrix object at 0x7fa93bc53190>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:07] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
_______________ TestMonotonicConstraints.test_gpu_hist_depthwise _______________

self = <unittest.case._Outcome object at 0x7fa925f16f10>
test_case = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_depthwise>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_depthwise>
result = <TestCaseFunction test_gpu_hist_depthwise>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_depthwise>
method = <bound method TestMonotonicConstraints.test_gpu_hist_depthwise of <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_depthwise>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_depthwise>

    def test_gpu_hist_depthwise(self):
        params = {
            'tree_method': 'gpu_hist',
            'grow_policy': 'depthwise',
            'monotone_constraints': '(1, -1)'
        }
>       model = xgb.train(params, tmc.training_dset)

tests/python-gpu/test_monotonic_constraints.py:53: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'grow_policy': 'depthwise', 'monotone_constraints': '(1, -1)', 'tree_method': 'gpu_hist'}
dtrain = <xgboost.core.DMatrix object at 0x7fa9279d8c10>, num_boost_round = 10
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'grow_policy': 'depthwise', 'monotone_constraints': '(1, -1)', 'tree_method': 'gpu_hist', 'validate_parameters': True}
dtrain = <xgboost.core.DMatrix object at 0x7fa9279d8c10>, num_boost_round = 10
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa925f165b0>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa925f162e0>
dtrain = <xgboost.core.DMatrix object at 0x7fa9279d8c10>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:07] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
_______________ TestMonotonicConstraints.test_gpu_hist_lossguide _______________

self = <unittest.case._Outcome object at 0x7fa92609c1c0>
test_case = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_lossguide>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_lossguide>
result = <TestCaseFunction test_gpu_hist_lossguide>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_lossguide>
method = <bound method TestMonotonicConstraints.test_gpu_hist_lossguide of <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_lossguide>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_lossguide>

    def test_gpu_hist_lossguide(self):
        params = {
            'tree_method': 'gpu_hist',
            'grow_policy': 'lossguide',
            'monotone_constraints': '(1, -1)'
        }
>       model = xgb.train(params, tmc.training_dset)

tests/python-gpu/test_monotonic_constraints.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'grow_policy': 'lossguide', 'monotone_constraints': '(1, -1)', 'tree_method': 'gpu_hist'}
dtrain = <xgboost.core.DMatrix object at 0x7fa9279d8c10>, num_boost_round = 10
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

.venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'grow_policy': 'lossguide', 'monotone_constraints': '(1, -1)', 'tree_method': 'gpu_hist', 'validate_parameters': True}
dtrain = <xgboost.core.DMatrix object at 0x7fa9279d8c10>, num_boost_round = 10
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fa92609c1f0>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

.venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fa92609cd30>
dtrain = <xgboost.core.DMatrix object at 0x7fa9279d8c10>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

.venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [15:59:07] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fa92b75df79]
E             [bt] (1) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fa92b84e8c5]
E             [bt] (2) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fa92b853658]
E             [bt] (3) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fa92b890cff]
E             [bt] (4) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fa92b87ea0e]
E             [bt] (5) /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fa92b754549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fa94d176ff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fa94d17640a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fa94d18f28c]

.venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
=============================== warnings summary ===============================
python-gpu/test_gpu_basic_models.py::TestGPUBasicModels::test_eta_decay_gpu_hist
  /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/xgboost/training.py:17: DeprecationWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
    warnings.warn(f'Old style callback is deprecated.  See: {link}', DeprecationWarning)

python-gpu/test_gpu_with_sklearn.py::test_gpu_binary_classification
  /home/otivedani/xgboost/.venv/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass n_class=2 as keyword args. From version 0.25 passing these as positional arguments will result in an error
    warnings.warn("Pass {} as keyword args. From version 0.25 "

-- Docs: https://docs.pytest.org/en/stable/warnings.html
=========================== short test summary info ============================
FAILED tests/python-gpu/test_gpu_basic_models.py::TestGPUBasicModels::test_deterministic_gpu_hist
FAILED tests/python-gpu/test_gpu_basic_models.py::TestGPUBasicModels::test_eta_decay_gpu_hist
FAILED tests/python-gpu/test_gpu_interaction_constraints.py::TestGPUInteractionConstraints::test_interaction_constraints
FAILED tests/python-gpu/test_gpu_interaction_constraints.py::TestGPUInteractionConstraints::test_training_accuracy
FAILED tests/python-gpu/test_gpu_linear.py::TestGPULinear::test_gpu_coordinate
FAILED tests/python-gpu/test_gpu_linear.py::TestGPULinear::test_gpu_coordinate_regularised
FAILED tests/python-gpu/test_gpu_pickling.py::TestPickling::test_pickled_predictor
FAILED tests/python-gpu/test_gpu_pickling.py::TestPickling::test_pickling - x...
FAILED tests/python-gpu/test_gpu_pickling.py::TestPickling::test_predict_sklearn_pickle
FAILED tests/python-gpu/test_gpu_pickling.py::TestPickling::test_training_on_cpu_only_env
FAILED tests/python-gpu/test_gpu_pickling.py::TestPickling::test_wrap_gpu_id
FAILED tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_multi_predict
FAILED tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_predict
FAILED tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_shap - x...
FAILED tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_shap_interactions
FAILED tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_sklearn
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_map_auc
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_map_map
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_map_ndcg
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_ndcg_auc
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_ndcg_map
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_ndcg_ndcg
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_pairwise_auc_metric
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_pairwise_map_metric
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_pairwise_ndcg_metric
FAILED tests/python-gpu/test_gpu_training_continuation.py::TestGPUTrainingContinuation::test_gpu_training_continuation_binary
FAILED tests/python-gpu/test_gpu_training_continuation.py::TestGPUTrainingContinuation::test_gpu_training_continuation_json
FAILED tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_gpu_hist
FAILED tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_categorical_32_cat
FAILED tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_external_memory
FAILED tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_empty_dmatrix_prediction
FAILED tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_specified_gpu_id_gpu_update
FAILED tests/python-gpu/test_gpu_with_sklearn.py::test_gpu_binary_classification
FAILED tests/python-gpu/test_gpu_with_sklearn.py::TestGPUBoostFromPrediction::test_boost_from_prediction_gpu_hist
FAILED tests/python-gpu/test_monotonic_constraints.py::TestMonotonicConstraints::test_gpu_hist_basic
FAILED tests/python-gpu/test_monotonic_constraints.py::TestMonotonicConstraints::test_gpu_hist_depthwise
FAILED tests/python-gpu/test_monotonic_constraints.py::TestMonotonicConstraints::test_gpu_hist_lossguide
======= 37 failed, 1 passed, 31 skipped, 2 warnings in 206.18s (0:03:26) =======
============================= test session starts ==============================
platform linux -- Python 3.8.5, pytest-6.1.1, py-1.9.0, pluggy-0.13.1 -- /home/otivedani/xgboost/.venv/bin/python3
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples')
rootdir: /home/otivedani/xgboost/tests, configfile: pytest.ini
plugins: hypothesis-5.37.4
collecting ... collected 69 items

tests/python-gpu/test_device_quantile_dmatrix.py::TestDeviceQuantileDMatrix::test_dmatrix_cupy_init SKIPPED
tests/python-gpu/test_device_quantile_dmatrix.py::TestDeviceQuantileDMatrix::test_dmatrix_feature_weights SKIPPED
tests/python-gpu/test_device_quantile_dmatrix.py::TestDeviceQuantileDMatrix::test_dmatrix_numpy_init PASSED
tests/python-gpu/test_from_cudf.py::TestFromColumnar::test_simple_dmatrix_from_cudf SKIPPED
tests/python-gpu/test_from_cudf.py::TestFromColumnar::test_device_dmatrix_from_cudf SKIPPED
tests/python-gpu/test_from_cudf.py::TestFromColumnar::test_cudf_training_simple_dmatrix SKIPPED
tests/python-gpu/test_from_cudf.py::TestFromColumnar::test_cudf_training_device_dmatrix SKIPPED
tests/python-gpu/test_from_cudf.py::TestFromColumnar::test_cudf_metainfo_simple_dmatrix SKIPPED
tests/python-gpu/test_from_cudf.py::TestFromColumnar::test_cudf_metainfo_device_dmatrix SKIPPED
tests/python-gpu/test_from_cudf.py::test_from_cudf_iter SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_simple_dmat_from_cupy SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_device_dmat_from_cupy SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_cupy_training_device_dmat SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_cupy_training_simple_dmat SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_cupy_metainfo_simple_dmat SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_cupy_metainfo_device_dmat SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_dlpack_simple_dmat SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_dlpack_device_dmat SKIPPED
tests/python-gpu/test_from_cupy.py::TestFromCupy::test_specified_device SKIPPED
tests/python-gpu/test_gpu_basic_models.py::TestGPUBasicModels::test_deterministic_gpu_hist FAILED
tests/python-gpu/test_gpu_basic_models.py::TestGPUBasicModels::test_eta_decay_gpu_hist FAILED
tests/python-gpu/test_gpu_demos.py::test_data_iterator SKIPPED
tests/python-gpu/test_gpu_demos.py::test_dask_training SKIPPED
tests/python-gpu/test_gpu_interaction_constraints.py::TestGPUInteractionConstraints::test_interaction_constraints FAILED
tests/python-gpu/test_gpu_interaction_constraints.py::TestGPUInteractionConstraints::test_training_accuracy FAILED
tests/python-gpu/test_gpu_linear.py::TestGPULinear::test_gpu_coordinate Falsifying example: test_gpu_coordinate(
    self=<test_gpu_linear.TestGPULinear at 0x7fdf505c12e0>,
    param={'booster': 'gblinear',
     'eta': 0.01,
     'feature_selector': 'cyclic',
     'nthread': 1,
     'tolerance': 1e-05,
     'top_k': 1},
    num_rounds=10,
    dataset=boston,
)
FAILED
tests/python-gpu/test_gpu_linear.py::TestGPULinear::test_gpu_coordinate_regularised Falsifying example: test_gpu_coordinate_regularised(
    self=<test_gpu_linear.TestGPULinear at 0x7fdf50478a30>,
    param={'booster': 'gblinear',
     'eta': 0.01,
     'feature_selector': 'cyclic',
     'nthread': 1,
     'tolerance': 1e-05,
     'top_k': 1},
    num_rounds=10,
    dataset=boston,
    alpha=1e-05,
    lambd=1e-05,
)
FAILED
tests/python-gpu/test_gpu_pickling.py::TestPickling::test_pickled_predictor FAILED
tests/python-gpu/test_gpu_pickling.py::TestPickling::test_pickling FAILED
tests/python-gpu/test_gpu_pickling.py::TestPickling::test_predict_sklearn_pickle FAILED
tests/python-gpu/test_gpu_pickling.py::TestPickling::test_training_on_cpu_only_env ============================= test session starts ==============================
platform linux -- Python 3.8.5, pytest-6.1.1, py-1.9.0, pluggy-0.13.1 -- /home/otivedani/xgboost/.venv/bin/python3
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples')
rootdir: /home/otivedani/xgboost/tests, configfile: pytest.ini
plugins: hypothesis-5.37.4
collecting ... collected 1 item

tests/python-gpu/load_pickle.py::TestLoadPickle::test_training_on_cpu_only_env FAILED

=================================== FAILURES ===================================
_________________ TestLoadPickle.test_training_on_cpu_only_env _________________

self = <unittest.case._Outcome object at 0x7f79a5b5c790>
test_case = <load_pickle.TestLoadPickle testMethod=test_training_on_cpu_only_env>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <load_pickle.TestLoadPickle testMethod=test_training_on_cpu_only_env>
result = <TestCaseFunction test_training_on_cpu_only_env>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <load_pickle.TestLoadPickle testMethod=test_training_on_cpu_only_env>
method = <bound method TestLoadPickle.test_training_on_cpu_only_env of <load_pickle.TestLoadPickle testMethod=test_training_on_cpu_only_env>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <load_pickle.TestLoadPickle testMethod=test_training_on_cpu_only_env>

    def test_training_on_cpu_only_env(self):
        assert os.environ['CUDA_VISIBLE_DEVICES'] == '-1'
        rng = np.random.RandomState(1994)
        X = rng.randn(10, 10)
        y = rng.randn(10)
        with tb.captured_output() as (out, err):
            # Test no thrust exception is thrown
            with pytest.raises(xgb.core.XGBoostError):
                xgb.train({'tree_method': 'gpu_hist'}, xgb.DMatrix(X, y))
    
>           assert out.getvalue().find('No visible GPU is found') != -1
E           AssertionError: assert -1 != -1
E            +  where -1 = <built-in method find of str object at 0x7f79c99522f0>('No visible GPU is found')
E            +    where <built-in method find of str object at 0x7f79c99522f0> = ''.find
E            +      where '' = <built-in method getvalue of _io.StringIO object at 0x7f79a5ba9d30>()
E            +        where <built-in method getvalue of _io.StringIO object at 0x7f79a5ba9d30> = <_io.StringIO object at 0x7f79a5ba9d30>.getvalue

tests/python-gpu/load_pickle.py:69: AssertionError
=========================== short test summary info ============================
FAILED tests/python-gpu/load_pickle.py::TestLoadPickle::test_training_on_cpu_only_env
============================== 1 failed in 0.09s ===============================
FAILED
tests/python-gpu/test_gpu_pickling.py::TestPickling::test_wrap_gpu_id FAILED
tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_inplace_predict_cudf SKIPPED
tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_inplace_predict_cupy SKIPPED
tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_multi_predict FAILED
tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_predict FAILED
tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_shap Falsifying example: test_shap(
    self=<test_gpu_prediction.TestGPUPredict testMethod=test_shap>,
    num_rounds=1,
    dataset=boston,
    param={'max_depth': 0, 'max_leaves': 1, 'num_parallel_tree': 1},
)
FAILED
tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_shap_interactions Falsifying example: test_shap_interactions(
    self=<test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>,
    num_rounds=1,
    dataset=sparse,
    param={'max_depth': 0, 'max_leaves': 1, 'num_parallel_tree': 1},
)
Traceback (most recent call last):
  File "/home/otivedani/xgboost/tests/python-gpu/test_gpu_prediction.py", line 217, in test_shap_interactions
    pytest.xfail(reason=f'GPU shap with sparse is flaky: {issue}')
  File "/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/outcomes.py", line 172, in xfail
    raise XFailed(reason)
_pytest.outcomes.XFailed: GPU shap with sparse is flaky: https://github.com/dmlc/xgboost/issues/6074

Falsifying example: test_shap_interactions(
    self=<test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>,
    num_rounds=1,
    dataset=boston,
    param={'max_depth': 0, 'max_leaves': 1, 'num_parallel_tree': 1},
)
Traceback (most recent call last):
  File "/home/otivedani/xgboost/tests/python-gpu/test_gpu_prediction.py", line 222, in test_shap_interactions
    bst = xgb.train(param, dmat, num_rounds)
  File "/home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/training.py", line 216, in train
    bst = _train_internal(params, dtrain,
  File "/home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/training.py", line 111, in _train_internal
    bst.update(dtrain, i, obj)
  File "/home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/core.py", line 1178, in update
    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
  File "/home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/core.py", line 186, in _check_call
    raise XGBoostError(py_str(_LIB.XGBGetLastError()))
xgboost.core.XGBoostError: [22:16:33] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
Stack trace:
  [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
  [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::GetPredictor(xgboost::HostDeviceVector<float> const*, xgboost::DMatrix*) const+0x225) [0x7fdf56032c35]
  [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::PredictBatch(xgboost::DMatrix*, xgboost::PredictionCacheEntry*, bool, unsigned int)+0x49) [0x7fdf56032f29]
  [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x3b2) [0x7fdf56062d42]
  [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x137c0) [0x7fdf779737c0]



FAILED
tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_sklearn FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_map_auc FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_map_map FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_map_ndcg FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_ndcg_auc FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_ndcg_map FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_ndcg_ndcg FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_pairwise_auc_metric FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_pairwise_map_metric FAILED
tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_pairwise_ndcg_metric FAILED
tests/python-gpu/test_gpu_training_continuation.py::TestGPUTrainingContinuation::test_gpu_training_continuation_binary FAILED
tests/python-gpu/test_gpu_training_continuation.py::TestGPUTrainingContinuation::test_gpu_training_continuation_json FAILED
tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_gpu_hist Falsifying example: test_gpu_hist(
    self=<test_gpu_updaters.TestGPUUpdaters at 0x7fdf52188700>,
    param={'colsample_bylevel': 0.5,
     'colsample_bytree': 0.5,
     'grow_policy': 'lossguide',
     'max_bin': 2,
     'max_depth': 0,
     'max_leaves': 1,
     'min_child_weight': 0.5,
     'seed': 0,
     'single_precision_histogram': False},
    num_rounds=1,
    dataset=boston,
)
FAILED
tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_categorical SKIPPED
tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_categorical_32_cat FAILED
tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_gpu_hist_device_dmatrix SKIPPED
tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_external_memory Falsifying example: test_external_memory(
    self=<test_gpu_updaters.TestGPUUpdaters at 0x7fdf5051b790>,
    param={'colsample_bylevel': 0.5,
     'colsample_bytree': 0.5,
     'grow_policy': 'lossguide',
     'max_bin': 2,
     'max_depth': 0,
     'max_leaves': 1,
     'min_child_weight': 0.5,
     'seed': 0,
     'single_precision_histogram': False},
    num_rounds=1,
    dataset=boston,
)
FAILED
tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_empty_dmatrix_prediction FAILED
tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_specified_gpu_id_gpu_update Falsifying example: test_specified_gpu_id_gpu_update(
    self=<test_gpu_updaters.TestGPUUpdaters at 0x7fdf508ebe20>,
    dataset=boston,
    gpu_id=0,
)
FAILED
tests/python-gpu/test_gpu_with_dask.py::TestDistributedGPU::test_dask_dataframe SKIPPED
tests/python-gpu/test_gpu_with_dask.py::TestDistributedGPU::test_gpu_hist[local_cuda_cluster0] SKIPPED
tests/python-gpu/test_gpu_with_dask.py::TestDistributedGPU::test_dask_array SKIPPED
tests/python-gpu/test_gpu_with_dask.py::TestDistributedGPU::test_empty_dmatrix SKIPPED
tests/python-gpu/test_gpu_with_dask.py::TestDistributedGPU::test_quantile_basic SKIPPED
tests/python-gpu/test_gpu_with_dask.py::TestDistributedGPU::test_quantile_same_on_all_workers SKIPPED
tests/python-gpu/test_gpu_with_dask.py::test_with_asyncio SKIPPED
tests/python-gpu/test_gpu_with_sklearn.py::test_gpu_binary_classification FAILED
tests/python-gpu/test_gpu_with_sklearn.py::TestGPUBoostFromPrediction::test_boost_from_prediction_gpu_hist FAILED
tests/python-gpu/test_monotonic_constraints.py::TestMonotonicConstraints::test_gpu_hist_basic FAILED
tests/python-gpu/test_monotonic_constraints.py::TestMonotonicConstraints::test_gpu_hist_depthwise FAILED
tests/python-gpu/test_monotonic_constraints.py::TestMonotonicConstraints::test_gpu_hist_lossguide FAILED

=================================== FAILURES ===================================
________________ TestGPUBasicModels.test_deterministic_gpu_hist ________________

self = <unittest.case._Outcome object at 0x7fdf5091d460>
test_case = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_deterministic_gpu_hist>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_deterministic_gpu_hist>
result = <TestCaseFunction test_deterministic_gpu_hist>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_deterministic_gpu_hist>
method = <bound method TestGPUBasicModels.test_deterministic_gpu_hist of <test_gpu_basic_models.TestGPUBasicModels testMethod=test_deterministic_gpu_hist>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_deterministic_gpu_hist>

    def test_deterministic_gpu_hist(self):
        kRows = 1000
        kCols = 64
        kClasses = 4
        # Create large values to force rounding.
        X = np.random.randn(kRows, kCols) * 1e4
        y = np.random.randint(0, kClasses, size=kRows) * 1e4
    
>       model_0, model_1 = self.run_cls(X, y, True)

tests/python-gpu/test_gpu_basic_models.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_deterministic_gpu_hist>
X = array([[  5563.2490371 , -11423.62698646,  -5108.29435512, ...,
           164.60513501,  -3727.32321466,  11434.64713...  [-26595.09940795,  10319.96847025,  10115.87168718, ...,
          4285.61814395,  -5460.30393083,  -1042.99031088]])
y = array([20000., 10000., 20000., 20000., 20000., 20000., 20000., 20000.,
       10000.,     0.,     0., 10000., 10000., ...    0.,     0., 20000., 30000., 20000.,     0.,
       30000., 20000., 10000., 10000., 30000.,     0., 30000., 30000.])
deterministic = True

    def run_cls(self, X, y, deterministic):
        cls = xgb.XGBClassifier(tree_method='gpu_hist',
                                deterministic_histogram=deterministic,
                                single_precision_histogram=True)
>       cls.fit(X, y)

tests/python-gpu/test_gpu_basic_models.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_by...ram=True,
              subsample=None, tree_method='gpu_hist', validate_parameters=None,
              verbosity=None)
X = array([[  5563.2490371 , -11423.62698646,  -5108.29435512, ...,
           164.60513501,  -3727.32321466,  11434.64713...  [-26595.09940795,  10319.96847025,  10115.87168718, ...,
          4285.61814395,  -5460.30393083,  -1042.99031088]])
y = array([20000., 10000., 20000., 20000., 20000., 20000., 20000., 20000.,
       10000.,     0.,     0., 10000., 10000., ...    0.,     0., 20000., 30000., 20000.,     0.,
       30000., 20000., 10000., 10000., 30000.,     0., 30000., 30000.])
sample_weight = None, base_margin = None, eval_set = None, eval_metric = None
early_stopping_rounds = None, verbose = True, xgb_model = None
sample_weight_eval_set = None, feature_weights = None, callbacks = None

    def fit(self, X, y, sample_weight=None, base_margin=None,
            eval_set=None, eval_metric=None,
            early_stopping_rounds=None, verbose=True, xgb_model=None,
            sample_weight_eval_set=None, feature_weights=None, callbacks=None):
        # pylint: disable = attribute-defined-outside-init,arguments-differ
    
        evals_result = {}
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
    
        xgb_options = self.get_xgb_params()
    
        if callable(self.objective):
            obj = _objective_decorator(self.objective)
            # Use default value. Is it really not used ?
            xgb_options["objective"] = "binary:logistic"
        else:
            obj = None
    
        if self.n_classes_ > 2:
            # Switch to using a multiclass objective in the underlying
            # XGB instance
            xgb_options['objective'] = 'multi:softprob'
            xgb_options['num_class'] = self.n_classes_
    
        feval = eval_metric if callable(eval_metric) else None
        if eval_metric is not None:
            if callable(eval_metric):
                eval_metric = None
            else:
                xgb_options.update({"eval_metric": eval_metric})
    
        self._le = XGBoostLabelEncoder().fit(y)
        training_labels = self._le.transform(y)
    
        if eval_set is not None:
            if sample_weight_eval_set is None:
                sample_weight_eval_set = [None] * len(eval_set)
            else:
                assert len(sample_weight_eval_set) == len(eval_set)
            evals = list(
                DMatrix(eval_set[i][0],
                        label=self._le.transform(eval_set[i][1]),
                        missing=self.missing, weight=sample_weight_eval_set[i],
                        nthread=self.n_jobs)
                for i in range(len(eval_set))
            )
            nevals = len(evals)
            eval_names = ["validation_{}".format(i) for i in range(nevals)]
            evals = list(zip(evals, eval_names))
        else:
            evals = ()
    
        if len(X.shape) != 2:
            # Simply raise an error here since there might be many
            # different ways of reshaping
            raise ValueError(
                'Please reshape the input data X into 2-dimensional matrix.')
    
        self._features_count = X.shape[1]
        self.n_features_in_ = self._features_count
    
        train_dmatrix = DMatrix(X, label=training_labels, weight=sample_weight,
                                base_margin=base_margin,
                                missing=self.missing, nthread=self.n_jobs)
        train_dmatrix.set_info(feature_weights=feature_weights)
    
>       self._Booster = train(xgb_options, train_dmatrix,
                              self.get_num_boosting_rounds(),
                              evals=evals,
                              early_stopping_rounds=early_stopping_rounds,
                              evals_result=evals_result, obj=obj, feval=feval,
                              verbose_eval=verbose, xgb_model=xgb_model,
                              callbacks=callbacks)

venv/lib/python3.8/site-packages/xgboost/sklearn.py:836: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf5091d580>, num_boost_round = 100
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = {}, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf5091d580>, num_boost_round = 100
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf5091d7f0>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf5091d5e0>
dtrain = <xgboost.core.DMatrix object at 0x7fdf5091d580>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:16:31] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
__________________ TestGPUBasicModels.test_eta_decay_gpu_hist __________________

self = <unittest.case._Outcome object at 0x7fdf50718c40>
test_case = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_eta_decay_gpu_hist>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_eta_decay_gpu_hist>
result = <TestCaseFunction test_eta_decay_gpu_hist>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_eta_decay_gpu_hist>
method = <bound method TestGPUBasicModels.test_eta_decay_gpu_hist of <test_gpu_basic_models.TestGPUBasicModels testMethod=test_eta_decay_gpu_hist>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_basic_models.TestGPUBasicModels testMethod=test_eta_decay_gpu_hist>

    def test_eta_decay_gpu_hist(self):
>       self.cputest.run_eta_decay('gpu_hist', True)

tests/python-gpu/test_gpu_basic_models.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_callback.TestCallbacks testMethod=runTest>
tree_method = 'gpu_hist', deprecated_callback = True

    def run_eta_decay(self, tree_method, deprecated_callback):
        if deprecated_callback:
            scheduler = xgb.callback.reset_learning_rate
        else:
            scheduler = xgb.callback.LearningRateScheduler
    
        dpath = os.path.join(tm.PROJECT_ROOT, 'demo/data/')
        dtrain = xgb.DMatrix(dpath + 'agaricus.txt.train')
        dtest = xgb.DMatrix(dpath + 'agaricus.txt.test')
        watchlist = [(dtest, 'eval'), (dtrain, 'train')]
        num_round = 4
    
        # learning_rates as a list
        # init eta with 0 to check whether learning_rates work
        param = {'max_depth': 2, 'eta': 0, 'verbosity': 0,
                 'objective': 'binary:logistic', 'eval_metric': 'error',
                 'tree_method': tree_method}
        evals_result = {}
>       bst = xgb.train(param, dtrain, num_round, watchlist,
                        callbacks=[scheduler([
                            0.8, 0.7, 0.6, 0.5
                        ])],
                        evals_result=evals_result)

tests/python/test_callback.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eta': 0, 'eval_metric': 'error', 'max_depth': 2, 'objective': 'binary:logistic', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50718ca0>, num_boost_round = 4
evals = [(<xgboost.core.DMatrix object at 0x7fdf50718be0>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50718ca0>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = None
evals_result = {}, verbose_eval = True, xgb_model = None
callbacks = [<function reset_learning_rate.<locals>.callback at 0x7fdf507eb8b0>, <function print_evaluation.<locals>.callback at 0x7fdf508e6430>, <function record_evaluation.<locals>.callback at 0x7fdf50904700>]

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eta': 0, 'eval_metric': 'error', 'max_depth': 2, 'objective': 'binary:logistic', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50718ca0>, num_boost_round = 4
evals = [(<xgboost.core.DMatrix object at 0x7fdf50718be0>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50718ca0>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.LegacyCallbacks object at 0x7fdf50718fa0>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf50718dc0>
dtrain = <xgboost.core.DMatrix object at 0x7fdf50718ca0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:16:31] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
__________ TestGPUInteractionConstraints.test_interaction_constraints __________

self = <unittest.case._Outcome object at 0x7fdf505d7b20>
test_case = <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_interaction_constraints>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_interaction_constraints>
result = <TestCaseFunction test_interaction_constraints>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_interaction_constraints>
method = <bound method TestGPUInteractionConstraints.test_interaction_constraints of <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_interaction_constraints>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_interaction_constraints>

    def test_interaction_constraints(self):
>       self.cputest.run_interaction_constraints(tree_method='gpu_hist')

tests/python-gpu/test_gpu_interaction_constraints.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_interaction_constraints.TestInteractionConstraints testMethod=runTest>
tree_method = 'gpu_hist'

    def run_interaction_constraints(self, tree_method):
        x1 = np.random.normal(loc=1.0, scale=1.0, size=1000)
        x2 = np.random.normal(loc=1.0, scale=1.0, size=1000)
        x3 = np.random.choice([1, 2, 3], size=1000, replace=True)
        y = x1 + x2 + x3 + x1 * x2 * x3 \
            + np.random.normal(
                loc=0.001, scale=1.0, size=1000) + 3 * np.sin(x1)
        X = np.column_stack((x1, x2, x3))
        dtrain = xgboost.DMatrix(X, label=y)
    
        params = {
            'max_depth': 3,
            'eta': 0.1,
            'nthread': 2,
            'interaction_constraints': '[[0, 1]]',
            'tree_method': tree_method
        }
        num_boost_round = 12
        # Fit a model that only allows interaction between x1 and x2
>       bst = xgboost.train(
            params, dtrain, num_boost_round, evals=[(dtrain, 'train')])

tests/python/test_interaction_constraints.py:32: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eta': 0.1, 'interaction_constraints': '[[0, 1]]', 'max_depth': 3, 'nthread': 2, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf505d7ca0>, num_boost_round = 12
evals = [(<xgboost.core.DMatrix object at 0x7fdf505d7ca0>, 'train')], obj = None
feval = None, maximize = None, early_stopping_rounds = None, evals_result = None
verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eta': 0.1, 'interaction_constraints': '[[0, 1]]', 'max_depth': 3, 'nthread': 2, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf505d7ca0>, num_boost_round = 12
evals = [(<xgboost.core.DMatrix object at 0x7fdf505d7ca0>, 'train')], obj = None
feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf505d7d90>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf505d7d60>
dtrain = <xgboost.core.DMatrix object at 0x7fdf505d7ca0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:16:31] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
_____________ TestGPUInteractionConstraints.test_training_accuracy _____________

self = <unittest.case._Outcome object at 0x7fdf502fa5e0>
test_case = <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_training_accuracy>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_training_accuracy>
result = <TestCaseFunction test_training_accuracy>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_training_accuracy>
method = <bound method TestGPUInteractionConstraints.test_training_accuracy of <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_training_accuracy>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_interaction_constraints.TestGPUInteractionConstraints testMethod=test_training_accuracy>

    def test_training_accuracy(self):
>       self.cputest.training_accuracy(tree_method='gpu_hist')

tests/python-gpu/test_gpu_interaction_constraints.py:17: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_interaction_constraints.TestInteractionConstraints testMethod=runTest>
tree_method = 'gpu_hist'

    @pytest.mark.skipif(**tm.no_sklearn())
    def training_accuracy(self, tree_method):
        from sklearn.metrics import accuracy_score
        dtrain = xgboost.DMatrix(dpath + 'agaricus.txt.train?indexing_mode=1')
        dtest = xgboost.DMatrix(dpath + 'agaricus.txt.test?indexing_mode=1')
        params = {
            'eta': 1,
            'max_depth': 6,
            'objective': 'binary:logistic',
            'tree_method': tree_method,
            'interaction_constraints': '[[1,2], [2,3,4]]'
        }
        num_boost_round = 5
    
        params['grow_policy'] = 'lossguide'
>       bst = xgboost.train(params, dtrain, num_boost_round)

tests/python/test_interaction_constraints.py:76: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eta': 1, 'grow_policy': 'lossguide', 'interaction_constraints': '[[1,2], [2,3,4]]', 'max_depth': 6, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf502fa880>, num_boost_round = 5
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eta': 1, 'grow_policy': 'lossguide', 'interaction_constraints': '[[1,2], [2,3,4]]', 'max_depth': 6, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf502fa880>, num_boost_round = 5
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf502fa8e0>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf502fa8b0>
dtrain = <xgboost.core.DMatrix object at 0x7fdf502fa880>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:16:31] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
______________________ TestGPULinear.test_gpu_coordinate _______________________

cls = <class '_pytest.runner.CallInfo'>
func = <function call_runtest_hook.<locals>.<lambda> at 0x7fdf507729d0>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: "Optional[Union[Type[BaseException], Tuple[Type[BaseException], ...]]]" = None,
    ) -> "CallInfo[TResult]":
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result = func()  # type: Optional[TResult]

.venv/lib/python3.8/site-packages/_pytest/runner.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: ihook(item=item, **kwds), when=when, reraise=reraise
    )

.venv/lib/python3.8/site-packages/_pytest/runner.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_runtest_call'>, args = ()
kwargs = {'item': <Function test_gpu_coordinate>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fdf78b19310>
hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
kwargs = {'item': <Function test_gpu_coordinate>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
kwargs = {'item': <Function test_gpu_coordinate>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
caller_kwargs = {'item': <Function test_gpu_coordinate>}, firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fdf5086cbb0>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
caller_kwargs = {'item': <Function test_gpu_coordinate>}, firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_gpu_coordinate>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
            item.runtest()
        except Exception as e:
            # Store trace info to allow postmortem debugging
            sys.last_type = type(e)
            sys.last_value = e
            assert e.__traceback__ is not None
            # Skip *this* frame
            sys.last_traceback = e.__traceback__.tb_next
>           raise e

.venv/lib/python3.8/site-packages/_pytest/runner.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_gpu_coordinate>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
>           item.runtest()

.venv/lib/python3.8/site-packages/_pytest/runner.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Function test_gpu_coordinate>

    def runtest(self) -> None:
        """Execute the underlying test function."""
>       self.ihook.pytest_pyfunc_call(pyfuncitem=self)

.venv/lib/python3.8/site-packages/_pytest/python.py:1627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_pyfunc_call'>, args = ()
kwargs = {'pyfuncitem': <Function test_gpu_coordinate>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fdf78b19310>
hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_gpu_coordinate>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_gpu_coordinate>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_gpu_coordinate>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fdf50711550>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_gpu_coordinate>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pyfuncitem = <Function test_gpu_coordinate>

    @hookimpl(trylast=True)
    def pytest_pyfunc_call(pyfuncitem: "Function") -> Optional[object]:
        testfunction = pyfuncitem.obj
        if is_async_function(testfunction):
            async_warn_and_skip(pyfuncitem.nodeid)
        funcargs = pyfuncitem.funcargs
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
>       result = testfunction(**testargs)

.venv/lib/python3.8/site-packages/_pytest/python.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_linear.TestGPULinear object at 0x7fdf505c12e0>

    @given(parameter_strategy, strategies.integers(10, 50),
>          tm.dataset_strategy)
    @settings(deadline=None)

tests/python-gpu/test_gpu_linear.py:27: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arguments = ()
kwargs = {'self': <test_gpu_linear.TestGPULinear object at 0x7fdf505c12e0>}
__tracebackhide__ = True
test = <function TestGPULinear.test_gpu_coordinate at 0x7fdf524c7430>
settings = settings(database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples'), deadline=None, derand...nt_blob=False, report_multiple_bugs=True, stateful_step_count=50, suppress_health_check=(), verbosity=Verbosity.normal)
random = <random.Random object at 0x2e22ef0>
processed_args = ((), {'self': <test_gpu_linear.TestGPULinear object at 0x7fdf505c12e0>}, <function default_new_style_executor at 0x7fd...lue=10)})}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_linear.TestGPULinear object at 0x7fdf505c12e0>))
test_runner = <function default_new_style_executor at 0x7fdf77a913a0>
search_strategy = WithRunner(TupleStrategy((just(()), fixed_dictionaries({'dataset': _dataset_weight_margin(),
 'num_rounds': integers(m...alue=10)})}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_linear.TestGPULinear object at 0x7fdf505c12e0>)
runner = <test_gpu_linear.TestGPULinear object at 0x7fdf505c12e0>
state = <hypothesis.core.StateForActualGivenExecution object at 0x7fdf50711430>

    @impersonate(test)
    @define_function_signature(test.__name__, test.__doc__, argspec)
    def wrapped_test(*arguments, **kwargs):
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
    
        test = wrapped_test.hypothesis.inner_test
    
        if getattr(test, "is_hypothesis_test", False):
            raise InvalidArgument(
                (
                    "You have applied @given to the test %s more than once, which "
                    "wraps the test several times and is extremely slow. A "
                    "similar effect can be gained by combining the arguments "
                    "of the two calls to given. For example, instead of "
                    "@given(booleans()) @given(integers()), you could write "
                    "@given(booleans(), integers())"
                )
                % (test.__name__,)
            )
    
        settings = wrapped_test._hypothesis_internal_use_settings
    
        random = get_random_for_wrapped_test(test, wrapped_test)
    
        processed_args = process_arguments_to_given(
            wrapped_test, arguments, kwargs, given_kwargs, argspec
        )
        arguments, kwargs, test_runner, search_strategy = processed_args
    
        runner = getattr(search_strategy, "runner", None)
        if isinstance(runner, TestCase) and test.__name__ in dir(TestCase):
            msg = (
                "You have applied @given to the method %s, which is "
                "used by the unittest runner but is not itself a test."
                "  This is not useful in any way." % test.__name__
            )
            fail_health_check(settings, msg, HealthCheck.not_a_test_method)
        if bad_django_TestCase(runner):  # pragma: no cover
            # Covered by the Django tests, but not the pytest coverage task
            raise InvalidArgument(
                "You have applied @given to a method on %s, but this "
                "class does not inherit from the supported versions in "
                "`hypothesis.extra.django`.  Use the Hypothesis variants "
                "to ensure that each example is run in a separate "
                "database transaction." % qualname(type(runner))
            )
    
        state = StateForActualGivenExecution(
            test_runner, search_strategy, test, settings, random, wrapped_test
        )
    
        reproduce_failure = wrapped_test._hypothesis_internal_use_reproduce_failure
    
        # If there was a @reproduce_failure decorator, use it to reproduce
        # the error (or complain that we couldn't). Either way, this will
        # always raise some kind of error.
        if reproduce_failure is not None:
            expected_version, failure = reproduce_failure
            if expected_version != __version__:
                raise InvalidArgument(
                    (
                        "Attempting to reproduce a failure from a different "
                        "version of Hypothesis. This failure is from %s, but "
                        "you are currently running %r. Please change your "
                        "Hypothesis version to a matching one."
                    )
                    % (expected_version, __version__)
                )
            try:
                state.execute_once(
                    ConjectureData.for_buffer(decode_failure(failure)),
                    print_example=True,
                    is_final=True,
                )
                raise DidNotReproduce(
                    "Expected the test to raise an error, but it "
                    "completed successfully."
                )
            except StopTest:
                raise DidNotReproduce(
                    "The shape of the test data has changed in some way "
                    "from where this blob was defined. Are you sure "
                    "you're running the same test?"
                )
            except UnsatisfiedAssumption:
                raise DidNotReproduce(
                    "The test data failed to satisfy an assumption in the "
                    "test. Have you added it since this blob was "
                    "generated?"
                )
    
        # There was no @reproduce_failure, so start by running any explicit
        # examples from @example decorators.
        errors = list(
            execute_explicit_examples(state, wrapped_test, arguments, kwargs)
        )
        with local_settings(state.settings):
            if len(errors) > 1:
                # If we're not going to report multiple bugs, we would have
                # stopped running explicit examples at the first failure.
                assert state.settings.report_multiple_bugs
                for fragments, err in errors:
                    for f in fragments:
                        report(f)
                    tb_lines = traceback.format_exception(
                        type(err), err, err.__traceback__
                    )
                    report("".join(tb_lines))
                msg = "Hypothesis found %d failures in explicit examples."
                raise MultipleFailures(msg % (len(errors)))
            elif errors:
                fragments, the_error_hypothesis_found = errors[0]
                for f in fragments:
                    report(f)
                raise the_error_hypothesis_found
    
        # If there were any explicit examples, they all ran successfully.
        # The next step is to use the Conjecture engine to run the test on
        # many different inputs.
    
        if not (
            Phase.reuse in settings.phases or Phase.generate in settings.phases
        ):
            return
    
        try:
            if isinstance(runner, TestCase) and hasattr(runner, "subTest"):
                subTest = runner.subTest
                try:
                    runner.subTest = types.MethodType(fake_subTest, runner)
                    state.run_engine()
                finally:
                    runner.subTest = subTest
            else:
                state.run_engine()
        except BaseException as e:
            # The exception caught here should either be an actual test
            # failure (or MultipleFailures), or some kind of fatal error
            # that caused the engine to stop.
    
            generated_seed = wrapped_test._hypothesis_internal_use_generated_seed
            with local_settings(settings):
                if not (state.failed_normally or generated_seed is None):
                    if running_under_pytest:
                        report(
                            "You can add @seed(%(seed)d) to this test or "
                            "run pytest with --hypothesis-seed=%(seed)d "
                            "to reproduce this failure." % {"seed": generated_seed}
                        )
                    else:
                        report(
                            "You can add @seed(%d) to this test to "
                            "reproduce this failure." % (generated_seed,)
                        )
                # The dance here is to avoid showing users long tracebacks
                # full of Hypothesis internals they don't care about.
                # We have to do this inline, to avoid adding another
                # internal stack frame just when we've removed the rest.
                #
                # Using a variable for our trimmed error ensures that the line
                # which will actually appear in tracebacks is as clear as
                # possible - "raise the_error_hypothesis_found".
                the_error_hypothesis_found = e.with_traceback(
                    get_trimmed_traceback()
                )
>               raise the_error_hypothesis_found

venv/lib/python3.8/site-packages/hypothesis/core.py:1162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_linear.TestGPULinear object at 0x7fdf505c12e0>
param = {'booster': 'gblinear', 'eta': 0.01, 'eval_metric': 'rmse', 'feature_selector': 'cyclic', ...}
num_rounds = 10, dataset = boston

    @given(parameter_strategy, strategies.integers(10, 50),
           tm.dataset_strategy)
    @settings(deadline=None)
    def test_gpu_coordinate(self, param, num_rounds, dataset):
        assume(len(dataset.y) > 0)
        param['updater'] = 'gpu_coord_descent'
        param = dataset.set_params(param)
>       result = train_result(param, dataset.get_dmat(), num_rounds)['train'][dataset.metric]

tests/python-gpu/test_gpu_linear.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

param = {'booster': 'gblinear', 'eta': 0.01, 'eval_metric': 'rmse', 'feature_selector': 'cyclic', ...}
dmat = <xgboost.core.DMatrix object at 0x7fdf5086ca30>, num_rounds = 10

    def train_result(param, dmat, num_rounds):
        result = {}
>       xgb.train(param, dmat, num_rounds, [(dmat, 'train')], verbose_eval=False,
                  evals_result=result)

tests/python-gpu/test_gpu_linear.py:20: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gblinear', 'eta': 0.01, 'eval_metric': 'rmse', 'feature_selector': 'cyclic', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf5086ca30>, num_boost_round = 10
evals = [(<xgboost.core.DMatrix object at 0x7fdf5086ca30>, 'train')], obj = None
feval = None, maximize = None, early_stopping_rounds = None, evals_result = {}
verbose_eval = False, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gblinear', 'eta': 0.01, 'eval_metric': 'rmse', 'feature_selector': 'cyclic', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf5086ca30>, num_boost_round = 10
evals = [(<xgboost.core.DMatrix object at 0x7fdf5086ca30>, 'train')], obj = None
feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf5086c730>
evals_result = {}, maximize = None, verbose_eval = False
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf5086caf0>
dtrain = <xgboost.core.DMatrix object at 0x7fdf5086ca30>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:16:31] /home/otivedani/xgboost/src/linear/linear_updater.cc:17: Unknown linear updater gpu_coord_descent
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LinearUpdater::Create(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, xgboost::GenericParameter const*)+0x403) [0x7fdf560791b3]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBLinear::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x252) [0x7fdf5602d632]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x137c0) [0x7fdf779737c0]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
---------------------------------- Hypothesis ----------------------------------
Falsifying example: test_gpu_coordinate(
    self=<test_gpu_linear.TestGPULinear at 0x7fdf505c12e0>,
    param={'booster': 'gblinear',
     'eta': 0.01,
     'feature_selector': 'cyclic',
     'nthread': 1,
     'tolerance': 1e-05,
     'top_k': 1},
    num_rounds=10,
    dataset=boston,
)
________________ TestGPULinear.test_gpu_coordinate_regularised _________________

cls = <class '_pytest.runner.CallInfo'>
func = <function call_runtest_hook.<locals>.<lambda> at 0x7fdf50772a60>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: "Optional[Union[Type[BaseException], Tuple[Type[BaseException], ...]]]" = None,
    ) -> "CallInfo[TResult]":
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result = func()  # type: Optional[TResult]

.venv/lib/python3.8/site-packages/_pytest/runner.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: ihook(item=item, **kwds), when=when, reraise=reraise
    )

.venv/lib/python3.8/site-packages/_pytest/runner.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_runtest_call'>, args = ()
kwargs = {'item': <Function test_gpu_coordinate_regularised>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fdf78b19310>
hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
kwargs = {'item': <Function test_gpu_coordinate_regularised>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
kwargs = {'item': <Function test_gpu_coordinate_regularised>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
caller_kwargs = {'item': <Function test_gpu_coordinate_regularised>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fdf50478160>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
caller_kwargs = {'item': <Function test_gpu_coordinate_regularised>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_gpu_coordinate_regularised>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
            item.runtest()
        except Exception as e:
            # Store trace info to allow postmortem debugging
            sys.last_type = type(e)
            sys.last_value = e
            assert e.__traceback__ is not None
            # Skip *this* frame
            sys.last_traceback = e.__traceback__.tb_next
>           raise e

.venv/lib/python3.8/site-packages/_pytest/runner.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_gpu_coordinate_regularised>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
>           item.runtest()

.venv/lib/python3.8/site-packages/_pytest/runner.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Function test_gpu_coordinate_regularised>

    def runtest(self) -> None:
        """Execute the underlying test function."""
>       self.ihook.pytest_pyfunc_call(pyfuncitem=self)

.venv/lib/python3.8/site-packages/_pytest/python.py:1627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_pyfunc_call'>, args = ()
kwargs = {'pyfuncitem': <Function test_gpu_coordinate_regularised>}
notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fdf78b19310>
hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_gpu_coordinate_regularised>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_gpu_coordinate_regularised>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_gpu_coordinate_regularised>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fdf504784f0>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_gpu_coordinate_regularised>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pyfuncitem = <Function test_gpu_coordinate_regularised>

    @hookimpl(trylast=True)
    def pytest_pyfunc_call(pyfuncitem: "Function") -> Optional[object]:
        testfunction = pyfuncitem.obj
        if is_async_function(testfunction):
            async_warn_and_skip(pyfuncitem.nodeid)
        funcargs = pyfuncitem.funcargs
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
>       result = testfunction(**testargs)

.venv/lib/python3.8/site-packages/_pytest/python.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_linear.TestGPULinear object at 0x7fdf50478a30>

>   ???

tests/python-gpu/test_gpu_linear.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arguments = ()
kwargs = {'self': <test_gpu_linear.TestGPULinear object at 0x7fdf50478a30>}
__tracebackhide__ = True
test = <function TestGPULinear.test_gpu_coordinate_regularised at 0x7fdf524c7160>
settings = settings(database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples'), deadline=None, derand...nt_blob=False, report_multiple_bugs=True, stateful_step_count=50, suppress_health_check=(), verbosity=Verbosity.normal)
random = <random.Random object at 0x2d526c0>
processed_args = ((), {'self': <test_gpu_linear.TestGPULinear object at 0x7fdf50478a30>}, <function default_new_style_executor at 0x7fd...lue=10)})}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_linear.TestGPULinear object at 0x7fdf50478a30>))
test_runner = <function default_new_style_executor at 0x7fdf77a913a0>
search_strategy = WithRunner(TupleStrategy((just(()), fixed_dictionaries({'alpha': floats(min_value=1e-05, max_value=2.0),
 'dataset': _...alue=10)})}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_linear.TestGPULinear object at 0x7fdf50478a30>)
runner = <test_gpu_linear.TestGPULinear object at 0x7fdf50478a30>
state = <hypothesis.core.StateForActualGivenExecution object at 0x7fdf50416d00>

    @impersonate(test)
    @define_function_signature(test.__name__, test.__doc__, argspec)
    def wrapped_test(*arguments, **kwargs):
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
    
        test = wrapped_test.hypothesis.inner_test
    
        if getattr(test, "is_hypothesis_test", False):
            raise InvalidArgument(
                (
                    "You have applied @given to the test %s more than once, which "
                    "wraps the test several times and is extremely slow. A "
                    "similar effect can be gained by combining the arguments "
                    "of the two calls to given. For example, instead of "
                    "@given(booleans()) @given(integers()), you could write "
                    "@given(booleans(), integers())"
                )
                % (test.__name__,)
            )
    
        settings = wrapped_test._hypothesis_internal_use_settings
    
        random = get_random_for_wrapped_test(test, wrapped_test)
    
        processed_args = process_arguments_to_given(
            wrapped_test, arguments, kwargs, given_kwargs, argspec
        )
        arguments, kwargs, test_runner, search_strategy = processed_args
    
        runner = getattr(search_strategy, "runner", None)
        if isinstance(runner, TestCase) and test.__name__ in dir(TestCase):
            msg = (
                "You have applied @given to the method %s, which is "
                "used by the unittest runner but is not itself a test."
                "  This is not useful in any way." % test.__name__
            )
            fail_health_check(settings, msg, HealthCheck.not_a_test_method)
        if bad_django_TestCase(runner):  # pragma: no cover
            # Covered by the Django tests, but not the pytest coverage task
            raise InvalidArgument(
                "You have applied @given to a method on %s, but this "
                "class does not inherit from the supported versions in "
                "`hypothesis.extra.django`.  Use the Hypothesis variants "
                "to ensure that each example is run in a separate "
                "database transaction." % qualname(type(runner))
            )
    
        state = StateForActualGivenExecution(
            test_runner, search_strategy, test, settings, random, wrapped_test
        )
    
        reproduce_failure = wrapped_test._hypothesis_internal_use_reproduce_failure
    
        # If there was a @reproduce_failure decorator, use it to reproduce
        # the error (or complain that we couldn't). Either way, this will
        # always raise some kind of error.
        if reproduce_failure is not None:
            expected_version, failure = reproduce_failure
            if expected_version != __version__:
                raise InvalidArgument(
                    (
                        "Attempting to reproduce a failure from a different "
                        "version of Hypothesis. This failure is from %s, but "
                        "you are currently running %r. Please change your "
                        "Hypothesis version to a matching one."
                    )
                    % (expected_version, __version__)
                )
            try:
                state.execute_once(
                    ConjectureData.for_buffer(decode_failure(failure)),
                    print_example=True,
                    is_final=True,
                )
                raise DidNotReproduce(
                    "Expected the test to raise an error, but it "
                    "completed successfully."
                )
            except StopTest:
                raise DidNotReproduce(
                    "The shape of the test data has changed in some way "
                    "from where this blob was defined. Are you sure "
                    "you're running the same test?"
                )
            except UnsatisfiedAssumption:
                raise DidNotReproduce(
                    "The test data failed to satisfy an assumption in the "
                    "test. Have you added it since this blob was "
                    "generated?"
                )
    
        # There was no @reproduce_failure, so start by running any explicit
        # examples from @example decorators.
        errors = list(
            execute_explicit_examples(state, wrapped_test, arguments, kwargs)
        )
        with local_settings(state.settings):
            if len(errors) > 1:
                # If we're not going to report multiple bugs, we would have
                # stopped running explicit examples at the first failure.
                assert state.settings.report_multiple_bugs
                for fragments, err in errors:
                    for f in fragments:
                        report(f)
                    tb_lines = traceback.format_exception(
                        type(err), err, err.__traceback__
                    )
                    report("".join(tb_lines))
                msg = "Hypothesis found %d failures in explicit examples."
                raise MultipleFailures(msg % (len(errors)))
            elif errors:
                fragments, the_error_hypothesis_found = errors[0]
                for f in fragments:
                    report(f)
                raise the_error_hypothesis_found
    
        # If there were any explicit examples, they all ran successfully.
        # The next step is to use the Conjecture engine to run the test on
        # many different inputs.
    
        if not (
            Phase.reuse in settings.phases or Phase.generate in settings.phases
        ):
            return
    
        try:
            if isinstance(runner, TestCase) and hasattr(runner, "subTest"):
                subTest = runner.subTest
                try:
                    runner.subTest = types.MethodType(fake_subTest, runner)
                    state.run_engine()
                finally:
                    runner.subTest = subTest
            else:
                state.run_engine()
        except BaseException as e:
            # The exception caught here should either be an actual test
            # failure (or MultipleFailures), or some kind of fatal error
            # that caused the engine to stop.
    
            generated_seed = wrapped_test._hypothesis_internal_use_generated_seed
            with local_settings(settings):
                if not (state.failed_normally or generated_seed is None):
                    if running_under_pytest:
                        report(
                            "You can add @seed(%(seed)d) to this test or "
                            "run pytest with --hypothesis-seed=%(seed)d "
                            "to reproduce this failure." % {"seed": generated_seed}
                        )
                    else:
                        report(
                            "You can add @seed(%d) to this test to "
                            "reproduce this failure." % (generated_seed,)
                        )
                # The dance here is to avoid showing users long tracebacks
                # full of Hypothesis internals they don't care about.
                # We have to do this inline, to avoid adding another
                # internal stack frame just when we've removed the rest.
                #
                # Using a variable for our trimmed error ensures that the line
                # which will actually appear in tracebacks is as clear as
                # possible - "raise the_error_hypothesis_found".
                the_error_hypothesis_found = e.with_traceback(
                    get_trimmed_traceback()
                )
>               raise the_error_hypothesis_found

venv/lib/python3.8/site-packages/hypothesis/core.py:1162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_linear.TestGPULinear object at 0x7fdf50478a30>
param = {'alpha': 1e-05, 'booster': 'gblinear', 'eta': 0.01, 'eval_metric': 'rmse', ...}
num_rounds = 10, dataset = boston, alpha = 1e-05, lambd = 1e-05

    @given(parameter_strategy, strategies.integers(10, 50),
           tm.dataset_strategy, strategies.floats(1e-5, 2.0),
           strategies.floats(1e-5, 2.0))
    @settings(deadline=None)
    def test_gpu_coordinate_regularised(self, param, num_rounds, dataset, alpha, lambd):
        assume(len(dataset.y) > 0)
        param['updater'] = 'gpu_coord_descent'
        param['alpha'] = alpha
        param['lambda'] = lambd
        param = dataset.set_params(param)
>       result = train_result(param, dataset.get_dmat(), num_rounds)['train'][dataset.metric]

tests/python-gpu/test_gpu_linear.py:49: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

param = {'alpha': 1e-05, 'booster': 'gblinear', 'eta': 0.01, 'eval_metric': 'rmse', ...}
dmat = <xgboost.core.DMatrix object at 0x7fdf507771c0>, num_rounds = 10

    def train_result(param, dmat, num_rounds):
        result = {}
>       xgb.train(param, dmat, num_rounds, [(dmat, 'train')], verbose_eval=False,
                  evals_result=result)

tests/python-gpu/test_gpu_linear.py:20: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'alpha': 1e-05, 'booster': 'gblinear', 'eta': 0.01, 'eval_metric': 'rmse', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf507771c0>, num_boost_round = 10
evals = [(<xgboost.core.DMatrix object at 0x7fdf507771c0>, 'train')], obj = None
feval = None, maximize = None, early_stopping_rounds = None, evals_result = {}
verbose_eval = False, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'alpha': 1e-05, 'booster': 'gblinear', 'eta': 0.01, 'eval_metric': 'rmse', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf507771c0>, num_boost_round = 10
evals = [(<xgboost.core.DMatrix object at 0x7fdf507771c0>, 'train')], obj = None
feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf50777d00>
evals_result = {}, maximize = None, verbose_eval = False
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf50777130>
dtrain = <xgboost.core.DMatrix object at 0x7fdf507771c0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:16:31] /home/otivedani/xgboost/src/linear/linear_updater.cc:17: Unknown linear updater gpu_coord_descent
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LinearUpdater::Create(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, xgboost::GenericParameter const*)+0x403) [0x7fdf560791b3]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBLinear::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x252) [0x7fdf5602d632]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x137c0) [0x7fdf779737c0]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
---------------------------------- Hypothesis ----------------------------------
Falsifying example: test_gpu_coordinate_regularised(
    self=<test_gpu_linear.TestGPULinear at 0x7fdf50478a30>,
    param={'booster': 'gblinear',
     'eta': 0.01,
     'feature_selector': 'cyclic',
     'nthread': 1,
     'tolerance': 1e-05,
     'top_k': 1},
    num_rounds=10,
    dataset=boston,
    alpha=1e-05,
    lambd=1e-05,
)
_____________________ TestPickling.test_pickled_predictor ______________________

self = <unittest.case._Outcome object at 0x7fdf5086c4c0>
test_case = <test_gpu_pickling.TestPickling testMethod=test_pickled_predictor>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_pickled_predictor>
result = <TestCaseFunction test_pickled_predictor>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_pickled_predictor>
method = <bound method TestPickling.test_pickled_predictor of <test_gpu_pickling.TestPickling testMethod=test_pickled_predictor>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_pickled_predictor>

    def test_pickled_predictor(self):
        x, y = build_dataset()
        train_x = xgb.DMatrix(x, label=y)
    
        param = {'tree_method': 'gpu_hist',
                 'verbosity': 1, 'predictor': 'gpu_predictor'}
>       bst = xgb.train(param, train_x)

tests/python-gpu/test_gpu_pickling.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'predictor': 'gpu_predictor', 'tree_method': 'gpu_hist', 'verbosity': 1}
dtrain = <xgboost.core.DMatrix object at 0x7fdf5086cb50>, num_boost_round = 10
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'predictor': 'gpu_predictor', 'tree_method': 'gpu_hist', 'validate_parameters': True, 'verbosity': 1}
dtrain = <xgboost.core.DMatrix object at 0x7fdf5086cb50>, num_boost_round = 10
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf5086c430>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf5086c460>
dtrain = <xgboost.core.DMatrix object at 0x7fdf5086cb50>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:16:31] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
__________________________ TestPickling.test_pickling __________________________

self = <unittest.case._Outcome object at 0x7fdf50372310>
test_case = <test_gpu_pickling.TestPickling testMethod=test_pickling>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_pickling>
result = <TestCaseFunction test_pickling>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_pickling>
method = <bound method TestPickling.test_pickling of <test_gpu_pickling.TestPickling testMethod=test_pickling>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_pickling>

    def test_pickling(self):
        x, y = build_dataset()
        train_x = xgb.DMatrix(x, label=y)
        param = {'tree_method': 'gpu_hist',
                 'verbosity': 1}
>       bst = xgb.train(param, train_x)

tests/python-gpu/test_gpu_pickling.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'tree_method': 'gpu_hist', 'verbosity': 1}
dtrain = <xgboost.core.DMatrix object at 0x7fdf503723a0>, num_boost_round = 10
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'tree_method': 'gpu_hist', 'validate_parameters': True, 'verbosity': 1}
dtrain = <xgboost.core.DMatrix object at 0x7fdf503723a0>, num_boost_round = 10
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf50372340>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf50372400>
dtrain = <xgboost.core.DMatrix object at 0x7fdf503723a0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:16:31] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
___________________ TestPickling.test_predict_sklearn_pickle ___________________

self = <unittest.case._Outcome object at 0x7fdf50383790>
test_case = <test_gpu_pickling.TestPickling testMethod=test_predict_sklearn_pickle>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_predict_sklearn_pickle>
result = <TestCaseFunction test_predict_sklearn_pickle>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_predict_sklearn_pickle>
method = <bound method TestPickling.test_predict_sklearn_pickle of <test_gpu_pickling.TestPickling testMethod=test_predict_sklearn_pickle>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_predict_sklearn_pickle>

    def test_predict_sklearn_pickle(self):
        x, y = build_dataset()
    
        kwargs = {'tree_method': 'gpu_hist',
                  'predictor': 'gpu_predictor',
                  'objective': 'binary:logistic',
                  'n_estimators': 10}
    
        model = XGBClassifier(**kwargs)
>       model.fit(x, y)

tests/python-gpu/test_gpu_pickling.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_by... scale_pos_weight=None, subsample=None,
              tree_method='gpu_hist', validate_parameters=None, verbosity=None)
X = array([[  0.        ,   1.01010101,   2.02020202,   3.03030303,
          4.04040404,   5.05050505,   6.06060606,   7....93,  93.93939394,
         94.94949495,  95.95959596,  96.96969697,  97.97979798,
         98.98989899, 100.        ]])
y = array([ 0.        ,  1.11111111,  2.22222222,  3.33333333,  4.44444444,
        5.55555556,  6.66666667,  7.77777778,  8.88888889, 10.        ])
sample_weight = None, base_margin = None, eval_set = None, eval_metric = None
early_stopping_rounds = None, verbose = True, xgb_model = None
sample_weight_eval_set = None, feature_weights = None, callbacks = None

    def fit(self, X, y, sample_weight=None, base_margin=None,
            eval_set=None, eval_metric=None,
            early_stopping_rounds=None, verbose=True, xgb_model=None,
            sample_weight_eval_set=None, feature_weights=None, callbacks=None):
        # pylint: disable = attribute-defined-outside-init,arguments-differ
    
        evals_result = {}
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
    
        xgb_options = self.get_xgb_params()
    
        if callable(self.objective):
            obj = _objective_decorator(self.objective)
            # Use default value. Is it really not used ?
            xgb_options["objective"] = "binary:logistic"
        else:
            obj = None
    
        if self.n_classes_ > 2:
            # Switch to using a multiclass objective in the underlying
            # XGB instance
            xgb_options['objective'] = 'multi:softprob'
            xgb_options['num_class'] = self.n_classes_
    
        feval = eval_metric if callable(eval_metric) else None
        if eval_metric is not None:
            if callable(eval_metric):
                eval_metric = None
            else:
                xgb_options.update({"eval_metric": eval_metric})
    
        self._le = XGBoostLabelEncoder().fit(y)
        training_labels = self._le.transform(y)
    
        if eval_set is not None:
            if sample_weight_eval_set is None:
                sample_weight_eval_set = [None] * len(eval_set)
            else:
                assert len(sample_weight_eval_set) == len(eval_set)
            evals = list(
                DMatrix(eval_set[i][0],
                        label=self._le.transform(eval_set[i][1]),
                        missing=self.missing, weight=sample_weight_eval_set[i],
                        nthread=self.n_jobs)
                for i in range(len(eval_set))
            )
            nevals = len(evals)
            eval_names = ["validation_{}".format(i) for i in range(nevals)]
            evals = list(zip(evals, eval_names))
        else:
            evals = ()
    
        if len(X.shape) != 2:
            # Simply raise an error here since there might be many
            # different ways of reshaping
            raise ValueError(
                'Please reshape the input data X into 2-dimensional matrix.')
    
        self._features_count = X.shape[1]
        self.n_features_in_ = self._features_count
    
        train_dmatrix = DMatrix(X, label=training_labels, weight=sample_weight,
                                base_margin=base_margin,
                                missing=self.missing, nthread=self.n_jobs)
        train_dmatrix.set_info(feature_weights=feature_weights)
    
>       self._Booster = train(xgb_options, train_dmatrix,
                              self.get_num_boosting_rounds(),
                              evals=evals,
                              early_stopping_rounds=early_stopping_rounds,
                              evals_result=evals_result, obj=obj, feval=feval,
                              verbose_eval=verbose, xgb_model=xgb_model,
                              callbacks=callbacks)

venv/lib/python3.8/site-packages/xgboost/sklearn.py:836: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf503838e0>, num_boost_round = 10
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = {}, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf503838e0>, num_boost_round = 10
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf50383b20>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf50383a60>
dtrain = <xgboost.core.DMatrix object at 0x7fdf503838e0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:16:31] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
__________________ TestPickling.test_training_on_cpu_only_env __________________

self = <unittest.case._Outcome object at 0x7fdf5086cb50>
test_case = <test_gpu_pickling.TestPickling testMethod=test_training_on_cpu_only_env>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_training_on_cpu_only_env>
result = <TestCaseFunction test_training_on_cpu_only_env>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_training_on_cpu_only_env>
method = <bound method TestPickling.test_training_on_cpu_only_env of <test_gpu_pickling.TestPickling testMethod=test_training_on_cpu_only_env>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_training_on_cpu_only_env>

    def test_training_on_cpu_only_env(self):
        cuda_environment = {'CUDA_VISIBLE_DEVICES': '-1'}
        env = os.environ.copy()
        env.update(cuda_environment)
        args = self.args_template.copy()
        args.append(
            "./tests/python-gpu/"
            "load_pickle.py::TestLoadPickle::test_training_on_cpu_only_env")
        status = subprocess.call(args, env=env)
>       assert status == 0
E       AssertionError: assert 1 == 0
E         +1
E         -0

tests/python-gpu/test_gpu_pickling.py:173: AssertionError
________________________ TestPickling.test_wrap_gpu_id _________________________

self = <unittest.case._Outcome object at 0x7fdf50628820>
test_case = <test_gpu_pickling.TestPickling testMethod=test_wrap_gpu_id>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_wrap_gpu_id>
result = <TestCaseFunction test_wrap_gpu_id>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_wrap_gpu_id>
method = <bound method TestPickling.test_wrap_gpu_id of <test_gpu_pickling.TestPickling testMethod=test_wrap_gpu_id>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_pickling.TestPickling testMethod=test_wrap_gpu_id>

    @pytest.mark.mgpu
    def test_wrap_gpu_id(self):
        X, y = build_dataset()
        dtrain = xgb.DMatrix(X, y)
    
>       bst = xgb.train({'tree_method': 'gpu_hist',
                         'gpu_id': 1},
                        dtrain, num_boost_round=6)

tests/python-gpu/test_gpu_pickling.py:81: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'gpu_id': 1, 'tree_method': 'gpu_hist'}
dtrain = <xgboost.core.DMatrix object at 0x7fdf506288e0>, num_boost_round = 6
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'gpu_id': 1, 'tree_method': 'gpu_hist', 'validate_parameters': True}
dtrain = <xgboost.core.DMatrix object at 0x7fdf506288e0>, num_boost_round = 6
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf50628910>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf50628160>
dtrain = <xgboost.core.DMatrix object at 0x7fdf506288e0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:16:32] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
______________________ TestGPUPredict.test_multi_predict _______________________

self = <unittest.case._Outcome object at 0x7fdf5068ab80>
test_case = <test_gpu_prediction.TestGPUPredict testMethod=test_multi_predict>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_multi_predict>
result = <TestCaseFunction test_multi_predict>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_multi_predict>
method = <bound method TestGPUPredict.test_multi_predict of <test_gpu_prediction.TestGPUPredict testMethod=test_multi_predict>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_multi_predict>

    @pytest.mark.skipif(**tm.no_sklearn())
    def test_multi_predict(self):
        from sklearn.datasets import make_regression
        from sklearn.model_selection import train_test_split
    
        n = 1000
        X, y = make_regression(n, random_state=rng)
        X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                            random_state=123)
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dtest = xgb.DMatrix(X_test)
    
        params = {}
        params["tree_method"] = "gpu_hist"
    
        params['predictor'] = "gpu_predictor"
>       bst_gpu_predict = xgb.train(params, dtrain)

tests/python-gpu/test_gpu_prediction.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'predictor': 'gpu_predictor', 'tree_method': 'gpu_hist'}
dtrain = <xgboost.core.DMatrix object at 0x7fdf5068ac70>, num_boost_round = 10
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'predictor': 'gpu_predictor', 'tree_method': 'gpu_hist', 'validate_parameters': True}
dtrain = <xgboost.core.DMatrix object at 0x7fdf5068ac70>, num_boost_round = 10
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf5068abe0>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf5068ad60>
dtrain = <xgboost.core.DMatrix object at 0x7fdf5068ac70>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:16:33] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
_________________________ TestGPUPredict.test_predict __________________________

self = <unittest.case._Outcome object at 0x7fdf50460940>
test_case = <test_gpu_prediction.TestGPUPredict testMethod=test_predict>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_predict>
result = <TestCaseFunction test_predict>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_predict>
method = <bound method TestGPUPredict.test_predict of <test_gpu_prediction.TestGPUPredict testMethod=test_predict>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_predict>

    def test_predict(self):
        iterations = 10
        np.random.seed(1)
        test_num_rows = [10, 1000, 5000]
        test_num_cols = [10, 50, 500]
        # This test passes for tree_method=gpu_hist and tree_method=exact. but
        # for `hist` and `approx` the floating point error accumulates faster
        # and fails even tol is set to 1e-4.  For `hist`, the mismatching rate
        # with 5000 rows is 0.04.
        for num_rows in test_num_rows:
            for num_cols in test_num_cols:
                dtrain = xgb.DMatrix(np.random.randn(num_rows, num_cols),
                                     label=[0, 1] * int(num_rows / 2))
                dval = xgb.DMatrix(np.random.randn(num_rows, num_cols),
                                   label=[0, 1] * int(num_rows / 2))
                dtest = xgb.DMatrix(np.random.randn(num_rows, num_cols),
                                    label=[0, 1] * int(num_rows / 2))
                watchlist = [(dtrain, 'train'), (dval, 'validation')]
                res = {}
                param = {
                    "objective": "binary:logistic",
                    "predictor": "gpu_predictor",
                    'eval_metric': 'logloss',
                    'tree_method': 'gpu_hist',
                    'max_depth': 1
                }
>               bst = xgb.train(param, dtrain, iterations, evals=watchlist,
                                evals_result=res)

tests/python-gpu/test_gpu_prediction.py:49: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eval_metric': 'logloss', 'max_depth': 1, 'objective': 'binary:logistic', 'predictor': 'gpu_predictor', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50460a90>, num_boost_round = 10
evals = [(<xgboost.core.DMatrix object at 0x7fdf50460a90>, 'train'), (<xgboost.core.DMatrix object at 0x7fdf50460a60>, 'validation')]
obj = None, feval = None, maximize = None, early_stopping_rounds = None
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eval_metric': 'logloss', 'max_depth': 1, 'objective': 'binary:logistic', 'predictor': 'gpu_predictor', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50460a90>, num_boost_round = 10
evals = [(<xgboost.core.DMatrix object at 0x7fdf50460a90>, 'train'), (<xgboost.core.DMatrix object at 0x7fdf50460a60>, 'validation')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf50460b80>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf50460a00>
dtrain = <xgboost.core.DMatrix object at 0x7fdf50460a90>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:16:33] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
___________________________ TestGPUPredict.test_shap ___________________________

self = <unittest.case._Outcome object at 0x7fdf66440760>
test_case = <test_gpu_prediction.TestGPUPredict testMethod=test_shap>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_shap>
result = <TestCaseFunction test_shap>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_shap>
method = <bound method accept.<locals>.test_shap of <test_gpu_prediction.TestGPUPredict testMethod=test_shap>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_shap>

>   ???

tests/python-gpu/test_gpu_prediction.py:198: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arguments = ()
kwargs = {'self': <test_gpu_prediction.TestGPUPredict testMethod=test_shap>}
__tracebackhide__ = True
test = <function TestGPUPredict.test_shap at 0x7fdf522115e0>
settings = settings(database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples'), deadline=None, derand...nt_blob=False, report_multiple_bugs=True, stateful_step_count=50, suppress_health_check=(), verbosity=Verbosity.normal)
random = <random.Random object at 0x2d35c70>
processed_args = ((), {'self': <test_gpu_prediction.TestGPUPredict testMethod=test_shap>}, <function default_new_style_executor at 0x7f...nknown>)}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_prediction.TestGPUPredict testMethod=test_shap>))
test_runner = <function default_new_style_executor at 0x7fdf77a913a0>
search_strategy = WithRunner(TupleStrategy((just(()), fixed_dictionaries({'dataset': _dataset_weight_margin(),
 'num_rounds': integers(m...unknown>)}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_prediction.TestGPUPredict testMethod=test_shap>)
runner = <test_gpu_prediction.TestGPUPredict testMethod=test_shap>
state = <hypothesis.core.StateForActualGivenExecution object at 0x7fdf5066e700>

    @impersonate(test)
    @define_function_signature(test.__name__, test.__doc__, argspec)
    def wrapped_test(*arguments, **kwargs):
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
    
        test = wrapped_test.hypothesis.inner_test
    
        if getattr(test, "is_hypothesis_test", False):
            raise InvalidArgument(
                (
                    "You have applied @given to the test %s more than once, which "
                    "wraps the test several times and is extremely slow. A "
                    "similar effect can be gained by combining the arguments "
                    "of the two calls to given. For example, instead of "
                    "@given(booleans()) @given(integers()), you could write "
                    "@given(booleans(), integers())"
                )
                % (test.__name__,)
            )
    
        settings = wrapped_test._hypothesis_internal_use_settings
    
        random = get_random_for_wrapped_test(test, wrapped_test)
    
        processed_args = process_arguments_to_given(
            wrapped_test, arguments, kwargs, given_kwargs, argspec
        )
        arguments, kwargs, test_runner, search_strategy = processed_args
    
        runner = getattr(search_strategy, "runner", None)
        if isinstance(runner, TestCase) and test.__name__ in dir(TestCase):
            msg = (
                "You have applied @given to the method %s, which is "
                "used by the unittest runner but is not itself a test."
                "  This is not useful in any way." % test.__name__
            )
            fail_health_check(settings, msg, HealthCheck.not_a_test_method)
        if bad_django_TestCase(runner):  # pragma: no cover
            # Covered by the Django tests, but not the pytest coverage task
            raise InvalidArgument(
                "You have applied @given to a method on %s, but this "
                "class does not inherit from the supported versions in "
                "`hypothesis.extra.django`.  Use the Hypothesis variants "
                "to ensure that each example is run in a separate "
                "database transaction." % qualname(type(runner))
            )
    
        state = StateForActualGivenExecution(
            test_runner, search_strategy, test, settings, random, wrapped_test
        )
    
        reproduce_failure = wrapped_test._hypothesis_internal_use_reproduce_failure
    
        # If there was a @reproduce_failure decorator, use it to reproduce
        # the error (or complain that we couldn't). Either way, this will
        # always raise some kind of error.
        if reproduce_failure is not None:
            expected_version, failure = reproduce_failure
            if expected_version != __version__:
                raise InvalidArgument(
                    (
                        "Attempting to reproduce a failure from a different "
                        "version of Hypothesis. This failure is from %s, but "
                        "you are currently running %r. Please change your "
                        "Hypothesis version to a matching one."
                    )
                    % (expected_version, __version__)
                )
            try:
                state.execute_once(
                    ConjectureData.for_buffer(decode_failure(failure)),
                    print_example=True,
                    is_final=True,
                )
                raise DidNotReproduce(
                    "Expected the test to raise an error, but it "
                    "completed successfully."
                )
            except StopTest:
                raise DidNotReproduce(
                    "The shape of the test data has changed in some way "
                    "from where this blob was defined. Are you sure "
                    "you're running the same test?"
                )
            except UnsatisfiedAssumption:
                raise DidNotReproduce(
                    "The test data failed to satisfy an assumption in the "
                    "test. Have you added it since this blob was "
                    "generated?"
                )
    
        # There was no @reproduce_failure, so start by running any explicit
        # examples from @example decorators.
        errors = list(
            execute_explicit_examples(state, wrapped_test, arguments, kwargs)
        )
        with local_settings(state.settings):
            if len(errors) > 1:
                # If we're not going to report multiple bugs, we would have
                # stopped running explicit examples at the first failure.
                assert state.settings.report_multiple_bugs
                for fragments, err in errors:
                    for f in fragments:
                        report(f)
                    tb_lines = traceback.format_exception(
                        type(err), err, err.__traceback__
                    )
                    report("".join(tb_lines))
                msg = "Hypothesis found %d failures in explicit examples."
                raise MultipleFailures(msg % (len(errors)))
            elif errors:
                fragments, the_error_hypothesis_found = errors[0]
                for f in fragments:
                    report(f)
                raise the_error_hypothesis_found
    
        # If there were any explicit examples, they all ran successfully.
        # The next step is to use the Conjecture engine to run the test on
        # many different inputs.
    
        if not (
            Phase.reuse in settings.phases or Phase.generate in settings.phases
        ):
            return
    
        try:
            if isinstance(runner, TestCase) and hasattr(runner, "subTest"):
                subTest = runner.subTest
                try:
                    runner.subTest = types.MethodType(fake_subTest, runner)
                    state.run_engine()
                finally:
                    runner.subTest = subTest
            else:
                state.run_engine()
        except BaseException as e:
            # The exception caught here should either be an actual test
            # failure (or MultipleFailures), or some kind of fatal error
            # that caused the engine to stop.
    
            generated_seed = wrapped_test._hypothesis_internal_use_generated_seed
            with local_settings(settings):
                if not (state.failed_normally or generated_seed is None):
                    if running_under_pytest:
                        report(
                            "You can add @seed(%(seed)d) to this test or "
                            "run pytest with --hypothesis-seed=%(seed)d "
                            "to reproduce this failure." % {"seed": generated_seed}
                        )
                    else:
                        report(
                            "You can add @seed(%d) to this test to "
                            "reproduce this failure." % (generated_seed,)
                        )
                # The dance here is to avoid showing users long tracebacks
                # full of Hypothesis internals they don't care about.
                # We have to do this inline, to avoid adding another
                # internal stack frame just when we've removed the rest.
                #
                # Using a variable for our trimmed error ensures that the line
                # which will actually appear in tracebacks is as clear as
                # possible - "raise the_error_hypothesis_found".
                the_error_hypothesis_found = e.with_traceback(
                    get_trimmed_traceback()
                )
>               raise the_error_hypothesis_found

venv/lib/python3.8/site-packages/hypothesis/core.py:1162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_shap>, num_rounds = 1
dataset = boston
param = {'eval_metric': 'rmse', 'gpu_id': 0, 'max_depth': 0, 'max_leaves': 1, ...}

    @given(strategies.integers(1, 10),
           tm.dataset_strategy, shap_parameter_strategy)
    @settings(deadline=None)
    def test_shap(self, num_rounds, dataset, param):
        param.update({"predictor": "gpu_predictor", "gpu_id": 0})
        param = dataset.set_params(param)
        dmat = dataset.get_dmat()
>       bst = xgb.train(param, dmat, num_rounds)

tests/python-gpu/test_gpu_prediction.py:204: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eval_metric': 'rmse', 'gpu_id': 0, 'max_depth': 0, 'max_leaves': 1, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf5034d9d0>, num_boost_round = 1
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eval_metric': 'rmse', 'gpu_id': 0, 'max_depth': 0, 'max_leaves': 1, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf5034d9d0>, num_boost_round = 1
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf5034dc40>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf5034de20>
dtrain = <xgboost.core.DMatrix object at 0x7fdf5034d9d0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:16:33] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::GetPredictor(xgboost::HostDeviceVector<float> const*, xgboost::DMatrix*) const+0x225) [0x7fdf56032c35]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::PredictBatch(xgboost::DMatrix*, xgboost::PredictionCacheEntry*, bool, unsigned int)+0x49) [0x7fdf56032f29]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x3b2) [0x7fdf56062d42]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x137c0) [0x7fdf779737c0]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
---------------------------------- Hypothesis ----------------------------------
Falsifying example: test_shap(
    self=<test_gpu_prediction.TestGPUPredict testMethod=test_shap>,
    num_rounds=1,
    dataset=boston,
    param={'max_depth': 0, 'max_leaves': 1, 'num_parallel_tree': 1},
)
____________________ TestGPUPredict.test_shap_interactions _____________________

self = <unittest.case._Outcome object at 0x7fdf5043c2b0>
test_case = <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>
result = <TestCaseFunction test_shap_interactions>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>
method = <bound method accept.<locals>.test_shap_interactions of <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>

>   ???

tests/python-gpu/test_gpu_prediction.py:212: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arguments = ()
kwargs = {'self': <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>}
__tracebackhide__ = True
test = <function TestGPUPredict.test_shap_interactions at 0x7fdf52211700>
settings = settings(database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples'), deadline=None, derand...nt_blob=False, report_multiple_bugs=True, stateful_step_count=50, suppress_health_check=(), verbosity=Verbosity.normal)
random = <random.Random object at 0x2e20c60>
processed_args = ((), {'self': <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>}, <function default_new_style_exe...p(lambda args: dict(args, **kwargs)))), runner=<test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>))
test_runner = <function default_new_style_executor at 0x7fdf77a913a0>
search_strategy = WithRunner(TupleStrategy((just(()), fixed_dictionaries({'dataset': _dataset_weight_margin(),
 'num_rounds': integers(m...ap(lambda args: dict(args, **kwargs)))), runner=<test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>)
runner = <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>
state = <hypothesis.core.StateForActualGivenExecution object at 0x7fdf50651cd0>

    @impersonate(test)
    @define_function_signature(test.__name__, test.__doc__, argspec)
    def wrapped_test(*arguments, **kwargs):
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
    
        test = wrapped_test.hypothesis.inner_test
    
        if getattr(test, "is_hypothesis_test", False):
            raise InvalidArgument(
                (
                    "You have applied @given to the test %s more than once, which "
                    "wraps the test several times and is extremely slow. A "
                    "similar effect can be gained by combining the arguments "
                    "of the two calls to given. For example, instead of "
                    "@given(booleans()) @given(integers()), you could write "
                    "@given(booleans(), integers())"
                )
                % (test.__name__,)
            )
    
        settings = wrapped_test._hypothesis_internal_use_settings
    
        random = get_random_for_wrapped_test(test, wrapped_test)
    
        processed_args = process_arguments_to_given(
            wrapped_test, arguments, kwargs, given_kwargs, argspec
        )
        arguments, kwargs, test_runner, search_strategy = processed_args
    
        runner = getattr(search_strategy, "runner", None)
        if isinstance(runner, TestCase) and test.__name__ in dir(TestCase):
            msg = (
                "You have applied @given to the method %s, which is "
                "used by the unittest runner but is not itself a test."
                "  This is not useful in any way." % test.__name__
            )
            fail_health_check(settings, msg, HealthCheck.not_a_test_method)
        if bad_django_TestCase(runner):  # pragma: no cover
            # Covered by the Django tests, but not the pytest coverage task
            raise InvalidArgument(
                "You have applied @given to a method on %s, but this "
                "class does not inherit from the supported versions in "
                "`hypothesis.extra.django`.  Use the Hypothesis variants "
                "to ensure that each example is run in a separate "
                "database transaction." % qualname(type(runner))
            )
    
        state = StateForActualGivenExecution(
            test_runner, search_strategy, test, settings, random, wrapped_test
        )
    
        reproduce_failure = wrapped_test._hypothesis_internal_use_reproduce_failure
    
        # If there was a @reproduce_failure decorator, use it to reproduce
        # the error (or complain that we couldn't). Either way, this will
        # always raise some kind of error.
        if reproduce_failure is not None:
            expected_version, failure = reproduce_failure
            if expected_version != __version__:
                raise InvalidArgument(
                    (
                        "Attempting to reproduce a failure from a different "
                        "version of Hypothesis. This failure is from %s, but "
                        "you are currently running %r. Please change your "
                        "Hypothesis version to a matching one."
                    )
                    % (expected_version, __version__)
                )
            try:
                state.execute_once(
                    ConjectureData.for_buffer(decode_failure(failure)),
                    print_example=True,
                    is_final=True,
                )
                raise DidNotReproduce(
                    "Expected the test to raise an error, but it "
                    "completed successfully."
                )
            except StopTest:
                raise DidNotReproduce(
                    "The shape of the test data has changed in some way "
                    "from where this blob was defined. Are you sure "
                    "you're running the same test?"
                )
            except UnsatisfiedAssumption:
                raise DidNotReproduce(
                    "The test data failed to satisfy an assumption in the "
                    "test. Have you added it since this blob was "
                    "generated?"
                )
    
        # There was no @reproduce_failure, so start by running any explicit
        # examples from @example decorators.
        errors = list(
            execute_explicit_examples(state, wrapped_test, arguments, kwargs)
        )
        with local_settings(state.settings):
            if len(errors) > 1:
                # If we're not going to report multiple bugs, we would have
                # stopped running explicit examples at the first failure.
                assert state.settings.report_multiple_bugs
                for fragments, err in errors:
                    for f in fragments:
                        report(f)
                    tb_lines = traceback.format_exception(
                        type(err), err, err.__traceback__
                    )
                    report("".join(tb_lines))
                msg = "Hypothesis found %d failures in explicit examples."
                raise MultipleFailures(msg % (len(errors)))
            elif errors:
                fragments, the_error_hypothesis_found = errors[0]
                for f in fragments:
                    report(f)
                raise the_error_hypothesis_found
    
        # If there were any explicit examples, they all ran successfully.
        # The next step is to use the Conjecture engine to run the test on
        # many different inputs.
    
        if not (
            Phase.reuse in settings.phases or Phase.generate in settings.phases
        ):
            return
    
        try:
            if isinstance(runner, TestCase) and hasattr(runner, "subTest"):
                subTest = runner.subTest
                try:
                    runner.subTest = types.MethodType(fake_subTest, runner)
                    state.run_engine()
                finally:
                    runner.subTest = subTest
            else:
                state.run_engine()
        except BaseException as e:
            # The exception caught here should either be an actual test
            # failure (or MultipleFailures), or some kind of fatal error
            # that caused the engine to stop.
    
            generated_seed = wrapped_test._hypothesis_internal_use_generated_seed
            with local_settings(settings):
                if not (state.failed_normally or generated_seed is None):
                    if running_under_pytest:
                        report(
                            "You can add @seed(%(seed)d) to this test or "
                            "run pytest with --hypothesis-seed=%(seed)d "
                            "to reproduce this failure." % {"seed": generated_seed}
                        )
                    else:
                        report(
                            "You can add @seed(%d) to this test to "
                            "reproduce this failure." % (generated_seed,)
                        )
                # The dance here is to avoid showing users long tracebacks
                # full of Hypothesis internals they don't care about.
                # We have to do this inline, to avoid adding another
                # internal stack frame just when we've removed the rest.
                #
                # Using a variable for our trimmed error ensures that the line
                # which will actually appear in tracebacks is as clear as
                # possible - "raise the_error_hypothesis_found".
                the_error_hypothesis_found = e.with_traceback(
                    get_trimmed_traceback()
                )
>               raise the_error_hypothesis_found

venv/lib/python3.8/site-packages/hypothesis/core.py:1162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arguments = ()
kwargs = {'self': <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>}
__tracebackhide__ = True
test = <function TestGPUPredict.test_shap_interactions at 0x7fdf52211700>
settings = settings(database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples'), deadline=None, derand...nt_blob=False, report_multiple_bugs=True, stateful_step_count=50, suppress_health_check=(), verbosity=Verbosity.normal)
random = <random.Random object at 0x2e20c60>
processed_args = ((), {'self': <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>}, <function default_new_style_exe...p(lambda args: dict(args, **kwargs)))), runner=<test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>))
test_runner = <function default_new_style_executor at 0x7fdf77a913a0>
search_strategy = WithRunner(TupleStrategy((just(()), fixed_dictionaries({'dataset': _dataset_weight_margin(),
 'num_rounds': integers(m...ap(lambda args: dict(args, **kwargs)))), runner=<test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>)
runner = <test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>
state = <hypothesis.core.StateForActualGivenExecution object at 0x7fdf50651cd0>

    @impersonate(test)
    @define_function_signature(test.__name__, test.__doc__, argspec)
    def wrapped_test(*arguments, **kwargs):
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
    
        test = wrapped_test.hypothesis.inner_test
    
        if getattr(test, "is_hypothesis_test", False):
            raise InvalidArgument(
                (
                    "You have applied @given to the test %s more than once, which "
                    "wraps the test several times and is extremely slow. A "
                    "similar effect can be gained by combining the arguments "
                    "of the two calls to given. For example, instead of "
                    "@given(booleans()) @given(integers()), you could write "
                    "@given(booleans(), integers())"
                )
                % (test.__name__,)
            )
    
        settings = wrapped_test._hypothesis_internal_use_settings
    
        random = get_random_for_wrapped_test(test, wrapped_test)
    
        processed_args = process_arguments_to_given(
            wrapped_test, arguments, kwargs, given_kwargs, argspec
        )
        arguments, kwargs, test_runner, search_strategy = processed_args
    
        runner = getattr(search_strategy, "runner", None)
        if isinstance(runner, TestCase) and test.__name__ in dir(TestCase):
            msg = (
                "You have applied @given to the method %s, which is "
                "used by the unittest runner but is not itself a test."
                "  This is not useful in any way." % test.__name__
            )
            fail_health_check(settings, msg, HealthCheck.not_a_test_method)
        if bad_django_TestCase(runner):  # pragma: no cover
            # Covered by the Django tests, but not the pytest coverage task
            raise InvalidArgument(
                "You have applied @given to a method on %s, but this "
                "class does not inherit from the supported versions in "
                "`hypothesis.extra.django`.  Use the Hypothesis variants "
                "to ensure that each example is run in a separate "
                "database transaction." % qualname(type(runner))
            )
    
        state = StateForActualGivenExecution(
            test_runner, search_strategy, test, settings, random, wrapped_test
        )
    
        reproduce_failure = wrapped_test._hypothesis_internal_use_reproduce_failure
    
        # If there was a @reproduce_failure decorator, use it to reproduce
        # the error (or complain that we couldn't). Either way, this will
        # always raise some kind of error.
        if reproduce_failure is not None:
            expected_version, failure = reproduce_failure
            if expected_version != __version__:
                raise InvalidArgument(
                    (
                        "Attempting to reproduce a failure from a different "
                        "version of Hypothesis. This failure is from %s, but "
                        "you are currently running %r. Please change your "
                        "Hypothesis version to a matching one."
                    )
                    % (expected_version, __version__)
                )
            try:
                state.execute_once(
                    ConjectureData.for_buffer(decode_failure(failure)),
                    print_example=True,
                    is_final=True,
                )
                raise DidNotReproduce(
                    "Expected the test to raise an error, but it "
                    "completed successfully."
                )
            except StopTest:
                raise DidNotReproduce(
                    "The shape of the test data has changed in some way "
                    "from where this blob was defined. Are you sure "
                    "you're running the same test?"
                )
            except UnsatisfiedAssumption:
                raise DidNotReproduce(
                    "The test data failed to satisfy an assumption in the "
                    "test. Have you added it since this blob was "
                    "generated?"
                )
    
        # There was no @reproduce_failure, so start by running any explicit
        # examples from @example decorators.
        errors = list(
            execute_explicit_examples(state, wrapped_test, arguments, kwargs)
        )
        with local_settings(state.settings):
            if len(errors) > 1:
                # If we're not going to report multiple bugs, we would have
                # stopped running explicit examples at the first failure.
                assert state.settings.report_multiple_bugs
                for fragments, err in errors:
                    for f in fragments:
                        report(f)
                    tb_lines = traceback.format_exception(
                        type(err), err, err.__traceback__
                    )
                    report("".join(tb_lines))
                msg = "Hypothesis found %d failures in explicit examples."
                raise MultipleFailures(msg % (len(errors)))
            elif errors:
                fragments, the_error_hypothesis_found = errors[0]
                for f in fragments:
                    report(f)
                raise the_error_hypothesis_found
    
        # If there were any explicit examples, they all ran successfully.
        # The next step is to use the Conjecture engine to run the test on
        # many different inputs.
    
        if not (
            Phase.reuse in settings.phases or Phase.generate in settings.phases
        ):
            return
    
        try:
            if isinstance(runner, TestCase) and hasattr(runner, "subTest"):
                subTest = runner.subTest
                try:
                    runner.subTest = types.MethodType(fake_subTest, runner)
>                   state.run_engine()

venv/lib/python3.8/site-packages/hypothesis/core.py:1127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <hypothesis.core.StateForActualGivenExecution object at 0x7fdf50651cd0>

    def run_engine(self):
        """Run the test function many times, on database input and generated
        input, using the Conjecture engine.
        """
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
        try:
            database_key = self.wrapped_test._hypothesis_internal_database_key
        except AttributeError:
            if global_force_seed is None:
                database_key = function_digest(self.test)
            else:
                database_key = None
    
        runner = ConjectureRunner(
            self._execute_once_for_engine,
            settings=self.settings,
            random=self.random,
            database_key=database_key,
        )
        # Use the Conjecture engine to run the test function many times
        # on different inputs.
        runner.run()
        note_statistics(runner.statistics)
    
        if runner.call_count == 0:
            return
        if runner.interesting_examples:
            self.falsifying_examples = sorted(
                runner.interesting_examples.values(),
                key=lambda d: sort_key(d.buffer),
                reverse=True,
            )
        else:
            if runner.valid_examples == 0:
                raise Unsatisfiable(
                    "Unable to satisfy assumptions of hypothesis %s."
                    % (get_pretty_function_description(self.test),)
                )
    
        if not self.falsifying_examples:
            return
        elif not self.settings.report_multiple_bugs:
            # Pretend that we only found one failure, by discarding the others.
            del self.falsifying_examples[:-1]
    
        # The engine found one or more failures, so we need to reproduce and
        # report them.
    
        self.failed_normally = True
    
        flaky = 0
    
        if runner.best_observed_targets:
            for line in describe_targets(runner.best_observed_targets):
                report(line)
            report("")
    
        for falsifying_example in self.falsifying_examples:
            info = falsifying_example.extra_information
    
            ran_example = ConjectureData.for_buffer(falsifying_example.buffer)
            self.__was_flaky = False
            assert info.__expected_exception is not None
            try:
                self.execute_once(
                    ran_example,
                    print_example=not self.is_find,
                    is_final=True,
                    expected_failure=(
                        info.__expected_exception,
                        info.__expected_traceback,
                    ),
                )
            except (UnsatisfiedAssumption, StopTest):
                report(traceback.format_exc())
                self.__flaky(
                    "Unreliable assumption: An example which satisfied "
                    "assumptions on the first run now fails it."
                )
            except BaseException as e:
                if len(self.falsifying_examples) <= 1:
                    # There is only one failure, so we can report it by raising
                    # it directly.
                    raise
    
                # We are reporting multiple failures, so we need to manually
                # print each exception's stack trace and information.
                tb = get_trimmed_traceback()
                report("".join(traceback.format_exception(type(e), e, tb)))
    
            finally:  # pragma: no cover
                # Mostly useful for ``find`` and ensuring that objects that
                # hold on to a reference to ``data`` know that it's now been
                # finished and they shouldn't attempt to draw more data from
                # it.
                ran_example.freeze()
    
                # This section is in fact entirely covered by the tests in
                # test_reproduce_failure, but it seems to trigger a lovely set
                # of coverage bugs: The branches show up as uncovered (despite
                # definitely being covered - you can add an assert False else
                # branch to verify this and see it fail - and additionally the
                # second branch still complains about lack of coverage even if
                # you add a pragma: no cover to it!
                # See https://bitbucket.org/ned/coveragepy/issues/623/
                if self.settings.print_blob:
                    report(
                        (
                            "\nYou can reproduce this example by temporarily "
                            "adding @reproduce_failure(%r, %r) as a decorator "
                            "on your test case"
                        )
                        % (__version__, encode_failure(falsifying_example.buffer))
                    )
            if self.__was_flaky:
                flaky += 1
    
        # If we only have one example then we should have raised an error or
        # flaky prior to this point.
        assert len(self.falsifying_examples) > 1
    
        if flaky > 0:
            raise Flaky(
                (
                    "Hypothesis found %d distinct failures, but %d of them "
                    "exhibited some sort of flaky behaviour."
                )
                % (len(self.falsifying_examples), flaky)
            )
        else:
>           raise MultipleFailures(
                ("Hypothesis found %d distinct failures.")
                % (len(self.falsifying_examples))
            )
E           hypothesis.errors.MultipleFailures: Hypothesis found 2 distinct failures.

venv/lib/python3.8/site-packages/hypothesis/core.py:859: MultipleFailures
---------------------------------- Hypothesis ----------------------------------
Falsifying example: test_shap_interactions(
    self=<test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>,
    num_rounds=1,
    dataset=sparse,
    param={'max_depth': 0, 'max_leaves': 1, 'num_parallel_tree': 1},
)
Traceback (most recent call last):
  File "/home/otivedani/xgboost/tests/python-gpu/test_gpu_prediction.py", line 217, in test_shap_interactions
    pytest.xfail(reason=f'GPU shap with sparse is flaky: {issue}')
  File "/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/outcomes.py", line 172, in xfail
    raise XFailed(reason)
_pytest.outcomes.XFailed: GPU shap with sparse is flaky: https://github.com/dmlc/xgboost/issues/6074

Falsifying example: test_shap_interactions(
    self=<test_gpu_prediction.TestGPUPredict testMethod=test_shap_interactions>,
    num_rounds=1,
    dataset=boston,
    param={'max_depth': 0, 'max_leaves': 1, 'num_parallel_tree': 1},
)
Traceback (most recent call last):
  File "/home/otivedani/xgboost/tests/python-gpu/test_gpu_prediction.py", line 222, in test_shap_interactions
    bst = xgb.train(param, dmat, num_rounds)
  File "/home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/training.py", line 216, in train
    bst = _train_internal(params, dtrain,
  File "/home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/training.py", line 111, in _train_internal
    bst.update(dtrain, i, obj)
  File "/home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/core.py", line 1178, in update
    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
  File "/home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/core.py", line 186, in _check_call
    raise XGBoostError(py_str(_LIB.XGBGetLastError()))
xgboost.core.XGBoostError: [22:16:33] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
Stack trace:
  [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
  [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::GetPredictor(xgboost::HostDeviceVector<float> const*, xgboost::DMatrix*) const+0x225) [0x7fdf56032c35]
  [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::PredictBatch(xgboost::DMatrix*, xgboost::PredictionCacheEntry*, bool, unsigned int)+0x49) [0x7fdf56032f29]
  [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x3b2) [0x7fdf56062d42]
  [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x137c0) [0x7fdf779737c0]


_________________________ TestGPUPredict.test_sklearn __________________________

self = <unittest.case._Outcome object at 0x7fdf5066ecd0>
test_case = <test_gpu_prediction.TestGPUPredict testMethod=test_sklearn>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_sklearn>
result = <TestCaseFunction test_sklearn>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_sklearn>
method = <bound method TestGPUPredict.test_sklearn of <test_gpu_prediction.TestGPUPredict testMethod=test_sklearn>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_prediction.TestGPUPredict testMethod=test_sklearn>

    @pytest.mark.skipif(**tm.no_sklearn())
    def test_sklearn(self):
        m, n = 15000, 14
        tr_size = 2500
        X = np.random.rand(m, n)
        y = 200 * np.matmul(X, np.arange(-3, -3 + n))
        X_train, y_train = X[:tr_size, :], y[:tr_size]
        X_test, y_test = X[tr_size:, :], y[tr_size:]
    
        # First with cpu_predictor
        params = {'tree_method': 'gpu_hist',
                  'predictor': 'cpu_predictor',
                  'n_jobs': -1,
                  'seed': 123}
>       m = xgb.XGBRegressor(**params).fit(X_train, y_train)

tests/python-gpu/test_gpu_prediction.py:116: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,
             colsample_bynode=None, colsample_bytr...s_weight=None, seed=123, subsample=None,
             tree_method='gpu_hist', validate_parameters=None, verbosity=None)
X = array([[0.44190428, 0.58157741, 0.98975171, ..., 0.79860424, 0.29733138,
        0.02760601],
       [0.59343245, 0.84...44,
        0.07752733],
       [0.70544139, 0.52570295, 0.57479217, ..., 0.85518733, 0.85803751,
        0.04484167]])
y = array([3106.44411075, 4751.03056709, 4862.18427997, ..., 4386.84112123,
       5714.23745935, 4689.73084773])
sample_weight = None, base_margin = None, eval_set = None, eval_metric = None
early_stopping_rounds = None, verbose = True, xgb_model = None
sample_weight_eval_set = None, feature_weights = None, callbacks = None

    def fit(self, X, y, sample_weight=None, base_margin=None,
            eval_set=None, eval_metric=None, early_stopping_rounds=None,
            verbose=True, xgb_model=None, sample_weight_eval_set=None,
            feature_weights=None,
            callbacks=None):
        # pylint: disable=invalid-name,attribute-defined-outside-init
        """Fit gradient boosting model
    
        Parameters
        ----------
        X : array_like
            Feature matrix
        y : array_like
            Labels
        sample_weight : array_like
            instance weights
        base_margin : array_like
            global bias for each instance.
        eval_set : list, optional
            A list of (X, y) tuple pairs to use as validation sets, for which
            metrics will be computed.
            Validation metrics will help us track the performance of the model.
        eval_metric : str, list of str, or callable, optional
            If a str, should be a built-in evaluation metric to use. See
            doc/parameter.rst.
            If a list of str, should be the list of multiple built-in evaluation metrics
            to use.
            If callable, a custom evaluation metric. The call
            signature is ``func(y_predicted, y_true)`` where ``y_true`` will be a
            DMatrix object such that you may need to call the ``get_label``
            method. It must return a str, value pair where the str is a name
            for the evaluation and value is the value of the evaluation
            function. The callable custom objective is always minimized.
        early_stopping_rounds : int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **eval_set**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **eval_set**, the last entry will be used
            for early stopping.
            If there's more than one metric in **eval_metric**, the last metric will be
            used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``clf.best_score``, ``clf.best_iteration`` and ``clf.best_ntree_limit``.
        verbose : bool
            If `verbose` and an evaluation set is used, writes the evaluation
            metric measured on the validation set to stderr.
        xgb_model : str
            file name of stored XGBoost model or 'Booster' instance XGBoost model to be
            loaded before training (allows training continuation).
        sample_weight_eval_set : list, optional
            A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of
            instance weights on the i-th validation set.
        feature_weights: array_like
            Weight for each feature, defines the probability of each feature
            being selected when colsample is being used.  All values must be
            greater than 0, otherwise a `ValueError` is thrown.
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using :ref:`callback_api`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        """
        self.n_features_in_ = X.shape[1]
    
        train_dmatrix = DMatrix(data=X, label=y, weight=sample_weight,
                                base_margin=base_margin,
                                missing=self.missing,
                                nthread=self.n_jobs)
        train_dmatrix.set_info(feature_weights=feature_weights)
    
        evals_result = {}
    
        if eval_set is not None:
            if not isinstance(eval_set[0], (list, tuple)):
                raise TypeError('Unexpected input type for `eval_set`')
            if sample_weight_eval_set is None:
                sample_weight_eval_set = [None] * len(eval_set)
            else:
                assert len(eval_set) == len(sample_weight_eval_set)
            evals = list(
                DMatrix(eval_set[i][0], label=eval_set[i][1], missing=self.missing,
                        weight=sample_weight_eval_set[i], nthread=self.n_jobs)
                for i in range(len(eval_set)))
            evals = list(zip(evals, ["validation_{}".format(i) for i in
                                     range(len(evals))]))
        else:
            evals = ()
    
        params = self.get_xgb_params()
    
        if callable(self.objective):
            obj = _objective_decorator(self.objective)
            params["objective"] = "reg:squarederror"
        else:
            obj = None
    
        feval = eval_metric if callable(eval_metric) else None
        if eval_metric is not None:
            if callable(eval_metric):
                eval_metric = None
            else:
                params.update({'eval_metric': eval_metric})
    
>       self._Booster = train(params, train_dmatrix,
                              self.get_num_boosting_rounds(), evals=evals,
                              early_stopping_rounds=early_stopping_rounds,
                              evals_result=evals_result,
                              obj=obj, feval=feval,
                              verbose_eval=verbose, xgb_model=xgb_model,
                              callbacks=callbacks)

venv/lib/python3.8/site-packages/xgboost/sklearn.py:549: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf5066e9d0>, num_boost_round = 100
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = {}, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf5066e9d0>, num_boost_round = 100
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf5066ee50>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf5066eb20>
dtrain = <xgboost.core.DMatrix object at 0x7fdf5066e9d0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:16:33] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
____________________ TestRanking.test_training_rank_map_auc ____________________

self = <unittest.case._Outcome object at 0x7fdf503224c0>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_auc>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_auc>
result = <TestCaseFunction test_training_rank_map_auc>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_auc>
method = <bound method TestRanking.test_training_rank_map_auc of <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_auc>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_auc>

    def test_training_rank_map_auc(self):
        """
        Train an XGBoost ranking model with map objective function and compare auc metric
        """
>       self.__test_training_with_rank_objective('rank:map', 'auc')

tests/python-gpu/test_gpu_ranking.py:183: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:map'
metric_name = 'auc', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'auc', 'gpu_id': 0, 'objective': 'rank:map', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'auc', 'gpu_id': 0, 'objective': 'rank:map', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf50322af0>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf503221c0>
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:15] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
____________________ TestRanking.test_training_rank_map_map ____________________

self = <unittest.case._Outcome object at 0x7fdf663383a0>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_map>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_map>
result = <TestCaseFunction test_training_rank_map_map>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_map>
method = <bound method TestRanking.test_training_rank_map_map of <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_map>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_map>

    def test_training_rank_map_map(self):
        """
        Train an XGBoost ranking model with map objective function and compare map metric
        """
>       self.__test_training_with_rank_objective('rank:map', 'map')

tests/python-gpu/test_gpu_ranking.py:177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:map'
metric_name = 'map', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'map', 'gpu_id': 0, 'objective': 'rank:map', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'map', 'gpu_id': 0, 'objective': 'rank:map', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf66338580>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf66338370>
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:15] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
___________________ TestRanking.test_training_rank_map_ndcg ____________________

self = <unittest.case._Outcome object at 0x7fdf50322d90>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_ndcg>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_ndcg>
result = <TestCaseFunction test_training_rank_map_ndcg>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_ndcg>
method = <bound method TestRanking.test_training_rank_map_ndcg of <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_ndcg>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_map_ndcg>

    def test_training_rank_map_ndcg(self):
        """
        Train an XGBoost ranking model with map objective function and compare ndcg metric
        """
>       self.__test_training_with_rank_objective('rank:map', 'ndcg')

tests/python-gpu/test_gpu_ranking.py:189: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:map'
metric_name = 'ndcg', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'ndcg', 'gpu_id': 0, 'objective': 'rank:map', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'ndcg', 'gpu_id': 0, 'objective': 'rank:map', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf503221c0>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf50322af0>
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:15] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
___________________ TestRanking.test_training_rank_ndcg_auc ____________________

self = <unittest.case._Outcome object at 0x7fdf505d7cd0>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_auc>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_auc>
result = <TestCaseFunction test_training_rank_ndcg_auc>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_auc>
method = <bound method TestRanking.test_training_rank_ndcg_auc of <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_auc>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_auc>

    def test_training_rank_ndcg_auc(self):
        """
        Train an XGBoost ranking model with ndcg objective function and compare auc metric
        """
>       self.__test_training_with_rank_objective('rank:ndcg', 'auc')

tests/python-gpu/test_gpu_ranking.py:165: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:ndcg'
metric_name = 'auc', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'auc', 'gpu_id': 0, 'objective': 'rank:ndcg', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'auc', 'gpu_id': 0, 'objective': 'rank:ndcg', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf505d7e50>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf505d7d90>
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:15] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
___________________ TestRanking.test_training_rank_ndcg_map ____________________

self = <unittest.case._Outcome object at 0x7fdf50653f40>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_map>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_map>
result = <TestCaseFunction test_training_rank_ndcg_map>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_map>
method = <bound method TestRanking.test_training_rank_ndcg_map of <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_map>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_map>

    def test_training_rank_ndcg_map(self):
        """
        Train an XGBoost ranking model with ndcg objective function and compare map metric
        """
>       self.__test_training_with_rank_objective('rank:ndcg', 'map')

tests/python-gpu/test_gpu_ranking.py:159: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:ndcg'
metric_name = 'map', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'map', 'gpu_id': 0, 'objective': 'rank:ndcg', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'map', 'gpu_id': 0, 'objective': 'rank:ndcg', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf506534c0>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf506530d0>
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:15] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
___________________ TestRanking.test_training_rank_ndcg_ndcg ___________________

self = <unittest.case._Outcome object at 0x7fdf505d7c10>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_ndcg>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_ndcg>
result = <TestCaseFunction test_training_rank_ndcg_ndcg>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_ndcg>
method = <bound method TestRanking.test_training_rank_ndcg_ndcg of <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_ndcg>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_ndcg_ndcg>

    def test_training_rank_ndcg_ndcg(self):
        """
        Train an XGBoost ranking model with ndcg objective function and compare ndcg metric
        """
>       self.__test_training_with_rank_objective('rank:ndcg', 'ndcg')

tests/python-gpu/test_gpu_ranking.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:ndcg'
metric_name = 'ndcg', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'ndcg', 'gpu_id': 0, 'objective': 'rank:ndcg', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'ndcg', 'gpu_id': 0, 'objective': 'rank:ndcg', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf505d7ca0>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf505d7b50>
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:15] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
______________ TestRanking.test_training_rank_pairwise_auc_metric ______________

self = <unittest.case._Outcome object at 0x7fdf503b74c0>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_auc_metric>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_auc_metric>
result = <TestCaseFunction test_training_rank_pairwise_auc_metric>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_auc_metric>
method = <bound method TestRanking.test_training_rank_pairwise_auc_metric of <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_auc_metric>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_auc_metric>

    def test_training_rank_pairwise_auc_metric(self):
        """
        Train an XGBoost ranking model with pairwise objective function and compare auc metric
        """
>       self.__test_training_with_rank_objective('rank:pairwise', 'auc')

tests/python-gpu/test_gpu_ranking.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:pairwise'
metric_name = 'auc', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'auc', 'gpu_id': 0, 'objective': 'rank:pairwise', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'auc', 'gpu_id': 0, 'objective': 'rank:pairwise', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf503b7730>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf503b71f0>
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:15] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
______________ TestRanking.test_training_rank_pairwise_map_metric ______________

self = <unittest.case._Outcome object at 0x7fdf502ef370>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_map_metric>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_map_metric>
result = <TestCaseFunction test_training_rank_pairwise_map_metric>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_map_metric>
method = <bound method TestRanking.test_training_rank_pairwise_map_metric of <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_map_metric>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_map_metric>

    def test_training_rank_pairwise_map_metric(self):
        """
        Train an XGBoost ranking model with pairwise objective function and compare map metric
        """
>       self.__test_training_with_rank_objective('rank:pairwise', 'map')

tests/python-gpu/test_gpu_ranking.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:pairwise'
metric_name = 'map', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'map', 'gpu_id': 0, 'objective': 'rank:pairwise', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'map', 'gpu_id': 0, 'objective': 'rank:pairwise', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf502ef400>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf502ef280>
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:15] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
_____________ TestRanking.test_training_rank_pairwise_ndcg_metric ______________

self = <unittest.case._Outcome object at 0x7fdf505d7eb0>
test_case = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_ndcg_metric>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_ndcg_metric>
result = <TestCaseFunction test_training_rank_pairwise_ndcg_metric>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_ndcg_metric>
method = <bound method TestRanking.test_training_rank_pairwise_ndcg_metric of <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_ndcg_metric>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_ranking.TestRanking testMethod=test_training_rank_pairwise_ndcg_metric>

    def test_training_rank_pairwise_ndcg_metric(self):
        """
        Train an XGBoost ranking model with pairwise objective function and compare ndcg metric
        """
>       self.__test_training_with_rank_objective('rank:pairwise', 'ndcg')

tests/python-gpu/test_gpu_ranking.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'test_gpu_ranking.TestRanking'>, rank_objective = 'rank:pairwise'
metric_name = 'ndcg', tolerance = 0.01

    @classmethod
    def __test_training_with_rank_objective(cls, rank_objective, metric_name, tolerance=1e-02):
        """
        Internal method that trains the dataset using the rank objective on GPU and CPU, evaluates
        the metric and determines if the delta between the metric is within the tolerance level
        :return:
        """
        # specify validations set to watch performance
        watchlist = [(cls.dtest, 'eval'), (cls.dtrain, 'train')]
    
        num_trees = 2500
        check_metric_improvement_rounds = 10
    
        evals_result = {}
        cls.params['objective'] = rank_objective
        cls.params['eval_metric'] = metric_name
>       bst = xgboost.train(
            cls.params, cls.dtrain, num_boost_round=num_trees,
            early_stopping_rounds=check_metric_improvement_rounds,
            evals=watchlist, evals_result=evals_result)

tests/python-gpu/test_gpu_ranking.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'ndcg', 'gpu_id': 0, 'objective': 'rank:pairwise', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, maximize = None, early_stopping_rounds = 10
evals_result = {}, verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'booster': 'gbtree', 'eval_metric': 'ndcg', 'gpu_id': 0, 'objective': 'rank:pairwise', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, num_boost_round = 2500
evals = [(<xgboost.core.DMatrix object at 0x7fdf508b8070>, 'eval'), (<xgboost.core.DMatrix object at 0x7fdf50322640>, 'train')]
obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf505d7070>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = 10

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf505d71c0>
dtrain = <xgboost.core.DMatrix object at 0x7fdf50322640>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:15] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
______ TestGPUTrainingContinuation.test_gpu_training_continuation_binary _______

self = <unittest.case._Outcome object at 0x7fdf503893a0>
test_case = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_binary>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_binary>
result = <TestCaseFunction test_gpu_training_continuation_binary>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_binary>
method = <bound method TestGPUTrainingContinuation.test_gpu_training_continuation_binary of <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_binary>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_binary>

    def test_gpu_training_continuation_binary(self):
>       self.run_training_continuation(False)

tests/python-gpu/test_gpu_training_continuation.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_binary>
use_json = False

    def run_training_continuation(self, use_json):
        kRows = 64
        kCols = 32
        X = np.random.randn(kRows, kCols)
        y = np.random.randn(kRows)
        dtrain = xgb.DMatrix(X, y)
        params = {'tree_method': 'gpu_hist', 'max_depth': '2',
                  'gamma': '0.1', 'alpha': '0.01',
                  'enable_experimental_json_serialization': use_json}
>       bst_0 = xgb.train(params, dtrain, num_boost_round=64)

tests/python-gpu/test_gpu_training_continuation.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'alpha': '0.01', 'enable_experimental_json_serialization': False, 'gamma': '0.1', 'max_depth': '2', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50389130>, num_boost_round = 64
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'alpha': '0.01', 'enable_experimental_json_serialization': False, 'gamma': '0.1', 'max_depth': '2', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50389130>, num_boost_round = 64
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf503891f0>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf503891c0>
dtrain = <xgboost.core.DMatrix object at 0x7fdf50389130>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:15] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
_______ TestGPUTrainingContinuation.test_gpu_training_continuation_json ________

self = <unittest.case._Outcome object at 0x7fdf50460490>
test_case = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_json>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_json>
result = <TestCaseFunction test_gpu_training_continuation_json>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_json>
method = <bound method TestGPUTrainingContinuation.test_gpu_training_continuation_json of <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_json>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_json>

    def test_gpu_training_continuation_json(self):
>       self.run_training_continuation(True)

tests/python-gpu/test_gpu_training_continuation.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_training_continuation.TestGPUTrainingContinuation testMethod=test_gpu_training_continuation_json>
use_json = True

    def run_training_continuation(self, use_json):
        kRows = 64
        kCols = 32
        X = np.random.randn(kRows, kCols)
        y = np.random.randn(kRows)
        dtrain = xgb.DMatrix(X, y)
        params = {'tree_method': 'gpu_hist', 'max_depth': '2',
                  'gamma': '0.1', 'alpha': '0.01',
                  'enable_experimental_json_serialization': use_json}
>       bst_0 = xgb.train(params, dtrain, num_boost_round=64)

tests/python-gpu/test_gpu_training_continuation.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'alpha': '0.01', 'enable_experimental_json_serialization': True, 'gamma': '0.1', 'max_depth': '2', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50460550>, num_boost_round = 64
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'alpha': '0.01', 'enable_experimental_json_serialization': True, 'gamma': '0.1', 'max_depth': '2', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf50460550>, num_boost_round = 64
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf504604c0>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf50460f10>
dtrain = <xgboost.core.DMatrix object at 0x7fdf50460550>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:15] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
________________________ TestGPUUpdaters.test_gpu_hist _________________________

cls = <class '_pytest.runner.CallInfo'>
func = <function call_runtest_hook.<locals>.<lambda> at 0x7fdf503791f0>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: "Optional[Union[Type[BaseException], Tuple[Type[BaseException], ...]]]" = None,
    ) -> "CallInfo[TResult]":
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result = func()  # type: Optional[TResult]

.venv/lib/python3.8/site-packages/_pytest/runner.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: ihook(item=item, **kwds), when=when, reraise=reraise
    )

.venv/lib/python3.8/site-packages/_pytest/runner.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_runtest_call'>, args = ()
kwargs = {'item': <Function test_gpu_hist>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fdf78b19310>
hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
kwargs = {'item': <Function test_gpu_hist>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
kwargs = {'item': <Function test_gpu_hist>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
caller_kwargs = {'item': <Function test_gpu_hist>}, firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fdf52188340>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
caller_kwargs = {'item': <Function test_gpu_hist>}, firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_gpu_hist>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
            item.runtest()
        except Exception as e:
            # Store trace info to allow postmortem debugging
            sys.last_type = type(e)
            sys.last_value = e
            assert e.__traceback__ is not None
            # Skip *this* frame
            sys.last_traceback = e.__traceback__.tb_next
>           raise e

.venv/lib/python3.8/site-packages/_pytest/runner.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_gpu_hist>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
>           item.runtest()

.venv/lib/python3.8/site-packages/_pytest/runner.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Function test_gpu_hist>

    def runtest(self) -> None:
        """Execute the underlying test function."""
>       self.ihook.pytest_pyfunc_call(pyfuncitem=self)

.venv/lib/python3.8/site-packages/_pytest/python.py:1627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_pyfunc_call'>, args = ()
kwargs = {'pyfuncitem': <Function test_gpu_hist>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fdf78b19310>
hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_gpu_hist>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_gpu_hist>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_gpu_hist>}, firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fdf52188880>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_gpu_hist>}, firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pyfuncitem = <Function test_gpu_hist>

    @hookimpl(trylast=True)
    def pytest_pyfunc_call(pyfuncitem: "Function") -> Optional[object]:
        testfunction = pyfuncitem.obj
        if is_async_function(testfunction):
            async_warn_and_skip(pyfuncitem.nodeid)
        funcargs = pyfuncitem.funcargs
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
>       result = testfunction(**testargs)

.venv/lib/python3.8/site-packages/_pytest/python.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf52188700>

    @given(parameter_strategy, strategies.integers(1, 20),
>          tm.dataset_strategy)
    @settings(deadline=None)

tests/python-gpu/test_gpu_updaters.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arguments = ()
kwargs = {'self': <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf52188700>}
__tracebackhide__ = True
test = <function TestGPUUpdaters.test_gpu_hist at 0x7fdf5222f940>
settings = settings(database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples'), deadline=None, derand...nt_blob=False, report_multiple_bugs=True, stateful_step_count=50, suppress_health_check=(), verbosity=Verbosity.normal)
random = <random.Random object at 0x2e61630>
processed_args = ((), {'self': <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf52188700>}, <function default_new_style_executor at 0...own>)}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_updaters.TestGPUUpdaters object at 0x7fdf52188700>))
test_runner = <function default_new_style_executor at 0x7fdf77a913a0>
search_strategy = WithRunner(TupleStrategy((just(()), fixed_dictionaries({'dataset': _dataset_weight_margin(),
 'num_rounds': integers(m...nown>)}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_updaters.TestGPUUpdaters object at 0x7fdf52188700>)
runner = <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf52188700>
state = <hypothesis.core.StateForActualGivenExecution object at 0x7fdf502ef4f0>

    @impersonate(test)
    @define_function_signature(test.__name__, test.__doc__, argspec)
    def wrapped_test(*arguments, **kwargs):
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
    
        test = wrapped_test.hypothesis.inner_test
    
        if getattr(test, "is_hypothesis_test", False):
            raise InvalidArgument(
                (
                    "You have applied @given to the test %s more than once, which "
                    "wraps the test several times and is extremely slow. A "
                    "similar effect can be gained by combining the arguments "
                    "of the two calls to given. For example, instead of "
                    "@given(booleans()) @given(integers()), you could write "
                    "@given(booleans(), integers())"
                )
                % (test.__name__,)
            )
    
        settings = wrapped_test._hypothesis_internal_use_settings
    
        random = get_random_for_wrapped_test(test, wrapped_test)
    
        processed_args = process_arguments_to_given(
            wrapped_test, arguments, kwargs, given_kwargs, argspec
        )
        arguments, kwargs, test_runner, search_strategy = processed_args
    
        runner = getattr(search_strategy, "runner", None)
        if isinstance(runner, TestCase) and test.__name__ in dir(TestCase):
            msg = (
                "You have applied @given to the method %s, which is "
                "used by the unittest runner but is not itself a test."
                "  This is not useful in any way." % test.__name__
            )
            fail_health_check(settings, msg, HealthCheck.not_a_test_method)
        if bad_django_TestCase(runner):  # pragma: no cover
            # Covered by the Django tests, but not the pytest coverage task
            raise InvalidArgument(
                "You have applied @given to a method on %s, but this "
                "class does not inherit from the supported versions in "
                "`hypothesis.extra.django`.  Use the Hypothesis variants "
                "to ensure that each example is run in a separate "
                "database transaction." % qualname(type(runner))
            )
    
        state = StateForActualGivenExecution(
            test_runner, search_strategy, test, settings, random, wrapped_test
        )
    
        reproduce_failure = wrapped_test._hypothesis_internal_use_reproduce_failure
    
        # If there was a @reproduce_failure decorator, use it to reproduce
        # the error (or complain that we couldn't). Either way, this will
        # always raise some kind of error.
        if reproduce_failure is not None:
            expected_version, failure = reproduce_failure
            if expected_version != __version__:
                raise InvalidArgument(
                    (
                        "Attempting to reproduce a failure from a different "
                        "version of Hypothesis. This failure is from %s, but "
                        "you are currently running %r. Please change your "
                        "Hypothesis version to a matching one."
                    )
                    % (expected_version, __version__)
                )
            try:
                state.execute_once(
                    ConjectureData.for_buffer(decode_failure(failure)),
                    print_example=True,
                    is_final=True,
                )
                raise DidNotReproduce(
                    "Expected the test to raise an error, but it "
                    "completed successfully."
                )
            except StopTest:
                raise DidNotReproduce(
                    "The shape of the test data has changed in some way "
                    "from where this blob was defined. Are you sure "
                    "you're running the same test?"
                )
            except UnsatisfiedAssumption:
                raise DidNotReproduce(
                    "The test data failed to satisfy an assumption in the "
                    "test. Have you added it since this blob was "
                    "generated?"
                )
    
        # There was no @reproduce_failure, so start by running any explicit
        # examples from @example decorators.
        errors = list(
            execute_explicit_examples(state, wrapped_test, arguments, kwargs)
        )
        with local_settings(state.settings):
            if len(errors) > 1:
                # If we're not going to report multiple bugs, we would have
                # stopped running explicit examples at the first failure.
                assert state.settings.report_multiple_bugs
                for fragments, err in errors:
                    for f in fragments:
                        report(f)
                    tb_lines = traceback.format_exception(
                        type(err), err, err.__traceback__
                    )
                    report("".join(tb_lines))
                msg = "Hypothesis found %d failures in explicit examples."
                raise MultipleFailures(msg % (len(errors)))
            elif errors:
                fragments, the_error_hypothesis_found = errors[0]
                for f in fragments:
                    report(f)
                raise the_error_hypothesis_found
    
        # If there were any explicit examples, they all ran successfully.
        # The next step is to use the Conjecture engine to run the test on
        # many different inputs.
    
        if not (
            Phase.reuse in settings.phases or Phase.generate in settings.phases
        ):
            return
    
        try:
            if isinstance(runner, TestCase) and hasattr(runner, "subTest"):
                subTest = runner.subTest
                try:
                    runner.subTest = types.MethodType(fake_subTest, runner)
                    state.run_engine()
                finally:
                    runner.subTest = subTest
            else:
                state.run_engine()
        except BaseException as e:
            # The exception caught here should either be an actual test
            # failure (or MultipleFailures), or some kind of fatal error
            # that caused the engine to stop.
    
            generated_seed = wrapped_test._hypothesis_internal_use_generated_seed
            with local_settings(settings):
                if not (state.failed_normally or generated_seed is None):
                    if running_under_pytest:
                        report(
                            "You can add @seed(%(seed)d) to this test or "
                            "run pytest with --hypothesis-seed=%(seed)d "
                            "to reproduce this failure." % {"seed": generated_seed}
                        )
                    else:
                        report(
                            "You can add @seed(%d) to this test to "
                            "reproduce this failure." % (generated_seed,)
                        )
                # The dance here is to avoid showing users long tracebacks
                # full of Hypothesis internals they don't care about.
                # We have to do this inline, to avoid adding another
                # internal stack frame just when we've removed the rest.
                #
                # Using a variable for our trimmed error ensures that the line
                # which will actually appear in tracebacks is as clear as
                # possible - "raise the_error_hypothesis_found".
                the_error_hypothesis_found = e.with_traceback(
                    get_trimmed_traceback()
                )
>               raise the_error_hypothesis_found

venv/lib/python3.8/site-packages/hypothesis/core.py:1162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf52188700>
param = {'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'grow_policy': 'lossguide', ...}
num_rounds = 1, dataset = boston

    @given(parameter_strategy, strategies.integers(1, 20),
           tm.dataset_strategy)
    @settings(deadline=None)
    def test_gpu_hist(self, param, num_rounds, dataset):
        param['tree_method'] = 'gpu_hist'
        param = dataset.set_params(param)
>       result = train_result(param, dataset.get_dmat(), num_rounds)

tests/python-gpu/test_gpu_updaters.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

param = {'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'grow_policy': 'lossguide', ...}
dmat = <xgboost.core.DMatrix object at 0x7fdf507c9ac0>, num_rounds = 1

    def train_result(param, dmat, num_rounds):
        result = {}
>       xgb.train(param, dmat, num_rounds, [(dmat, 'train')], verbose_eval=False,
                  evals_result=result)

tests/python-gpu/test_gpu_updaters.py:28: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'grow_policy': 'lossguide', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf507c9ac0>, num_boost_round = 1
evals = [(<xgboost.core.DMatrix object at 0x7fdf507c9ac0>, 'train')], obj = None
feval = None, maximize = None, early_stopping_rounds = None, evals_result = {}
verbose_eval = False, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'grow_policy': 'lossguide', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf507c9ac0>, num_boost_round = 1
evals = [(<xgboost.core.DMatrix object at 0x7fdf507c9ac0>, 'train')], obj = None
feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf5057feb0>
evals_result = {}, maximize = None, verbose_eval = False
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf507c9910>
dtrain = <xgboost.core.DMatrix object at 0x7fdf507c9ac0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:16] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
---------------------------------- Hypothesis ----------------------------------
Falsifying example: test_gpu_hist(
    self=<test_gpu_updaters.TestGPUUpdaters at 0x7fdf52188700>,
    param={'colsample_bylevel': 0.5,
     'colsample_bytree': 0.5,
     'grow_policy': 'lossguide',
     'max_bin': 2,
     'max_depth': 0,
     'max_leaves': 1,
     'min_child_weight': 0.5,
     'seed': 0,
     'single_precision_histogram': False},
    num_rounds=1,
    dataset=boston,
)
___________________ TestGPUUpdaters.test_categorical_32_cat ____________________

cls = <class '_pytest.runner.CallInfo'>
func = <function call_runtest_hook.<locals>.<lambda> at 0x7fdf5078bdc0>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: "Optional[Union[Type[BaseException], Tuple[Type[BaseException], ...]]]" = None,
    ) -> "CallInfo[TResult]":
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result = func()  # type: Optional[TResult]

.venv/lib/python3.8/site-packages/_pytest/runner.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: ihook(item=item, **kwds), when=when, reraise=reraise
    )

.venv/lib/python3.8/site-packages/_pytest/runner.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_runtest_call'>, args = ()
kwargs = {'item': <Function test_categorical_32_cat>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fdf78b19310>
hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
kwargs = {'item': <Function test_categorical_32_cat>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
kwargs = {'item': <Function test_categorical_32_cat>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
caller_kwargs = {'item': <Function test_categorical_32_cat>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fdf663a4dc0>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
caller_kwargs = {'item': <Function test_categorical_32_cat>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_categorical_32_cat>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
            item.runtest()
        except Exception as e:
            # Store trace info to allow postmortem debugging
            sys.last_type = type(e)
            sys.last_value = e
            assert e.__traceback__ is not None
            # Skip *this* frame
            sys.last_traceback = e.__traceback__.tb_next
>           raise e

.venv/lib/python3.8/site-packages/_pytest/runner.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_categorical_32_cat>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
>           item.runtest()

.venv/lib/python3.8/site-packages/_pytest/runner.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Function test_categorical_32_cat>

    def runtest(self) -> None:
        """Execute the underlying test function."""
>       self.ihook.pytest_pyfunc_call(pyfuncitem=self)

.venv/lib/python3.8/site-packages/_pytest/python.py:1627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_pyfunc_call'>, args = ()
kwargs = {'pyfuncitem': <Function test_categorical_32_cat>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fdf78b19310>
hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_categorical_32_cat>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_categorical_32_cat>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_categorical_32_cat>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fdf663a4e20>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_categorical_32_cat>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pyfuncitem = <Function test_categorical_32_cat>

    @hookimpl(trylast=True)
    def pytest_pyfunc_call(pyfuncitem: "Function") -> Optional[object]:
        testfunction = pyfuncitem.obj
        if is_async_function(testfunction):
            async_warn_and_skip(pyfuncitem.nodeid)
        funcargs = pyfuncitem.funcargs
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
>       result = testfunction(**testargs)

.venv/lib/python3.8/site-packages/_pytest/python.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf663a4d60>

    def test_categorical_32_cat(self):
        '''32 hits the bound of integer bitset, so special test'''
        rows = 1000
        cols = 10
        cats = 32
        rounds = 4
>       self.run_categorical_basic(rows, cols, rounds, cats)

tests/python-gpu/test_gpu_updaters.py:97: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf663a4d60>, rows = 1000
cols = 10, rounds = 4, cats = 32

    def run_categorical_basic(self, rows, cols, rounds, cats):
>       import pandas as pd
E       ModuleNotFoundError: No module named 'pandas'

tests/python-gpu/test_gpu_updaters.py:45: ModuleNotFoundError
_____________________ TestGPUUpdaters.test_external_memory _____________________

cls = <class '_pytest.runner.CallInfo'>
func = <function call_runtest_hook.<locals>.<lambda> at 0x7fdf5078b1f0>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: "Optional[Union[Type[BaseException], Tuple[Type[BaseException], ...]]]" = None,
    ) -> "CallInfo[TResult]":
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result = func()  # type: Optional[TResult]

.venv/lib/python3.8/site-packages/_pytest/runner.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: ihook(item=item, **kwds), when=when, reraise=reraise
    )

.venv/lib/python3.8/site-packages/_pytest/runner.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_runtest_call'>, args = ()
kwargs = {'item': <Function test_external_memory>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fdf78b19310>
hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
kwargs = {'item': <Function test_external_memory>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
kwargs = {'item': <Function test_external_memory>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
caller_kwargs = {'item': <Function test_external_memory>}, firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fdf5051b100>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
caller_kwargs = {'item': <Function test_external_memory>}, firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_external_memory>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
            item.runtest()
        except Exception as e:
            # Store trace info to allow postmortem debugging
            sys.last_type = type(e)
            sys.last_value = e
            assert e.__traceback__ is not None
            # Skip *this* frame
            sys.last_traceback = e.__traceback__.tb_next
>           raise e

.venv/lib/python3.8/site-packages/_pytest/runner.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_external_memory>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
>           item.runtest()

.venv/lib/python3.8/site-packages/_pytest/runner.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Function test_external_memory>

    def runtest(self) -> None:
        """Execute the underlying test function."""
>       self.ihook.pytest_pyfunc_call(pyfuncitem=self)

.venv/lib/python3.8/site-packages/_pytest/python.py:1627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_pyfunc_call'>, args = ()
kwargs = {'pyfuncitem': <Function test_external_memory>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fdf78b19310>
hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_external_memory>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_external_memory>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_external_memory>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fdf5051b940>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_external_memory>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pyfuncitem = <Function test_external_memory>

    @hookimpl(trylast=True)
    def pytest_pyfunc_call(pyfuncitem: "Function") -> Optional[object]:
        testfunction = pyfuncitem.obj
        if is_async_function(testfunction):
            async_warn_and_skip(pyfuncitem.nodeid)
        funcargs = pyfuncitem.funcargs
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
>       result = testfunction(**testargs)

.venv/lib/python3.8/site-packages/_pytest/python.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf5051b790>

>   ???

tests/python-gpu/test_gpu_updaters.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arguments = ()
kwargs = {'self': <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf5051b790>}
__tracebackhide__ = True
test = <function TestGPUUpdaters.test_external_memory at 0x7fdf522403a0>
settings = settings(database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples'), deadline=None, derand...nt_blob=False, report_multiple_bugs=True, stateful_step_count=50, suppress_health_check=(), verbosity=Verbosity.normal)
random = <random.Random object at 0x2e173d0>
processed_args = ((), {'self': <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf5051b790>}, <function default_new_style_executor at 0...own>)}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_updaters.TestGPUUpdaters object at 0x7fdf5051b790>))
test_runner = <function default_new_style_executor at 0x7fdf77a913a0>
search_strategy = WithRunner(TupleStrategy((just(()), fixed_dictionaries({'dataset': _dataset_weight_margin(),
 'num_rounds': integers(m...nown>)}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_updaters.TestGPUUpdaters object at 0x7fdf5051b790>)
runner = <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf5051b790>
state = <hypothesis.core.StateForActualGivenExecution object at 0x7fdf50706b80>

    @impersonate(test)
    @define_function_signature(test.__name__, test.__doc__, argspec)
    def wrapped_test(*arguments, **kwargs):
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
    
        test = wrapped_test.hypothesis.inner_test
    
        if getattr(test, "is_hypothesis_test", False):
            raise InvalidArgument(
                (
                    "You have applied @given to the test %s more than once, which "
                    "wraps the test several times and is extremely slow. A "
                    "similar effect can be gained by combining the arguments "
                    "of the two calls to given. For example, instead of "
                    "@given(booleans()) @given(integers()), you could write "
                    "@given(booleans(), integers())"
                )
                % (test.__name__,)
            )
    
        settings = wrapped_test._hypothesis_internal_use_settings
    
        random = get_random_for_wrapped_test(test, wrapped_test)
    
        processed_args = process_arguments_to_given(
            wrapped_test, arguments, kwargs, given_kwargs, argspec
        )
        arguments, kwargs, test_runner, search_strategy = processed_args
    
        runner = getattr(search_strategy, "runner", None)
        if isinstance(runner, TestCase) and test.__name__ in dir(TestCase):
            msg = (
                "You have applied @given to the method %s, which is "
                "used by the unittest runner but is not itself a test."
                "  This is not useful in any way." % test.__name__
            )
            fail_health_check(settings, msg, HealthCheck.not_a_test_method)
        if bad_django_TestCase(runner):  # pragma: no cover
            # Covered by the Django tests, but not the pytest coverage task
            raise InvalidArgument(
                "You have applied @given to a method on %s, but this "
                "class does not inherit from the supported versions in "
                "`hypothesis.extra.django`.  Use the Hypothesis variants "
                "to ensure that each example is run in a separate "
                "database transaction." % qualname(type(runner))
            )
    
        state = StateForActualGivenExecution(
            test_runner, search_strategy, test, settings, random, wrapped_test
        )
    
        reproduce_failure = wrapped_test._hypothesis_internal_use_reproduce_failure
    
        # If there was a @reproduce_failure decorator, use it to reproduce
        # the error (or complain that we couldn't). Either way, this will
        # always raise some kind of error.
        if reproduce_failure is not None:
            expected_version, failure = reproduce_failure
            if expected_version != __version__:
                raise InvalidArgument(
                    (
                        "Attempting to reproduce a failure from a different "
                        "version of Hypothesis. This failure is from %s, but "
                        "you are currently running %r. Please change your "
                        "Hypothesis version to a matching one."
                    )
                    % (expected_version, __version__)
                )
            try:
                state.execute_once(
                    ConjectureData.for_buffer(decode_failure(failure)),
                    print_example=True,
                    is_final=True,
                )
                raise DidNotReproduce(
                    "Expected the test to raise an error, but it "
                    "completed successfully."
                )
            except StopTest:
                raise DidNotReproduce(
                    "The shape of the test data has changed in some way "
                    "from where this blob was defined. Are you sure "
                    "you're running the same test?"
                )
            except UnsatisfiedAssumption:
                raise DidNotReproduce(
                    "The test data failed to satisfy an assumption in the "
                    "test. Have you added it since this blob was "
                    "generated?"
                )
    
        # There was no @reproduce_failure, so start by running any explicit
        # examples from @example decorators.
        errors = list(
            execute_explicit_examples(state, wrapped_test, arguments, kwargs)
        )
        with local_settings(state.settings):
            if len(errors) > 1:
                # If we're not going to report multiple bugs, we would have
                # stopped running explicit examples at the first failure.
                assert state.settings.report_multiple_bugs
                for fragments, err in errors:
                    for f in fragments:
                        report(f)
                    tb_lines = traceback.format_exception(
                        type(err), err, err.__traceback__
                    )
                    report("".join(tb_lines))
                msg = "Hypothesis found %d failures in explicit examples."
                raise MultipleFailures(msg % (len(errors)))
            elif errors:
                fragments, the_error_hypothesis_found = errors[0]
                for f in fragments:
                    report(f)
                raise the_error_hypothesis_found
    
        # If there were any explicit examples, they all ran successfully.
        # The next step is to use the Conjecture engine to run the test on
        # many different inputs.
    
        if not (
            Phase.reuse in settings.phases or Phase.generate in settings.phases
        ):
            return
    
        try:
            if isinstance(runner, TestCase) and hasattr(runner, "subTest"):
                subTest = runner.subTest
                try:
                    runner.subTest = types.MethodType(fake_subTest, runner)
                    state.run_engine()
                finally:
                    runner.subTest = subTest
            else:
                state.run_engine()
        except BaseException as e:
            # The exception caught here should either be an actual test
            # failure (or MultipleFailures), or some kind of fatal error
            # that caused the engine to stop.
    
            generated_seed = wrapped_test._hypothesis_internal_use_generated_seed
            with local_settings(settings):
                if not (state.failed_normally or generated_seed is None):
                    if running_under_pytest:
                        report(
                            "You can add @seed(%(seed)d) to this test or "
                            "run pytest with --hypothesis-seed=%(seed)d "
                            "to reproduce this failure." % {"seed": generated_seed}
                        )
                    else:
                        report(
                            "You can add @seed(%d) to this test to "
                            "reproduce this failure." % (generated_seed,)
                        )
                # The dance here is to avoid showing users long tracebacks
                # full of Hypothesis internals they don't care about.
                # We have to do this inline, to avoid adding another
                # internal stack frame just when we've removed the rest.
                #
                # Using a variable for our trimmed error ensures that the line
                # which will actually appear in tracebacks is as clear as
                # possible - "raise the_error_hypothesis_found".
                the_error_hypothesis_found = e.with_traceback(
                    get_trimmed_traceback()
                )
>               raise the_error_hypothesis_found

venv/lib/python3.8/site-packages/hypothesis/core.py:1162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf5051b790>
param = {'colsample_bylevel': 0.5, 'colsample_bytree': 0.5, 'eval_metric': 'rmse', 'grow_policy': 'lossguide', ...}
num_rounds = 1, dataset = boston

    @given(parameter_strategy, strategies.integers(1, 20),
           tm.dataset_strategy)
    @settings(deadline=None)
    def test_external_memory(self, param, num_rounds, dataset):
        # We cannot handle empty dataset yet
        assume(len(dataset.y) > 0)
        param['tree_method'] = 'gpu_hist'
        param = dataset.set_params(param)
>       external_result = train_result(param, dataset.get_external_dmat(), num_rounds)

tests/python-gpu/test_gpu_updaters.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = boston

    def get_external_dmat(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            path = os.path.join(tmpdir, 'tmptmp_1234.csv')
            np.savetxt(path,
                       np.hstack((self.y.reshape(len(self.y), 1), self.X)),
                       delimiter=',')
            assert os.path.exists(path)
            uri = path + '?format=csv&label_column=0#tmptmp_'
            # The uri looks like:
            # 'tmptmp_1234.csv?format=csv&label_column=0#tmptmp_'
>           return xgb.DMatrix(uri, weight=self.w, base_margin=self.margin)

tests/python/testing.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.DMatrix object at 0x7fdf50767a00>
data = '/tmp/tmp7ckbbddi/tmptmp_1234.csv?format=csv&label_column=0#tmptmp_'
label = None
weight = array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
       0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,..., 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
       0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])
base_margin = array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,
       0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,..., 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,
       0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])
missing = None, silent = False, feature_names = None, feature_types = None
nthread = None, enable_categorical = False

    def __init__(self, data, label=None, weight=None, base_margin=None,
                 missing=None,
                 silent=False,
                 feature_names=None,
                 feature_types=None,
                 nthread=None,
                 enable_categorical=False):
        """Parameters
        ----------
        data : os.PathLike/string/numpy.array/scipy.sparse/pd.DataFrame/
               dt.Frame/cudf.DataFrame/cupy.array/dlpack
            Data source of DMatrix.
            When data is string or os.PathLike type, it represents the path
            libsvm format txt file, csv file (by specifying uri parameter
            'path_to_csv?format=csv'), or binary file that xgboost can read
            from.
        label : list, numpy 1-D array or cudf.DataFrame, optional
            Label of the training data.
        missing : float, optional
            Value in the input data which needs to be present as a missing
            value. If None, defaults to np.nan.
        weight : list, numpy 1-D array or cudf.DataFrame , optional
            Weight for each instance.
    
            .. note:: For ranking task, weights are per-group.
    
                In ranking task, one weight is assigned to each group (not each
                data point). This is because we only care about the relative
                ordering of data points within each group, so it doesn't make
                sense to assign weights to individual data points.
    
        silent : boolean, optional
            Whether print messages during construction
        feature_names : list, optional
            Set names for features.
        feature_types : list, optional
            Set types for features.
        nthread : integer, optional
            Number of threads to use for loading data when parallelization is
            applicable. If -1, uses maximum threads available on the system.
    
        enable_categorical: boolean, optional
    
            .. versionadded:: 1.3.0
    
            Experimental support of specializing for categorical features.  Do
            not set to True unless you are interested in development.
            Currently it's only available for `gpu_hist` tree method with 1 vs
            rest (one hot) categorical split.  Also, JSON serialization format,
            `gpu_predictor` and pandas input are required.
    
        """
        if isinstance(data, list):
            raise TypeError('Input data can not be a list.')
    
        self.missing = missing if missing is not None else np.nan
        self.nthread = nthread if nthread is not None else -1
        self.silent = silent
    
        # force into void_p, mac need to pass things in as void_p
        if data is None:
            self.handle = None
            return
    
        from .data import dispatch_data_backend
>       handle, feature_names, feature_types = dispatch_data_backend(
            data, missing=self.missing,
            threads=self.nthread,
            feature_names=feature_names,
            feature_types=feature_types,
            enable_categorical=enable_categorical)

venv/lib/python3.8/site-packages/xgboost/core.py:445: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = '/tmp/tmp7ckbbddi/tmptmp_1234.csv?format=csv&label_column=0#tmptmp_'
missing = nan, threads = -1, feature_names = None, feature_types = None
enable_categorical = False

    def dispatch_data_backend(data, missing, threads,
                              feature_names, feature_types,
                              enable_categorical=False):
        '''Dispatch data for DMatrix.'''
        if _is_scipy_csr(data):
            return _from_scipy_csr(data, missing, feature_names, feature_types)
        if _is_scipy_csc(data):
            return _from_scipy_csc(data, missing, feature_names, feature_types)
        if _is_scipy_coo(data):
            return _from_scipy_csr(data.tocsr(), missing, feature_names, feature_types)
        if _is_numpy_array(data):
            return _from_numpy_array(data, missing, threads, feature_names,
                                     feature_types)
        if _is_uri(data):
>           return _from_uri(data, missing, feature_names, feature_types)

venv/lib/python3.8/site-packages/xgboost/data.py:532: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = '/tmp/tmp7ckbbddi/tmptmp_1234.csv?format=csv&label_column=0#tmptmp_'
missing = nan, feature_names = None, feature_types = None

    def _from_uri(data, missing, feature_names, feature_types):
        _warn_unused_missing(data, missing)
        handle = ctypes.c_void_p()
>       _check_call(_LIB.XGDMatrixCreateFromFile(c_str(os.fspath(data)),
                                                 ctypes.c_int(1),
                                                 ctypes.byref(handle)))

venv/lib/python3.8/site-packages/xgboost/data.py:488: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:16] /home/otivedani/xgboost/src/data/data.cc:706: Encountered parser error:
E           [22:19:16] /home/otivedani/xgboost/src/data/./sparse_page_source.h:124: Cache file tmptmp_ exists already; Is there another DMatrix with the same cache prefix? Otherwise please remove it manually.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::data::CheckCacheFileExists(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)+0x19c) [0x7fdf55fd31ec]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::data::SparsePageSource::SparsePageSource<xgboost::data::FileAdapter>(xgboost::data::FileAdapter*, float, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long)+0x1cf) [0x7fdf55ff2eef]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::DMatrix* xgboost::DMatrix::Create<xgboost::data::FileAdapter>(xgboost::data::FileAdapter*, float, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long)+0x10d) [0x7fdf55ff755d]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::DMatrix::Load(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool, bool, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long)+0x309) [0x7fdf55fc3c79]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGDMatrixCreateFromFile+0x129) [0x7fdf55f33e39]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]
E           
E           
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0xd3776) [0x7fdf55f22776]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGDMatrixCreateFromFile+0x129) [0x7fdf55f33e39]
E             [bt] (3) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (4) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (5) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]
E             [bt] (6) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x137c0) [0x7fdf779737c0]
E             [bt] (7) /home/otivedani/xgboost/.venv/bin/python3(_PyObject_MakeTpCall+0x296) [0x5f46d6]
E             [bt] (8) /home/otivedani/xgboost/.venv/bin/python3(_PyEval_EvalFrameDefault+0x5de6) [0x570936]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
---------------------------------- Hypothesis ----------------------------------
Falsifying example: test_external_memory(
    self=<test_gpu_updaters.TestGPUUpdaters at 0x7fdf5051b790>,
    param={'colsample_bylevel': 0.5,
     'colsample_bytree': 0.5,
     'grow_policy': 'lossguide',
     'max_bin': 2,
     'max_depth': 0,
     'max_leaves': 1,
     'min_child_weight': 0.5,
     'seed': 0,
     'single_precision_histogram': False},
    num_rounds=1,
    dataset=boston,
)
________________ TestGPUUpdaters.test_empty_dmatrix_prediction _________________

cls = <class '_pytest.runner.CallInfo'>
func = <function call_runtest_hook.<locals>.<lambda> at 0x7fdf50607b80>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: "Optional[Union[Type[BaseException], Tuple[Type[BaseException], ...]]]" = None,
    ) -> "CallInfo[TResult]":
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result = func()  # type: Optional[TResult]

.venv/lib/python3.8/site-packages/_pytest/runner.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: ihook(item=item, **kwds), when=when, reraise=reraise
    )

.venv/lib/python3.8/site-packages/_pytest/runner.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_runtest_call'>, args = ()
kwargs = {'item': <Function test_empty_dmatrix_prediction>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fdf78b19310>
hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
kwargs = {'item': <Function test_empty_dmatrix_prediction>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
kwargs = {'item': <Function test_empty_dmatrix_prediction>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
caller_kwargs = {'item': <Function test_empty_dmatrix_prediction>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fdf659c31c0>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
caller_kwargs = {'item': <Function test_empty_dmatrix_prediction>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_empty_dmatrix_prediction>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
            item.runtest()
        except Exception as e:
            # Store trace info to allow postmortem debugging
            sys.last_type = type(e)
            sys.last_value = e
            assert e.__traceback__ is not None
            # Skip *this* frame
            sys.last_traceback = e.__traceback__.tb_next
>           raise e

.venv/lib/python3.8/site-packages/_pytest/runner.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_empty_dmatrix_prediction>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
>           item.runtest()

.venv/lib/python3.8/site-packages/_pytest/runner.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Function test_empty_dmatrix_prediction>

    def runtest(self) -> None:
        """Execute the underlying test function."""
>       self.ihook.pytest_pyfunc_call(pyfuncitem=self)

.venv/lib/python3.8/site-packages/_pytest/python.py:1627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_pyfunc_call'>, args = ()
kwargs = {'pyfuncitem': <Function test_empty_dmatrix_prediction>}
notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fdf78b19310>
hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_empty_dmatrix_prediction>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_empty_dmatrix_prediction>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_empty_dmatrix_prediction>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fdf659c3130>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_empty_dmatrix_prediction>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pyfuncitem = <Function test_empty_dmatrix_prediction>

    @hookimpl(trylast=True)
    def pytest_pyfunc_call(pyfuncitem: "Function") -> Optional[object]:
        testfunction = pyfuncitem.obj
        if is_async_function(testfunction):
            async_warn_and_skip(pyfuncitem.nodeid)
        funcargs = pyfuncitem.funcargs
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
>       result = testfunction(**testargs)

.venv/lib/python3.8/site-packages/_pytest/python.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf659c3160>

    def test_empty_dmatrix_prediction(self):
        # FIXME(trivialfis): This should be done with all updaters
        kRows = 0
        kCols = 100
    
        X = np.empty((kRows, kCols))
        y = np.empty((kRows))
    
        dtrain = xgb.DMatrix(X, y)
    
>       bst = xgb.train({'verbosity': 2,
                         'tree_method': 'gpu_hist',
                         'gpu_id': 0},
                        dtrain,
                        verbose_eval=True,
                        num_boost_round=6,
                        evals=[(dtrain, 'Train')])

tests/python-gpu/test_gpu_updaters.py:133: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'gpu_id': 0, 'tree_method': 'gpu_hist', 'verbosity': 2}
dtrain = <xgboost.core.DMatrix object at 0x7fdf659c3370>, num_boost_round = 6
evals = [(<xgboost.core.DMatrix object at 0x7fdf659c3370>, 'Train')], obj = None
feval = None, maximize = None, early_stopping_rounds = None, evals_result = None
verbose_eval = True, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'gpu_id': 0, 'tree_method': 'gpu_hist', 'validate_parameters': True, 'verbosity': 2}
dtrain = <xgboost.core.DMatrix object at 0x7fdf659c3370>, num_boost_round = 6
evals = [(<xgboost.core.DMatrix object at 0x7fdf659c3370>, 'Train')], obj = None
feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf659c3430>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf659c33d0>
dtrain = <xgboost.core.DMatrix object at 0x7fdf659c3370>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:16] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
_______________ TestGPUUpdaters.test_specified_gpu_id_gpu_update _______________

cls = <class '_pytest.runner.CallInfo'>
func = <function call_runtest_hook.<locals>.<lambda> at 0x7fdf507b8310>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: "Optional[Union[Type[BaseException], Tuple[Type[BaseException], ...]]]" = None,
    ) -> "CallInfo[TResult]":
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result = func()  # type: Optional[TResult]

.venv/lib/python3.8/site-packages/_pytest/runner.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: ihook(item=item, **kwds), when=when, reraise=reraise
    )

.venv/lib/python3.8/site-packages/_pytest/runner.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_runtest_call'>, args = ()
kwargs = {'item': <Function test_specified_gpu_id_gpu_update>}
notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fdf78b19310>
hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
kwargs = {'item': <Function test_specified_gpu_id_gpu_update>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
kwargs = {'item': <Function test_specified_gpu_id_gpu_update>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
caller_kwargs = {'item': <Function test_specified_gpu_id_gpu_update>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fdf508ebb20>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
caller_kwargs = {'item': <Function test_specified_gpu_id_gpu_update>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_specified_gpu_id_gpu_update>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
            item.runtest()
        except Exception as e:
            # Store trace info to allow postmortem debugging
            sys.last_type = type(e)
            sys.last_value = e
            assert e.__traceback__ is not None
            # Skip *this* frame
            sys.last_traceback = e.__traceback__.tb_next
>           raise e

.venv/lib/python3.8/site-packages/_pytest/runner.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_specified_gpu_id_gpu_update>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
>           item.runtest()

.venv/lib/python3.8/site-packages/_pytest/runner.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Function test_specified_gpu_id_gpu_update>

    def runtest(self) -> None:
        """Execute the underlying test function."""
>       self.ihook.pytest_pyfunc_call(pyfuncitem=self)

.venv/lib/python3.8/site-packages/_pytest/python.py:1627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_pyfunc_call'>, args = ()
kwargs = {'pyfuncitem': <Function test_specified_gpu_id_gpu_update>}
notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fdf78b19310>
hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_specified_gpu_id_gpu_update>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_specified_gpu_id_gpu_update>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_specified_gpu_id_gpu_update>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fdf508eb130>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_specified_gpu_id_gpu_update>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pyfuncitem = <Function test_specified_gpu_id_gpu_update>

    @hookimpl(trylast=True)
    def pytest_pyfunc_call(pyfuncitem: "Function") -> Optional[object]:
        testfunction = pyfuncitem.obj
        if is_async_function(testfunction):
            async_warn_and_skip(pyfuncitem.nodeid)
        funcargs = pyfuncitem.funcargs
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
>       result = testfunction(**testargs)

.venv/lib/python3.8/site-packages/_pytest/python.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf508ebe20>

>   ???

tests/python-gpu/test_gpu_updaters.py:149: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arguments = ()
kwargs = {'self': <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf508ebe20>}
__tracebackhide__ = True
test = <function TestGPUUpdaters.test_specified_gpu_id_gpu_update at 0x7fdf522408b0>
settings = settings(database=DirectoryBasedExampleDatabase('/home/otivedani/xgboost/.hypothesis/examples'), deadline=None, derand...nt_blob=False, report_multiple_bugs=True, stateful_step_count=50, suppress_health_check=(), verbosity=Verbosity.normal)
random = <random.Random object at 0x2e20c60>
processed_args = ((), {'self': <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf508ebe20>}, <function default_new_style_executor at 0...e=10)}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_updaters.TestGPUUpdaters object at 0x7fdf508ebe20>))
test_runner = <function default_new_style_executor at 0x7fdf77a913a0>
search_strategy = WithRunner(TupleStrategy((just(()), fixed_dictionaries({'dataset': _dataset_weight_margin(),
 'gpu_id': integers(min_v...ue=10)}).map(lambda args: dict(args, **kwargs)))), runner=<test_gpu_updaters.TestGPUUpdaters object at 0x7fdf508ebe20>)
runner = <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf508ebe20>
state = <hypothesis.core.StateForActualGivenExecution object at 0x7fdf506283a0>

    @impersonate(test)
    @define_function_signature(test.__name__, test.__doc__, argspec)
    def wrapped_test(*arguments, **kwargs):
        # Tell pytest to omit the body of this function from tracebacks
        __tracebackhide__ = True
    
        test = wrapped_test.hypothesis.inner_test
    
        if getattr(test, "is_hypothesis_test", False):
            raise InvalidArgument(
                (
                    "You have applied @given to the test %s more than once, which "
                    "wraps the test several times and is extremely slow. A "
                    "similar effect can be gained by combining the arguments "
                    "of the two calls to given. For example, instead of "
                    "@given(booleans()) @given(integers()), you could write "
                    "@given(booleans(), integers())"
                )
                % (test.__name__,)
            )
    
        settings = wrapped_test._hypothesis_internal_use_settings
    
        random = get_random_for_wrapped_test(test, wrapped_test)
    
        processed_args = process_arguments_to_given(
            wrapped_test, arguments, kwargs, given_kwargs, argspec
        )
        arguments, kwargs, test_runner, search_strategy = processed_args
    
        runner = getattr(search_strategy, "runner", None)
        if isinstance(runner, TestCase) and test.__name__ in dir(TestCase):
            msg = (
                "You have applied @given to the method %s, which is "
                "used by the unittest runner but is not itself a test."
                "  This is not useful in any way." % test.__name__
            )
            fail_health_check(settings, msg, HealthCheck.not_a_test_method)
        if bad_django_TestCase(runner):  # pragma: no cover
            # Covered by the Django tests, but not the pytest coverage task
            raise InvalidArgument(
                "You have applied @given to a method on %s, but this "
                "class does not inherit from the supported versions in "
                "`hypothesis.extra.django`.  Use the Hypothesis variants "
                "to ensure that each example is run in a separate "
                "database transaction." % qualname(type(runner))
            )
    
        state = StateForActualGivenExecution(
            test_runner, search_strategy, test, settings, random, wrapped_test
        )
    
        reproduce_failure = wrapped_test._hypothesis_internal_use_reproduce_failure
    
        # If there was a @reproduce_failure decorator, use it to reproduce
        # the error (or complain that we couldn't). Either way, this will
        # always raise some kind of error.
        if reproduce_failure is not None:
            expected_version, failure = reproduce_failure
            if expected_version != __version__:
                raise InvalidArgument(
                    (
                        "Attempting to reproduce a failure from a different "
                        "version of Hypothesis. This failure is from %s, but "
                        "you are currently running %r. Please change your "
                        "Hypothesis version to a matching one."
                    )
                    % (expected_version, __version__)
                )
            try:
                state.execute_once(
                    ConjectureData.for_buffer(decode_failure(failure)),
                    print_example=True,
                    is_final=True,
                )
                raise DidNotReproduce(
                    "Expected the test to raise an error, but it "
                    "completed successfully."
                )
            except StopTest:
                raise DidNotReproduce(
                    "The shape of the test data has changed in some way "
                    "from where this blob was defined. Are you sure "
                    "you're running the same test?"
                )
            except UnsatisfiedAssumption:
                raise DidNotReproduce(
                    "The test data failed to satisfy an assumption in the "
                    "test. Have you added it since this blob was "
                    "generated?"
                )
    
        # There was no @reproduce_failure, so start by running any explicit
        # examples from @example decorators.
        errors = list(
            execute_explicit_examples(state, wrapped_test, arguments, kwargs)
        )
        with local_settings(state.settings):
            if len(errors) > 1:
                # If we're not going to report multiple bugs, we would have
                # stopped running explicit examples at the first failure.
                assert state.settings.report_multiple_bugs
                for fragments, err in errors:
                    for f in fragments:
                        report(f)
                    tb_lines = traceback.format_exception(
                        type(err), err, err.__traceback__
                    )
                    report("".join(tb_lines))
                msg = "Hypothesis found %d failures in explicit examples."
                raise MultipleFailures(msg % (len(errors)))
            elif errors:
                fragments, the_error_hypothesis_found = errors[0]
                for f in fragments:
                    report(f)
                raise the_error_hypothesis_found
    
        # If there were any explicit examples, they all ran successfully.
        # The next step is to use the Conjecture engine to run the test on
        # many different inputs.
    
        if not (
            Phase.reuse in settings.phases or Phase.generate in settings.phases
        ):
            return
    
        try:
            if isinstance(runner, TestCase) and hasattr(runner, "subTest"):
                subTest = runner.subTest
                try:
                    runner.subTest = types.MethodType(fake_subTest, runner)
                    state.run_engine()
                finally:
                    runner.subTest = subTest
            else:
                state.run_engine()
        except BaseException as e:
            # The exception caught here should either be an actual test
            # failure (or MultipleFailures), or some kind of fatal error
            # that caused the engine to stop.
    
            generated_seed = wrapped_test._hypothesis_internal_use_generated_seed
            with local_settings(settings):
                if not (state.failed_normally or generated_seed is None):
                    if running_under_pytest:
                        report(
                            "You can add @seed(%(seed)d) to this test or "
                            "run pytest with --hypothesis-seed=%(seed)d "
                            "to reproduce this failure." % {"seed": generated_seed}
                        )
                    else:
                        report(
                            "You can add @seed(%d) to this test to "
                            "reproduce this failure." % (generated_seed,)
                        )
                # The dance here is to avoid showing users long tracebacks
                # full of Hypothesis internals they don't care about.
                # We have to do this inline, to avoid adding another
                # internal stack frame just when we've removed the rest.
                #
                # Using a variable for our trimmed error ensures that the line
                # which will actually appear in tracebacks is as clear as
                # possible - "raise the_error_hypothesis_found".
                the_error_hypothesis_found = e.with_traceback(
                    get_trimmed_traceback()
                )
>               raise the_error_hypothesis_found

venv/lib/python3.8/site-packages/hypothesis/core.py:1162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_updaters.TestGPUUpdaters object at 0x7fdf508ebe20>
dataset = boston, gpu_id = 0

    @pytest.mark.mgpu
    @given(tm.dataset_strategy, strategies.integers(0, 10))
    @settings(deadline=None, max_examples=10)
    def test_specified_gpu_id_gpu_update(self, dataset, gpu_id):
        param = {'tree_method': 'gpu_hist', 'gpu_id': gpu_id}
        param = dataset.set_params(param)
>       result = train_result(param, dataset.get_dmat(), 10)

tests/python-gpu/test_gpu_updaters.py:154: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

param = {'eval_metric': 'rmse', 'gpu_id': 0, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist'}
dmat = <xgboost.core.DMatrix object at 0x7fdf663c6070>, num_rounds = 10

    def train_result(param, dmat, num_rounds):
        result = {}
>       xgb.train(param, dmat, num_rounds, [(dmat, 'train')], verbose_eval=False,
                  evals_result=result)

tests/python-gpu/test_gpu_updaters.py:28: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eval_metric': 'rmse', 'gpu_id': 0, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist'}
dtrain = <xgboost.core.DMatrix object at 0x7fdf663c6070>, num_boost_round = 10
evals = [(<xgboost.core.DMatrix object at 0x7fdf663c6070>, 'train')], obj = None
feval = None, maximize = None, early_stopping_rounds = None, evals_result = {}
verbose_eval = False, xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'eval_metric': 'rmse', 'gpu_id': 0, 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist', ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf663c6070>, num_boost_round = 10
evals = [(<xgboost.core.DMatrix object at 0x7fdf663c6070>, 'train')], obj = None
feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf663c6ca0>
evals_result = {}, maximize = None, verbose_eval = False
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf663c6040>
dtrain = <xgboost.core.DMatrix object at 0x7fdf663c6070>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:17] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
---------------------------------- Hypothesis ----------------------------------
Falsifying example: test_specified_gpu_id_gpu_update(
    self=<test_gpu_updaters.TestGPUUpdaters at 0x7fdf508ebe20>,
    dataset=boston,
    gpu_id=0,
)
________________________ test_gpu_binary_classification ________________________

cls = <class '_pytest.runner.CallInfo'>
func = <function call_runtest_hook.<locals>.<lambda> at 0x7fdf5043b820>
when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: "Callable[[], TResult]",
        when: "Literal['collect', 'setup', 'call', 'teardown']",
        reraise: "Optional[Union[Type[BaseException], Tuple[Type[BaseException], ...]]]" = None,
    ) -> "CallInfo[TResult]":
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result = func()  # type: Optional[TResult]

.venv/lib/python3.8/site-packages/_pytest/runner.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>       lambda: ihook(item=item, **kwds), when=when, reraise=reraise
    )

.venv/lib/python3.8/site-packages/_pytest/runner.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_runtest_call'>, args = ()
kwargs = {'item': <Function test_gpu_binary_classification>}, notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fdf78b19310>
hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
kwargs = {'item': <Function test_gpu_binary_classification>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_runtest_call'>
methods = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
kwargs = {'item': <Function test_gpu_binary_classification>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
caller_kwargs = {'item': <Function test_gpu_binary_classification>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fdf6643e070>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='runner', plugin=<module '_pytest.runner' from '/home/otivedani/xgboost/.venv/lib/python3.8/sit...xture=None>>, <HookImpl plugin_name='logging-plugin', plugin=<_pytest.logging.LoggingPlugin object at 0x7fdf54eb5610>>]
caller_kwargs = {'item': <Function test_gpu_binary_classification>}
firstresult = False

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_gpu_binary_classification>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
            item.runtest()
        except Exception as e:
            # Store trace info to allow postmortem debugging
            sys.last_type = type(e)
            sys.last_value = e
            assert e.__traceback__ is not None
            # Skip *this* frame
            sys.last_traceback = e.__traceback__.tb_next
>           raise e

.venv/lib/python3.8/site-packages/_pytest/runner.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item = <Function test_gpu_binary_classification>

    def pytest_runtest_call(item: Item) -> None:
        _update_current_test_var(item, "call")
        try:
            del sys.last_type
            del sys.last_value
            del sys.last_traceback
        except AttributeError:
            pass
        try:
>           item.runtest()

.venv/lib/python3.8/site-packages/_pytest/runner.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Function test_gpu_binary_classification>

    def runtest(self) -> None:
        """Execute the underlying test function."""
>       self.ihook.pytest_pyfunc_call(pyfuncitem=self)

.venv/lib/python3.8/site-packages/_pytest/python.py:1627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_HookCaller 'pytest_pyfunc_call'>, args = ()
kwargs = {'pyfuncitem': <Function test_gpu_binary_classification>}
notincall = set()

    def __call__(self, *args, **kwargs):
        if args:
            raise TypeError("hook calling supports only keyword arguments")
        assert not self.is_historic()
        if self.spec and self.spec.argnames:
            notincall = (
                set(self.spec.argnames) - set(["__multicall__"]) - set(kwargs.keys())
            )
            if notincall:
                warnings.warn(
                    "Argument(s) {} which are declared in the hookspec "
                    "can not be found in this hook call".format(tuple(notincall)),
                    stacklevel=2,
                )
>       return self._hookexec(self, self.get_hookimpls(), kwargs)

.venv/lib/python3.8/site-packages/pluggy/hooks.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_pytest.config.PytestPluginManager object at 0x7fdf78b19310>
hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_gpu_binary_classification>}

    def _hookexec(self, hook, methods, kwargs):
        # called from all hookcaller instances.
        # enable_tracing will set its own wrapping function at self._inner_hookexec
>       return self._inner_hookexec(hook, methods, kwargs)

.venv/lib/python3.8/site-packages/pluggy/manager.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook = <_HookCaller 'pytest_pyfunc_call'>
methods = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
kwargs = {'pyfuncitem': <Function test_gpu_binary_classification>}

>   self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(
        methods,
        kwargs,
        firstresult=hook.spec.opts.get("firstresult") if hook.spec else False,
    )

.venv/lib/python3.8/site-packages/pluggy/manager.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_gpu_binary_classification>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
                        res = hook_impl.function(*args)
                        if res is not None:
                            results.append(res)
                            if firstresult:  # halt further impl calls
                                break
            except BaseException:
                excinfo = sys.exc_info()
        finally:
            if firstresult:  # first result hooks return a single value
                outcome = _Result(results[0] if results else None, excinfo)
            else:
                outcome = _Result(results, excinfo)
    
            # run all wrapper post-yield blocks
            for gen in reversed(teardowns):
                try:
                    gen.send(outcome)
                    _raise_wrapfail(gen, "has second yield")
                except StopIteration:
                    pass
    
>           return outcome.get_result()

.venv/lib/python3.8/site-packages/pluggy/callers.py:208: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <pluggy.callers._Result object at 0x7fdf6643e4f0>

    def get_result(self):
        """Get the result(s) for this hook call.
    
        If the hook was marked as a ``firstresult`` only a single value
        will be returned otherwise a list of results.
        """
        __tracebackhide__ = True
        if self._excinfo is None:
            return self._result
        else:
            ex = self._excinfo
            if _py3:
>               raise ex[1].with_traceback(ex[2])

.venv/lib/python3.8/site-packages/pluggy/callers.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hook_impls = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/otivedani/xgboost/.venv/lib/python3.8/site-packages/_pytest/python.py'>>]
caller_kwargs = {'pyfuncitem': <Function test_gpu_binary_classification>}
firstresult = True

    def _multicall(hook_impls, caller_kwargs, firstresult=False):
        """Execute a call into multiple python functions/methods and return the
        result(s).
    
        ``caller_kwargs`` comes from _HookCaller.__call__().
        """
        __tracebackhide__ = True
        results = []
        excinfo = None
        try:  # run impl and wrapper setup functions in a loop
            teardowns = []
            try:
                for hook_impl in reversed(hook_impls):
                    try:
                        args = [caller_kwargs[argname] for argname in hook_impl.argnames]
                    except KeyError:
                        for argname in hook_impl.argnames:
                            if argname not in caller_kwargs:
                                raise HookCallError(
                                    "hook call must provide argument %r" % (argname,)
                                )
    
                    if hook_impl.hookwrapper:
                        try:
                            gen = hook_impl.function(*args)
                            next(gen)  # first yield
                            teardowns.append(gen)
                        except StopIteration:
                            _raise_wrapfail(gen, "did not yield")
                    else:
>                       res = hook_impl.function(*args)

.venv/lib/python3.8/site-packages/pluggy/callers.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pyfuncitem = <Function test_gpu_binary_classification>

    @hookimpl(trylast=True)
    def pytest_pyfunc_call(pyfuncitem: "Function") -> Optional[object]:
        testfunction = pyfuncitem.obj
        if is_async_function(testfunction):
            async_warn_and_skip(pyfuncitem.nodeid)
        funcargs = pyfuncitem.funcargs
        testargs = {arg: funcargs[arg] for arg in pyfuncitem._fixtureinfo.argnames}
>       result = testfunction(**testargs)

.venv/lib/python3.8/site-packages/_pytest/python.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def test_gpu_binary_classification():
        from sklearn.datasets import load_digits
        from sklearn.model_selection import KFold
    
        digits = load_digits(2)
        y = digits['target']
        X = digits['data']
        kf = KFold(n_splits=2, shuffle=True, random_state=rng)
        for cls in (xgb.XGBClassifier, xgb.XGBRFClassifier):
            for train_index, test_index in kf.split(X, y):
>               xgb_model = cls(
                    random_state=42, tree_method='gpu_hist',
                    n_estimators=4, gpu_id='0').fit(X[train_index], y[train_index])

tests/python-gpu/test_gpu_with_sklearn.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_by... scale_pos_weight=None, subsample=None, tree_method='gpu_hist',
              validate_parameters=None, verbosity=None)
X = array([[ 0.,  0.,  0., ..., 10.,  0.,  0.],
       [ 0.,  0.,  1., ...,  3.,  0.,  0.],
       [ 0.,  0.,  0., ..., 16... 0.,  1., ..., 10.,  0.,  0.],
       [ 0.,  0.,  8., ...,  7.,  0.,  0.],
       [ 0.,  0.,  6., ...,  6.,  0.,  0.]])
y = array([1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
       0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,...0, 0, 1, 1, 0, 0, 1, 1, 1,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
       1, 1, 1, 0])
sample_weight = None, base_margin = None, eval_set = None, eval_metric = None
early_stopping_rounds = None, verbose = True, xgb_model = None
sample_weight_eval_set = None, feature_weights = None, callbacks = None

    def fit(self, X, y, sample_weight=None, base_margin=None,
            eval_set=None, eval_metric=None,
            early_stopping_rounds=None, verbose=True, xgb_model=None,
            sample_weight_eval_set=None, feature_weights=None, callbacks=None):
        # pylint: disable = attribute-defined-outside-init,arguments-differ
    
        evals_result = {}
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
    
        xgb_options = self.get_xgb_params()
    
        if callable(self.objective):
            obj = _objective_decorator(self.objective)
            # Use default value. Is it really not used ?
            xgb_options["objective"] = "binary:logistic"
        else:
            obj = None
    
        if self.n_classes_ > 2:
            # Switch to using a multiclass objective in the underlying
            # XGB instance
            xgb_options['objective'] = 'multi:softprob'
            xgb_options['num_class'] = self.n_classes_
    
        feval = eval_metric if callable(eval_metric) else None
        if eval_metric is not None:
            if callable(eval_metric):
                eval_metric = None
            else:
                xgb_options.update({"eval_metric": eval_metric})
    
        self._le = XGBoostLabelEncoder().fit(y)
        training_labels = self._le.transform(y)
    
        if eval_set is not None:
            if sample_weight_eval_set is None:
                sample_weight_eval_set = [None] * len(eval_set)
            else:
                assert len(sample_weight_eval_set) == len(eval_set)
            evals = list(
                DMatrix(eval_set[i][0],
                        label=self._le.transform(eval_set[i][1]),
                        missing=self.missing, weight=sample_weight_eval_set[i],
                        nthread=self.n_jobs)
                for i in range(len(eval_set))
            )
            nevals = len(evals)
            eval_names = ["validation_{}".format(i) for i in range(nevals)]
            evals = list(zip(evals, eval_names))
        else:
            evals = ()
    
        if len(X.shape) != 2:
            # Simply raise an error here since there might be many
            # different ways of reshaping
            raise ValueError(
                'Please reshape the input data X into 2-dimensional matrix.')
    
        self._features_count = X.shape[1]
        self.n_features_in_ = self._features_count
    
        train_dmatrix = DMatrix(X, label=training_labels, weight=sample_weight,
                                base_margin=base_margin,
                                missing=self.missing, nthread=self.n_jobs)
        train_dmatrix.set_info(feature_weights=feature_weights)
    
>       self._Booster = train(xgb_options, train_dmatrix,
                              self.get_num_boosting_rounds(),
                              evals=evals,
                              early_stopping_rounds=early_stopping_rounds,
                              evals_result=evals_result, obj=obj, feval=feval,
                              verbose_eval=verbose, xgb_model=xgb_model,
                              callbacks=callbacks)

venv/lib/python3.8/site-packages/xgboost/sklearn.py:836: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf6643e2b0>, num_boost_round = 4
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = {}, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf6643e2b0>, num_boost_round = 4
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf6643ebe0>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf6643e280>
dtrain = <xgboost.core.DMatrix object at 0x7fdf6643e2b0>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:17] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
________ TestGPUBoostFromPrediction.test_boost_from_prediction_gpu_hist ________

self = <unittest.case._Outcome object at 0x7fdf661cd7f0>
test_case = <test_gpu_with_sklearn.TestGPUBoostFromPrediction testMethod=test_boost_from_prediction_gpu_hist>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_with_sklearn.TestGPUBoostFromPrediction testMethod=test_boost_from_prediction_gpu_hist>
result = <TestCaseFunction test_boost_from_prediction_gpu_hist>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_with_sklearn.TestGPUBoostFromPrediction testMethod=test_boost_from_prediction_gpu_hist>
method = <bound method TestGPUBoostFromPrediction.test_boost_from_prediction_gpu_hist of <test_gpu_with_sklearn.TestGPUBoostFromPrediction testMethod=test_boost_from_prediction_gpu_hist>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_gpu_with_sklearn.TestGPUBoostFromPrediction testMethod=test_boost_from_prediction_gpu_hist>

    def test_boost_from_prediction_gpu_hist(self):
>       self.cpu_test.run_boost_from_prediction('gpu_hist')

tests/python-gpu/test_gpu_with_sklearn.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_with_sklearn.TestBoostFromPrediction testMethod=runTest>
tree_method = 'gpu_hist'

    def run_boost_from_prediction(self, tree_method):
        from sklearn.datasets import load_breast_cancer
        X, y = load_breast_cancer(return_X_y=True)
        model_0 = xgb.XGBClassifier(
            learning_rate=0.3, random_state=0, n_estimators=4,
            tree_method=tree_method)
>       model_0.fit(X=X, y=y)

tests/python/test_with_sklearn.py:991: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,
              colsample_bynode=None, colsample_by... scale_pos_weight=None, subsample=None, tree_method='gpu_hist',
              validate_parameters=None, verbosity=None)
X = array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,
        1.189e-01],
       [2.057e+01, 1.777e+01, ...4.087e-01,
        1.240e-01],
       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,
        7.039e-02]])
y = array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])
sample_weight = None, base_margin = None, eval_set = None, eval_metric = None
early_stopping_rounds = None, verbose = True, xgb_model = None
sample_weight_eval_set = None, feature_weights = None, callbacks = None

    def fit(self, X, y, sample_weight=None, base_margin=None,
            eval_set=None, eval_metric=None,
            early_stopping_rounds=None, verbose=True, xgb_model=None,
            sample_weight_eval_set=None, feature_weights=None, callbacks=None):
        # pylint: disable = attribute-defined-outside-init,arguments-differ
    
        evals_result = {}
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
    
        xgb_options = self.get_xgb_params()
    
        if callable(self.objective):
            obj = _objective_decorator(self.objective)
            # Use default value. Is it really not used ?
            xgb_options["objective"] = "binary:logistic"
        else:
            obj = None
    
        if self.n_classes_ > 2:
            # Switch to using a multiclass objective in the underlying
            # XGB instance
            xgb_options['objective'] = 'multi:softprob'
            xgb_options['num_class'] = self.n_classes_
    
        feval = eval_metric if callable(eval_metric) else None
        if eval_metric is not None:
            if callable(eval_metric):
                eval_metric = None
            else:
                xgb_options.update({"eval_metric": eval_metric})
    
        self._le = XGBoostLabelEncoder().fit(y)
        training_labels = self._le.transform(y)
    
        if eval_set is not None:
            if sample_weight_eval_set is None:
                sample_weight_eval_set = [None] * len(eval_set)
            else:
                assert len(sample_weight_eval_set) == len(eval_set)
            evals = list(
                DMatrix(eval_set[i][0],
                        label=self._le.transform(eval_set[i][1]),
                        missing=self.missing, weight=sample_weight_eval_set[i],
                        nthread=self.n_jobs)
                for i in range(len(eval_set))
            )
            nevals = len(evals)
            eval_names = ["validation_{}".format(i) for i in range(nevals)]
            evals = list(zip(evals, eval_names))
        else:
            evals = ()
    
        if len(X.shape) != 2:
            # Simply raise an error here since there might be many
            # different ways of reshaping
            raise ValueError(
                'Please reshape the input data X into 2-dimensional matrix.')
    
        self._features_count = X.shape[1]
        self.n_features_in_ = self._features_count
    
        train_dmatrix = DMatrix(X, label=training_labels, weight=sample_weight,
                                base_margin=base_margin,
                                missing=self.missing, nthread=self.n_jobs)
        train_dmatrix.set_info(feature_weights=feature_weights)
    
>       self._Booster = train(xgb_options, train_dmatrix,
                              self.get_num_boosting_rounds(),
                              evals=evals,
                              early_stopping_rounds=early_stopping_rounds,
                              evals_result=evals_result, obj=obj, feval=feval,
                              verbose_eval=verbose, xgb_model=xgb_model,
                              callbacks=callbacks)

venv/lib/python3.8/site-packages/xgboost/sklearn.py:836: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf661cd940>, num_boost_round = 4
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = {}, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, ...}
dtrain = <xgboost.core.DMatrix object at 0x7fdf661cd940>, num_boost_round = 4
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf661cda30>
evals_result = {}, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf661cd970>
dtrain = <xgboost.core.DMatrix object at 0x7fdf661cd940>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:17] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
_________________ TestMonotonicConstraints.test_gpu_hist_basic _________________

self = <unittest.case._Outcome object at 0x7fdf6591da30>
test_case = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_basic>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_basic>
result = <TestCaseFunction test_gpu_hist_basic>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_basic>
method = <bound method TestMonotonicConstraints.test_gpu_hist_basic of <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_basic>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_basic>

    @pytest.mark.skipif(**tm.no_sklearn())
    def test_gpu_hist_basic(self):
>       assert_constraint(1, 'gpu_hist')

tests/python-gpu/test_monotonic_constraints.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

constraint = 1, tree_method = 'gpu_hist'

    def assert_constraint(constraint, tree_method):
        from sklearn.datasets import make_regression
        n = 1000
        X, y = make_regression(n, random_state=rng, n_features=1, n_informative=1)
        dtrain = xgb.DMatrix(X, y)
        param = {}
        param['tree_method'] = tree_method
        param['monotone_constraints'] = "(" + str(constraint) + ")"
>       bst = xgb.train(param, dtrain)

tests/python-gpu/test_monotonic_constraints.py:31: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'monotone_constraints': '(1)', 'tree_method': 'gpu_hist'}
dtrain = <xgboost.core.DMatrix object at 0x7fdf6591d040>, num_boost_round = 10
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'monotone_constraints': '(1)', 'tree_method': 'gpu_hist', 'validate_parameters': True}
dtrain = <xgboost.core.DMatrix object at 0x7fdf6591d040>, num_boost_round = 10
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf6591d130>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf6591dbb0>
dtrain = <xgboost.core.DMatrix object at 0x7fdf6591d040>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:17] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
_______________ TestMonotonicConstraints.test_gpu_hist_depthwise _______________

self = <unittest.case._Outcome object at 0x7fdf505b4100>
test_case = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_depthwise>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_depthwise>
result = <TestCaseFunction test_gpu_hist_depthwise>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_depthwise>
method = <bound method TestMonotonicConstraints.test_gpu_hist_depthwise of <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_depthwise>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_depthwise>

    def test_gpu_hist_depthwise(self):
        params = {
            'tree_method': 'gpu_hist',
            'grow_policy': 'depthwise',
            'monotone_constraints': '(1, -1)'
        }
>       model = xgb.train(params, tmc.training_dset)

tests/python-gpu/test_monotonic_constraints.py:53: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'grow_policy': 'depthwise', 'monotone_constraints': '(1, -1)', 'tree_method': 'gpu_hist'}
dtrain = <xgboost.core.DMatrix object at 0x7fdf521c2c40>, num_boost_round = 10
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'grow_policy': 'depthwise', 'monotone_constraints': '(1, -1)', 'tree_method': 'gpu_hist', 'validate_parameters': True}
dtrain = <xgboost.core.DMatrix object at 0x7fdf521c2c40>, num_boost_round = 10
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf505b4ca0>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf505b4970>
dtrain = <xgboost.core.DMatrix object at 0x7fdf521c2c40>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:17] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
_______________ TestMonotonicConstraints.test_gpu_hist_lossguide _______________

self = <unittest.case._Outcome object at 0x7fdf5066e580>
test_case = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_lossguide>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/lib/python3.8/unittest/case.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_lossguide>
result = <TestCaseFunction test_gpu_hist_lossguide>

    def run(self, result=None):
        orig_result = result
        if result is None:
            result = self.defaultTestResult()
            startTestRun = getattr(result, 'startTestRun', None)
            if startTestRun is not None:
                startTestRun()
    
        result.startTest(self)
    
        testMethod = getattr(self, self._testMethodName)
        if (getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)):
            # If the class or method was skipped.
            try:
                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')
                            or getattr(testMethod, '__unittest_skip_why__', ''))
                self._addSkip(result, self, skip_why)
            finally:
                result.stopTest(self)
            return
        expecting_failure_method = getattr(testMethod,
                                           "__unittest_expecting_failure__", False)
        expecting_failure_class = getattr(self,
                                          "__unittest_expecting_failure__", False)
        expecting_failure = expecting_failure_class or expecting_failure_method
        outcome = _Outcome(result)
        try:
            self._outcome = outcome
    
            with outcome.testPartExecutor(self):
                self._callSetUp()
            if outcome.success:
                outcome.expecting_failure = expecting_failure
                with outcome.testPartExecutor(self, isTest=True):
>                   self._callTestMethod(testMethod)

/usr/lib/python3.8/unittest/case.py:676: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_lossguide>
method = <bound method TestMonotonicConstraints.test_gpu_hist_lossguide of <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_lossguide>>

    def _callTestMethod(self, method):
>       method()

/usr/lib/python3.8/unittest/case.py:633: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_monotonic_constraints.TestMonotonicConstraints testMethod=test_gpu_hist_lossguide>

    def test_gpu_hist_lossguide(self):
        params = {
            'tree_method': 'gpu_hist',
            'grow_policy': 'lossguide',
            'monotone_constraints': '(1, -1)'
        }
>       model = xgb.train(params, tmc.training_dset)

tests/python-gpu/test_monotonic_constraints.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'grow_policy': 'lossguide', 'monotone_constraints': '(1, -1)', 'tree_method': 'gpu_hist'}
dtrain = <xgboost.core.DMatrix object at 0x7fdf521c2c40>, num_boost_round = 10
evals = (), obj = None, feval = None, maximize = None
early_stopping_rounds = None, evals_result = None, verbose_eval = True
xgb_model = None, callbacks = None

    def train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None,
              maximize=None, early_stopping_rounds=None, evals_result=None,
              verbose_eval=True, xgb_model=None, callbacks=None):
        # pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init
        """Train a booster with given parameters.
    
        Parameters
        ----------
        params : dict
            Booster params.
        dtrain : DMatrix
            Data to be trained.
        num_boost_round: int
            Number of boosting iterations.
        evals: list of pairs (DMatrix, string)
            List of validation sets for which metrics will evaluated during training.
            Validation metrics will help us track the performance of the model.
        obj : function
            Customized objective function.
        feval : function
            Customized evaluation function.
        maximize : bool
            Whether to maximize feval.
        early_stopping_rounds: int
            Activates early stopping. Validation metric needs to improve at least once in
            every **early_stopping_rounds** round(s) to continue training.
            Requires at least one item in **evals**.
            The method returns the model from the last iteration (not the best one).
            If there's more than one item in **evals**, the last entry will be used
            for early stopping.
            If there's more than one metric in the **eval_metric** parameter given in
            **params**, the last metric will be used for early stopping.
            If early stopping occurs, the model will have three additional fields:
            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.
            (Use ``bst.best_ntree_limit`` to get the correct value if
            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)
        evals_result: dict
            This dictionary stores the evaluation results of all the items in watchlist.
    
            Example: with a watchlist containing
            ``[(dtest,'eval'), (dtrain,'train')]`` and
            a parameter containing ``('eval_metric': 'logloss')``,
            the **evals_result** returns
    
            .. code-block:: python
    
                {'train': {'logloss': ['0.48253', '0.35953']},
                 'eval': {'logloss': ['0.480385', '0.357756']}}
    
        verbose_eval : bool or int
            Requires at least one item in **evals**.
            If **verbose_eval** is True then the evaluation metric on the validation set is
            printed at each boosting stage.
            If **verbose_eval** is an integer then the evaluation metric on the validation set
            is printed at every given **verbose_eval** boosting stage. The last boosting stage
            / the boosting stage found by using **early_stopping_rounds** is also printed.
            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric
            is printed every 4 boosting stages, instead of every boosting stage.
        xgb_model : file name of stored xgb model or 'Booster' instance
            Xgb model to be loaded before training (allows training continuation).
        callbacks : list of callback functions
            List of callback functions that are applied at end of each iteration.
            It is possible to use predefined callbacks by using
            :ref:`Callback API <callback_api>`.
            Example:
    
            .. code-block:: python
    
                [xgb.callback.reset_learning_rate(custom_rates)]
    
        Returns
        -------
        Booster : a trained booster model
        """
>       bst = _train_internal(params, dtrain,
                              num_boost_round=num_boost_round,
                              evals=evals,
                              obj=obj, feval=feval,
                              xgb_model=xgb_model, callbacks=callbacks,
                              verbose_eval=verbose_eval,
                              evals_result=evals_result,
                              maximize=maximize,
                              early_stopping_rounds=early_stopping_rounds)

venv/lib/python3.8/site-packages/xgboost/training.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

params = {'grow_policy': 'lossguide', 'monotone_constraints': '(1, -1)', 'tree_method': 'gpu_hist', 'validate_parameters': True}
dtrain = <xgboost.core.DMatrix object at 0x7fdf521c2c40>, num_boost_round = 10
evals = [], obj = None, feval = None, xgb_model = None
callbacks = <xgboost.callback.CallbackContainer object at 0x7fdf5066e160>
evals_result = None, maximize = None, verbose_eval = True
early_stopping_rounds = None

    def _train_internal(params, dtrain,
                        num_boost_round=10, evals=(),
                        obj=None, feval=None,
                        xgb_model=None, callbacks=None,
                        evals_result=None, maximize=None,
                        verbose_eval=None, early_stopping_rounds=None):
        """internal training function"""
        callbacks = [] if callbacks is None else callbacks
        evals = list(evals)
        params = _configure_metrics(params.copy())
    
        bst = Booster(params, [dtrain] + [d[0] for d in evals])
        nboost = 0
        num_parallel_tree = 1
    
        if xgb_model is not None:
            bst = Booster(params, [dtrain] + [d[0] for d in evals],
                          model_file=xgb_model)
            nboost = len(bst.get_dump())
    
        _params = dict(params) if isinstance(params, list) else params
    
        if 'num_parallel_tree' in _params and _params[
                'num_parallel_tree'] is not None:
            num_parallel_tree = _params['num_parallel_tree']
            nboost //= num_parallel_tree
        if 'num_class' in _params and _params['num_class'] is not None:
            nboost //= _params['num_class']
    
        # Distributed code: Load the checkpoint from rabit.
        version = bst.load_rabit_checkpoint()
        assert rabit.get_world_size() != 1 or version == 0
        start_iteration = int(version / 2)
        nboost += start_iteration
    
        is_new_callback = _is_new_callback(callbacks)
        if is_new_callback:
            assert all(isinstance(c, callback.TrainingCallback)
                       for c in callbacks), "You can't mix new and old callback styles."
            if verbose_eval:
                callbacks.append(callback.EvaluationMonitor())
            if early_stopping_rounds:
                callbacks.append(callback.EarlyStopping(
                    rounds=early_stopping_rounds, maximize=maximize))
            callbacks = callback.CallbackContainer(callbacks, metric=feval)
        else:
            callbacks = _configure_deprecated_callbacks(
                verbose_eval, early_stopping_rounds, maximize, start_iteration,
                num_boost_round, feval, evals_result, callbacks,
                show_stdv=False, cvfolds=None)
    
        callbacks.before_training(bst)
        for i in range(start_iteration, num_boost_round):
            if callbacks.before_iteration(bst, i, dtrain, evals):
                break
            # Distributed code: need to resume to this point.
            # Skip the first update if it is a recovery step.
            if version % 2 == 0:
>               bst.update(dtrain, i, obj)

venv/lib/python3.8/site-packages/xgboost/training.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <xgboost.core.Booster object at 0x7fdf5066e6d0>
dtrain = <xgboost.core.DMatrix object at 0x7fdf521c2c40>, iteration = 0
fobj = None

    def update(self, dtrain, iteration, fobj=None):
        """Update for one iteration, with objective function calculated
        internally.  This function should not be called directly by users.
    
        Parameters
        ----------
        dtrain : DMatrix
            Training data.
        iteration : int
            Current iteration number.
        fobj : function
            Customized objective function.
    
        """
        if not isinstance(dtrain, DMatrix):
            raise TypeError('invalid training matrix: {}'.format(
                type(dtrain).__name__))
        self._validate_features(dtrain)
    
        if fobj is None:
>           _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,
                                                    ctypes.c_int(iteration),
                                                    dtrain.handle))

venv/lib/python3.8/site-packages/xgboost/core.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ret = -1

    def _check_call(ret):
        """Check the return value of C API call
    
        This function will raise exception when error occurs.
        Wrap every API call with this function
    
        Parameters
        ----------
        ret : int
            return value from API calls
        """
        if ret != 0:
>           raise XGBoostError(py_str(_LIB.XGBGetLastError()))
E           xgboost.core.XGBoostError: [22:19:17] /home/otivedani/xgboost/src/gbm/../common/common.h:156: XGBoost version not compiled with GPU support.
E           Stack trace:
E             [bt] (0) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x79) [0x7fdf55f41f79]
E             [bt] (1) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0x105) [0x7fdf560328c5]
E             [bt] (2) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x238) [0x7fdf56037658]
E             [bt] (3) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x87f) [0x7fdf56074cff]
E             [bt] (4) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x7e) [0x7fdf56062a0e]
E             [bt] (5) /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x69) [0x7fdf55f38549]
E             [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fdf7795aff5]
E             [bt] (7) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fdf7795a40a]
E             [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x58c) [0x7fdf7797328c]

venv/lib/python3.8/site-packages/xgboost/core.py:186: XGBoostError
=============================== warnings summary ===============================
python-gpu/test_gpu_basic_models.py::TestGPUBasicModels::test_eta_decay_gpu_hist
  /home/otivedani/xgboost/venv/lib/python3.8/site-packages/xgboost/training.py:17: DeprecationWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
    warnings.warn(f'Old style callback is deprecated.  See: {link}', DeprecationWarning)

python-gpu/test_gpu_with_sklearn.py::test_gpu_binary_classification
  /home/otivedani/xgboost/venv/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass n_class=2 as keyword args. From version 0.25 passing these as positional arguments will result in an error
    warnings.warn("Pass {} as keyword args. From version 0.25 "

-- Docs: https://docs.pytest.org/en/stable/warnings.html
=========================== short test summary info ============================
FAILED tests/python-gpu/test_gpu_basic_models.py::TestGPUBasicModels::test_deterministic_gpu_hist
FAILED tests/python-gpu/test_gpu_basic_models.py::TestGPUBasicModels::test_eta_decay_gpu_hist
FAILED tests/python-gpu/test_gpu_interaction_constraints.py::TestGPUInteractionConstraints::test_interaction_constraints
FAILED tests/python-gpu/test_gpu_interaction_constraints.py::TestGPUInteractionConstraints::test_training_accuracy
FAILED tests/python-gpu/test_gpu_linear.py::TestGPULinear::test_gpu_coordinate
FAILED tests/python-gpu/test_gpu_linear.py::TestGPULinear::test_gpu_coordinate_regularised
FAILED tests/python-gpu/test_gpu_pickling.py::TestPickling::test_pickled_predictor
FAILED tests/python-gpu/test_gpu_pickling.py::TestPickling::test_pickling - x...
FAILED tests/python-gpu/test_gpu_pickling.py::TestPickling::test_predict_sklearn_pickle
FAILED tests/python-gpu/test_gpu_pickling.py::TestPickling::test_training_on_cpu_only_env
FAILED tests/python-gpu/test_gpu_pickling.py::TestPickling::test_wrap_gpu_id
FAILED tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_multi_predict
FAILED tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_predict
FAILED tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_shap - x...
FAILED tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_shap_interactions
FAILED tests/python-gpu/test_gpu_prediction.py::TestGPUPredict::test_sklearn
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_map_auc
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_map_map
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_map_ndcg
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_ndcg_auc
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_ndcg_map
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_ndcg_ndcg
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_pairwise_auc_metric
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_pairwise_map_metric
FAILED tests/python-gpu/test_gpu_ranking.py::TestRanking::test_training_rank_pairwise_ndcg_metric
FAILED tests/python-gpu/test_gpu_training_continuation.py::TestGPUTrainingContinuation::test_gpu_training_continuation_binary
FAILED tests/python-gpu/test_gpu_training_continuation.py::TestGPUTrainingContinuation::test_gpu_training_continuation_json
FAILED tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_gpu_hist
FAILED tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_categorical_32_cat
FAILED tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_external_memory
FAILED tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_empty_dmatrix_prediction
FAILED tests/python-gpu/test_gpu_updaters.py::TestGPUUpdaters::test_specified_gpu_id_gpu_update
FAILED tests/python-gpu/test_gpu_with_sklearn.py::test_gpu_binary_classification
FAILED tests/python-gpu/test_gpu_with_sklearn.py::TestGPUBoostFromPrediction::test_boost_from_prediction_gpu_hist
FAILED tests/python-gpu/test_monotonic_constraints.py::TestMonotonicConstraints::test_gpu_hist_basic
FAILED tests/python-gpu/test_monotonic_constraints.py::TestMonotonicConstraints::test_gpu_hist_depthwise
FAILED tests/python-gpu/test_monotonic_constraints.py::TestMonotonicConstraints::test_gpu_hist_lossguide
======= 37 failed, 1 passed, 31 skipped, 2 warnings in 168.08s (0:02:48) =======
